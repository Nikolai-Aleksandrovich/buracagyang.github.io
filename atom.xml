<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Buracag的博客</title>
  <icon>https://www.gravatar.com/avatar/5d6a8fbb9f799ea7bec71b36b635ce18</icon>
  <subtitle>Beautiful is better than ugly.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://buracagyang.github.io/"/>
  <updated>2019-06-14T07:49:46.342Z</updated>
  <id>https://buracagyang.github.io/</id>
  
  <author>
    <name>Buracag</name>
    <email>15591875898@163.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>概率论1-随机事件和概率</title>
    <link href="https://buracagyang.github.io/2019/06/14/probability-theory-1/"/>
    <id>https://buracagyang.github.io/2019/06/14/probability-theory-1/</id>
    <published>2019-06-14T02:15:42.000Z</published>
    <updated>2019-06-14T07:49:46.342Z</updated>
    
    <content type="html"><![CDATA[<p>同步于<a href="https://blog.csdn.net/buracag_mc" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/" target="_blank" rel="noopener">音尘杂记</a></p><p>主要回顾概率论中关于样本空间、随机事件和常见概率分布的基础知识。</p><h1 id="1-样本空间"><a href="#1-样本空间" class="headerlink" title="1. 样本空间"></a>1. 样本空间</h1><p><strong>样本空间</strong> 是一个随机试验所有可能结果的集合。例如，如果抛掷一枚硬币，那么样本空间就是集合{正面，反面}。如果投掷一个骰子，那么样本空间就是{1, 2, 3, 4, 5, 6}。随机试验中的每个可能结果称为样本点。</p><p>有些试验有两个或多个可能的样本空间。例如，从52 张扑克牌中随机抽出一张，样本空间可以是数字（A到K），也可以是花色（黑桃，红桃，梅花，方块）。如果要完整地描述一张牌，就需要同时给出数字和花色，这时样本空间可以通过构建上述两个样本空间的笛卡儿乘积来得到。</p><a id="more"></a><h1 id="2-随机事件"><a href="#2-随机事件" class="headerlink" title="2. 随机事件"></a>2. 随机事件</h1><p><strong>随机事件</strong>（或简称<strong>事件</strong>） 指的是一个被赋予概率的事物集合，也就是样本空间中的一个子集。<strong>概率(Probability)</strong>表示一个随机事件发生的可能性大小，为0 到1 之间的一个非负实数。比如，一个0.5 的概率表示一个事件有50%的可能性发生。</p><p>对于一个机会均等的抛硬币动作来说，其样本空间为“正面”或“反面”。我们可以定义各个随机事件，并计算其概率。比如，</p><ul><li>{正面}，其概率为0.5；</li><li>{反面}，其概率为0.5；</li><li>空集∅，不是正面也不是反面，其概率为0；</li><li>{正面| 反面}，不是正面就是反面，其概率为1</li></ul><h1 id="3-随机变量"><a href="#3-随机变量" class="headerlink" title="3. 随机变量"></a>3. 随机变量</h1><p>在随机试验中，试验的结果可以用一个数$X$来表示，这个数$X$是随着试验结果的不同而变化的，是样本点的一个函数。我们把这种数称为<strong>随机变量（Random Variable）</strong>。例如，随机掷一个骰子，得到的点数就可以看成一个随机变量$X$，$X$的取值为{1, 2, 3, 4, 5, 6}。</p><p>如果随机掷两个骰子，整个事件空间Ω可以由36 个元素组成：<br>$$<br>Ω = \{(i, j)|i = 1, … , 6; j = 1, … , 6\} \tag{1}<br>$$<br>一个随机事件也可以定义多个随机变量。比如在掷两个骰子的随机事件中，可以定义随机变量$X$为获得的两个骰子的点数和，也可以定义随机变量$Y$为获得的两个骰子的点数差。随机变量$X$可以有11个整数值，而随机变量Y 只有6个。<br>$$<br>\begin{eqnarray}<br>X(i, j) &amp;:=&amp; i + j, x = 2, 3, … , 12 \tag{2} \\<br>Y (i, j) &amp;:=&amp; | i − j |, y = 0, 1, 2, 3, 4, 5 \tag{3}<br>\end{eqnarray}<br>$$</p><p>其中$i, j$分别为两个骰子的点数。</p><h2 id="3-1-离散随机变量"><a href="#3-1-离散随机变量" class="headerlink" title="3.1 离散随机变量"></a>3.1 离散随机变量</h2><p>如果随机变量$X$所有可能取的值为有限可列举的，有$n$个有限取值${x_1, … , x_n}$,则称$X$为离散随机变量</p><p>要了解$X$的统计规律，就必须知道它取每种可能值$x_i$的概率，即<br>$$<br>P(X = x_i) = p(x_i), \qquad ∀i \in [1, n] \tag{4}<br>$$<br>$p(x_1), … , p(x_n)$称为离散型随机变量$X$的<strong>概率分布（Probability Distribution）</strong>或<strong>分布</strong>，并且满足<br>$$<br>\begin{eqnarray}<br>\sum_{i=1}^{n}p(x_i) &amp;=&amp; 1 \\<br>p(x_i) &amp;\geq&amp; 0, \qquad \forall i \in [1, n]<br>\end{eqnarray}\tag{5}<br>$$<br>常见的离散随机变量的概率分布有：</p><p><strong>伯努利分布</strong> 在一次试验中，事件A出现的概率为$\mu$，不出现的概率为$1−\mu$。若用变量$X$表示事件A出现的次数，则$X$的取值为0和1，其相应的分布为:<br>$$<br>p(x) = μ^x(1 − μ)^{(1−x)} \tag{6}<br>$$<br>这个分布称为<strong>伯努利分布（Bernoulli Distribution）</strong>,又名两点分布或者<strong>0-1分布</strong>。</p><p><strong>二项分布</strong> 在n次伯努利分布中，若以变量$X$表示事件A出现的次数，则$X$的取值为{0, · · · , n}，其相应的分布为<strong>二项分布（Binomial Distribution）</strong>。<br>$$<br>P(X = k) = \tbinom{n}{k}μ^k(1 − μ)^{n−k}, \quad k = 1, … , n \tag{7}<br>$$<br>其中$\tbinom{n}{k}$为二项式系数（这就是二项分布的名称的由来），表示从$n$个元素中取出$k$个元素而不考虑其顺序的<strong>组合</strong>的总数。</p><h2 id="3-2-连续随机变量"><a href="#3-2-连续随机变量" class="headerlink" title="3.2 连续随机变量"></a>3.2 连续随机变量</h2><p>与离散随机变量不同，一些随机变量$X$的取值是不可列举的，由全部实数或者由一部分区间组成，比如<br>$$<br>X = \{x|a ≤ x ≤ b\}, -\infty &lt; a &lt; b &lt; \infty \tag{8}<br>$$<br>则称$X$为<strong>连续随机变量</strong>。连续随机变量的值是不可数及无穷尽的。</p><p>对于连续随机变量$X$，它取一个具体值$x_i$的概率为0，这个离散随机变量截然不同。因此用列举连续随机变量取某个值的概率来描述这种随机变量不但做不到，也毫无意义。</p><p>连续随机变量$X$的概率分布一般用<strong>概率密度函数（Probability Density Function，PDF）</strong> p(x)来描述。p(x)为可积函数，并满足<br>$$<br>\begin{eqnarray}<br>\int_{-\infty}^{\infty} p(x)dx &amp;=&amp; 1 \\<br>p(x) &amp;≥&amp; 0<br>\end{eqnarray} \tag{9}<br>$$<br>给定概率密度函数p(x)，便可以计算出随机变量落入某一个区间的概率，而p(x)本身反映了随机变量取落入x的非常小的邻近区间中的概率大小。常见的连续随机变量的概率分布有：</p><p><strong>均匀分布</strong> 若a, b为有限数，[a, b]上的<strong>均匀分布（Uniform Distribution）</strong>的概率密度函数定义为,<br>$$<br>p(x) =<br>\begin {cases}<br>\frac{1}{b-a}, a\leq x \leq b \\<br>0, x<a \quad or x>b<br>\end {cases} \tag{10}<br>$$</a></p><p><strong>正态分布</strong> 正态分布（Normal Distribution），又名<strong>高斯分布（Gaussian Distribution）</strong>，是自然界最常见的一种分布，并且具有很多良好的性质，在很多领域都有非常重要的影响力，其概率密度函数为<br>$$<br>p(x) = \frac{1}{\sqrt{2\pi}\sigma}exp(− \frac{(x − μ)^2}{2\sigma^2}) \tag{11}<br>$$<br>其中$\sigma &gt; 0$，$\mu$和$\sigma$均为常数。若随机变量$X$服从一个参数为$\mu$和$\sigma$的概率分布，简记为<br>$$<br>X \sim \cal N(\mu, \sigma^2) \tag{12}<br>$$<br>当$\mu = 0，\sigma = 1$时，称为<strong>标准正态分布（Standard Normal Distribution）</strong>。</p><h2 id="3-3-累积分布函数"><a href="#3-3-累积分布函数" class="headerlink" title="3.3 累积分布函数"></a>3.3 累积分布函数</h2><p>对于一个随机变量$X$，其<strong>累积分布函数（Cumulative Distribution Function，CDF）</strong>是随机变量$X$的取值小于等于$x$的概率。<br>$$<br>cdf(x) = P(X \leq x) \tag{13}<br>$$<br>以连续随机变量$X$为例，累积分布函数定义为<br>$$<br>cdf(x) =\int_{-\infty}^{x}p(t)dt \tag{14}<br>$$<br>其中p(x)为概率密度函数。下图给出了标准正态分布的累计分布函数和概率密度函数。</p><p><img src="/2019/06/14/probability-theory-1/pdf-cdf.png" alt="pdf-cdf"></p><h1 id="4-随机向量"><a href="#4-随机向量" class="headerlink" title="4. 随机向量"></a>4. 随机向量</h1><p><strong>随机向量</strong> 是指一组随机变量构成的向量。如果$X_1,X_2, … ,X_n$ 为$n$个随机变量, 那么称$[X_1,X_2, … ,X_n]$为一个$n$维随机向量。一维随机向量称为随机变量。随机向量也分为<strong>离散随机向量</strong>和<strong>连续随机向量</strong>。</p><h2 id="4-1离散随机向量"><a href="#4-1离散随机向量" class="headerlink" title="4.1离散随机向量"></a>4.1离散随机向量</h2><p>离散随机向量的<strong>联合概率分布（Joint Probability Distribution）</strong>为<br>$$<br>P(X_1 = x_1,X_2 = x_2, … ,X_n = x_n) = p(x_1, x_2, … , x_n) \tag{15}<br>$$<br>其中$x_i \in \omega_i$为变量$X_i$的取值，$\omega_i$为变量$X_i$的样本空间。和离散随机变量类似，离散随机向量的概率分布满足<br>$$<br>\begin{eqnarray}<br>&amp;p(x_1, x_2, … , x_n) \geq 0, \quad ∀x_1 \in \omega_1, x_2 \in \omega_2, … , x_n \in \omega_n \tag{16} \\<br>&amp;\sum_{x_1 \in \omega_1}\sum_{x_2 \in \omega_2}…\sum_{x_n \in \omega_n}p(x_1, x_2, … , x_n) = 1 \tag{17}<br>\end{eqnarray}<br>$$<br><strong>多项分布</strong> 一个常见的离散向量概率分布为<strong>多项分布（Multinomial Distribution）</strong>。多项分布是二项分布在随机向量的推广。假设一个袋子中装了很多球，总共有$K$个不同的颜色。我们从袋子中取出$n$个球。每次取出一个时，就在袋子中放入一个同样颜色的球（或者说有放回的抽样）。这样保证同一颜色的球在不同试验中被取出的概率是相等的。令$X$为一个$K$维随机向量，每个元素$X_k(k = 1, … ,K)$为取出的$n$个球中颜色为$k$的球的数量，则$X$服从多项分布，其概率分布为<br>$$<br>p(x_1, … , x_K|\mu) = \frac{n!}{x_1! … x_K!}μ_1^{x_1} … μ_K^{x_K} \tag{18}<br>$$<br>其中$\mu = [\mu_1, … , \mu_K]^T$分别为每次抽取的球的颜色为1, … ,K的概率；$x_1, … , x_K$为非负整数，并且满足$\sum_{k=1}^{K}x_k = n$。</p><p>多项分布的概率分布也可以用gamma函数表示：<br>$$<br>p(x_1, … , x_K|\mu) = \frac{\Gamma(\sum_k x_k+1)}{\prod_k \Gamma(x_k+1)}\prod_{k=1}^{K}\mu_k^{x_k} \tag{19}<br>$$<br>其中$\Gamma(z) = \int_{0}^{\infty}\frac{t^{z−1}}{exp(t)}dt$为gamma函数。这种表示形式和狄利克雷分布(  Dirichlet Distribution)类似，而狄利克雷分布可以作为多项分布的共轭先验。</p><h2 id="4-2-连续随机向量"><a href="#4-2-连续随机向量" class="headerlink" title="4.2 连续随机向量"></a>4.2 连续随机向量</h2><p>连续随机向量的其<strong>联合概率密度函数（Joint Probability Density Function）</strong>满足<br>$$<br>\begin{eqnarray}<br>p(x) = p(x_1, … , x_n) ≥ 0 \tag{20} \\<br>\int_{-\infty}^{+\infty} … \int_{-\infty}^{+\infty}p(x_1, … , x_n)dx_1 … dx_n = 1 \tag{21}<br>\end{eqnarray}<br>$$<br><strong>多元正态分布</strong> 一个常见的连续随机向量分布为<strong>多元正态分布（Multivariate Normal</strong><br><strong>Distribution）</strong>，也称为<strong>多元高斯分布（Multivariate Gaussian Distribution）</strong>。若$n$维随机向量$X = [X_1, … ,X_n]^T$服从$n$元正态分布，其密度函数为<br>$$<br>p(x) = \frac{1}{(2π)^{n/2}|\sum|^{1/2}} exp(-\frac{1}{2}(x−\mu)^T\sum^{−1}(x−\mu)) \tag{22}<br>$$<br>其中$\mu$为多元正态分布的均值向量，$\sum$为多元正态分布的协方差矩阵，$|\sum|$表示$\sum$的行列式。</p><p><strong>各项同性高斯分布</strong> 如果一个多元高斯分布的协方差矩阵简化为$\sum = \sigma^2I$，即每一个维随机变量都独立并且方差相同，那么这个多元高斯分布称为<strong>各项同性高斯分布（Isotropic Gaussian Distribution）</strong>。</p><p><strong>Dirichlet 分布</strong> 一个$n$维随机向量$X$的Dirichlet 分布为<br>$$<br>p(x|\alpha) = \frac{\Gamma(\alpha_0)}{\Gamma(\alpha_1) … \Gamma(\alpha_n)} \prod_{i=1}^{n}x_i^{\alpha_i - 1} \tag{23}<br>$$</p><p>其中$\alpha = [\alpha_1, … , \alpha_K]^T$为Dirichlet分布的参数。</p><h1 id="5-边际分布"><a href="#5-边际分布" class="headerlink" title="5. 边际分布"></a>5. 边际分布</h1><p>对于二维离散随机向量$(X, Y)$，假设$X$取值空间为$\Omega_x$，$Y$取值空间为$\Omega_y$。其联合概率分布满足<br>$$<br>p(x, y) \geq 0,\sum_{x\in \Omega_x}\sum_{y \in \Omega_y}p(x_i, y_j) = 1 \tag{24}<br>$$<br>对于联合概率分布p(x, y)，我们可以分别对x和y进行求和。</p><p>(1) 对于固定的x，<br>$$<br>\sum_{y\in \Omega_y}p(x, y) = P(X = x) = p(x) \tag{25}<br>$$<br>(2) 对于固定的y，<br>$$<br>\sum_{x \in \Omega_x}p(x, y) = P(Y = y) = p(y) \tag{26}<br>$$<br>由离散随机向量$(X, Y)$的联合概率分布，对$Y$的所有取值进行求和得到$X$的概率分布；而对$X$的所有取值进行求和得到$Y$的概率分布。这里p(x)和p(y)就称为p(x, y)的<strong>边际分布（Marginal Distribution）</strong>。</p><p>对于二维连续随机向量(X, Y)，其边际分布为：<br>$$<br>\begin{eqnarray}<br>p(x) = \int_{-\infty}^{+\infty}p(x, y)dy \tag{27} \\<br>p(y) = \int_{-\infty}^{+\infty}p(x, y)dx \tag{28}<br>\end{eqnarray}<br>$$<br>一个二元正态分布的边际分布仍为正态分布。</p><h1 id="6-条件概率分布"><a href="#6-条件概率分布" class="headerlink" title="6. 条件概率分布"></a>6. 条件概率分布</h1><p>对于离散随机向量$(X, Y)$，已知$X = x$的条件下，随机变量$Y = y$的<strong>条件概率（Conditional Probability）</strong>为：<br>$$<br>p(y|x) = P(Y = y|X = x) = \frac{p(x, y)}{p(x)} \tag{29}<br>$$<br>这个公式定义了随机变量$Y$关于随机变量X的条件概率分布（Conditional Probability Distribution），简称条件分布。</p><p>对于二维连续随机向量$(X, Y)$，已知$X = x$的条件下，随机变量$Y = y$的<strong>条件概率密度函数（Conditional Probability Density Function）</strong>为<br>$$<br>p(y|x) = \frac{p(x, y)}{p(x)} \tag{30}<br>$$<br>同理，已知$Y = y$的条件下，随机变量$X = x$的条件概率密度函数为<br>$$<br>p(x|y) = \frac{p(x, y)}{p(y)} \tag{31}<br>$$</p><p>通过公式(30) 和(31)，我们可以得到两个条件概率p(y|x) 和p(x|y) 之间的关系。<br>$$<br>p(y|x) = \frac{p(x|y)p(y)}{p(x)} \tag{32}<br>$$<br>这个公式称为<strong>贝叶斯定理（Bayes’ Theorem）</strong>，或贝叶斯公式。</p><h1 id="7-独立与条件独立"><a href="#7-独立与条件独立" class="headerlink" title="7. 独立与条件独立"></a>7. 独立与条件独立</h1><p>对于两个离散（或连续）随机变量$X$和$Y$，如果其联合概率（或联合概率密度函数）p(x, y) 满足<br>$$<br>p(x, y) = p(x)p(y) \tag{33}<br>$$<br>则称X 和Y相互<strong>独立（independence）</strong>，记为$X \perp !!! \perp Y$。</p><p>对于三个离散（或连续）随机变量X、Y 和Z，如果条件概率（或联合概率密度函数）p(x, y|z) 满足<br>$$<br>p(x, y|z) = P(X = x, Y = y|Z = z) = p(x|z)p(y|z) \tag{34}<br>$$</p><p>则称在给定变量$Z$时，$X$和$Y$<strong>条件独立（conditional independence）</strong>，记为$X \perp !!! \perp Y|Z$。</p><h1 id="8-期望和方差"><a href="#8-期望和方差" class="headerlink" title="8. 期望和方差"></a>8. 期望和方差</h1><p><strong>期望</strong> 对于离散变量$X$，其概率分布为$p(x_1), … , p(x_n)$，$X$的期望（Expectation）或均值定义为<br>$$<br>\Bbb{E}[X] = \sum_{i=1}^{n}x_ip(x_i) \tag{35}<br>$$<br>对于连续随机变量$X$，概率密度函数为$p(x)$，其期望定义为<br>$$<br>\Bbb{E}[X] = \int_{\Bbb{R}}xp(x) dx \tag{36}<br>$$<br><strong>方差</strong> 随机变量$X$的方差（Variance）用来定义它的概率分布的离散程度，定义为<br>$$<br>var(X) = \Bbb{E}[X − \Bbb{E}(X)]^2 \tag{37}<br>$$<br>随机变量$X$的方差也称为它的二阶矩。$\sqrt{var(X)}$则称为$X$的根方差或标准差。</p><p><strong>协方差</strong> 两个连续随机变量X和Y的<strong>协方差（Covariance）</strong>用来衡量两个随机变量的分布之间的总体变化性，定义为<br>$$<br>cov(X, Y) = \Bbb{E}[(X − \Bbb(X))((Y − \Bbb{E}(Y))] \tag{38}<br>$$<br>协方差经常也用来衡量两个随机变量之间的线性相关性。如果两个随机变量的协方差为0，那么称这两个随机变量是<strong>线性不相关</strong>。两个随机变量之间没有这里的线性相关性，并非表示它们之间独立的，可能存在某种非线性的函数关系。反之，如果X 与Y是统计独立的，那么它们之间的协方差一定为0。</p><p><strong>协方差矩阵</strong> 两个m和n维的连续随机向量X和Y，它们的协方差（Covariance）为m × n的矩阵，定义为<br>$$<br>cov(X,Y) = \Bbb{E}[(X − \Bbb{E}(X))(Y − \Bbb{E}(Y))^T] \tag{39}<br>$$<br>协方差矩阵$cov(X,Y)$的第$(i, j)$个元素等于随机变量$X_i$和$Y_j$的协方差。两个向量变量的协方差$cov(X,Y)$与$cov(Y,X)$互为转置关系。如果两个随机向量的协方差矩阵为对角阵，那么称这两个随机向量是无关的。</p><p>单个随机向量X的协方差矩阵定义为<br>$$<br>cov(X) = cov(X,X) \tag{40}<br>$$</p><h2 id="8-1-Jensen不等式"><a href="#8-1-Jensen不等式" class="headerlink" title="8.1 Jensen不等式"></a>8.1 Jensen不等式</h2><p>如果$X$是随机变量，$g$是凸函数，则<br>$$<br>g(\Bbb{E}[X]) \leq \Bbb{E}[g(X)] \tag{41}<br>$$</p><p>等式当且仅当$X$是一个常数或$g$是线性时成立。</p><h2 id="8-2-大数定律和中心极限定理"><a href="#8-2-大数定律和中心极限定理" class="headerlink" title="8.2 大数定律和中心极限定理"></a>8.2 大数定律和中心极限定理</h2><p><strong>大数定律（Law Of Large Numbers）</strong> 是指$n$个样本$X_1, … ,X_n$是独立同分布的，即$E[X_1] = … = E[X_n] = \mu$，那么其均值收敛于期望值$\mu$<br>$$<br>\lim_{n \to \infty} \bar{X}_n = \lim_{n\to \infty} \frac{1}{n}(X_1 + … + X_n) \to \mu \tag{42}<br>$$<br><strong>中心极限定理(Central Limit Theorem)</strong> 是指$n$个样本$X_1, … ,X_n$是独立同分布的，则对任意x，分布函数<br>$$<br>F_n(x) = P(\frac{\sum_{i=1}^{n}X_i - n\mu}{\sigma \sqrt{n}} \leq x)<br>$$<br>满足：</p><p>$\lim_{n \to \infty}  F_n(x)$ 近似服从标准正态分布 $\cal{N}(0, 1)$。</p><p>主要参考<a href="https://github.com/nndl/nndl.github.io" target="_blank" rel="noopener">https://github.com/nndl/nndl.github.io</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;主要回顾概率论中关于样本空间、随机事件和常见概率分布的基础知识。&lt;/p&gt;
&lt;h1 id=&quot;1-样本空间&quot;&gt;&lt;a href=&quot;#1-样本空间&quot; class=&quot;headerlink&quot; title=&quot;1. 样本空间&quot;&gt;&lt;/a&gt;1. 样本空间&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;样本空间&lt;/strong&gt; 是一个随机试验所有可能结果的集合。例如，如果抛掷一枚硬币，那么样本空间就是集合{正面，反面}。如果投掷一个骰子，那么样本空间就是{1, 2, 3, 4, 5, 6}。随机试验中的每个可能结果称为样本点。&lt;/p&gt;
&lt;p&gt;有些试验有两个或多个可能的样本空间。例如，从52 张扑克牌中随机抽出一张，样本空间可以是数字（A到K），也可以是花色（黑桃，红桃，梅花，方块）。如果要完整地描述一张牌，就需要同时给出数字和花色，这时样本空间可以通过构建上述两个样本空间的笛卡儿乘积来得到。&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术备忘" scheme="https://buracagyang.github.io/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/"/>
    
      <category term="基础知识" scheme="https://buracagyang.github.io/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>微积分2-常见函数的导数</title>
    <link href="https://buracagyang.github.io/2019/06/12/calculus-2/"/>
    <id>https://buracagyang.github.io/2019/06/12/calculus-2/</id>
    <published>2019-06-12T12:44:46.000Z</published>
    <updated>2019-06-14T07:34:00.002Z</updated>
    
    <content type="html"><![CDATA[<p>同步于<a href="https://blog.csdn.net/buracag_mc" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/" target="_blank" rel="noopener">音尘杂记</a></p><p>在微积分1中已经附上了一个常见函数形式的导数，下文主要是关于向量函数及其导数，以及在机器学习和神经网络中常见的Logistic函数、Softmax函数的导数形式。</p><a id="more"></a><h1 id="1-向量函数及其导数"><a href="#1-向量函数及其导数" class="headerlink" title="1. 向量函数及其导数"></a>1. 向量函数及其导数</h1><p>$$<br>\begin{eqnarray}<br>\frac{\partial x}{\partial x} &amp;=&amp; I \tag{1.1} \\<br>\frac{\partial Ax}{\partial x} &amp;=&amp; A^T \tag{1.2} \\<br>\frac{\partial x^TA}{\partial x} &amp;=&amp; A \tag{1.3}<br>\end{eqnarray}<br>$$</p><h1 id="2-按位计算的向量函数及其导数"><a href="#2-按位计算的向量函数及其导数" class="headerlink" title="2. 按位计算的向量函数及其导数"></a>2. 按位计算的向量函数及其导数</h1><p>假设一个函数$f(x)$的输入是标量$x$。对于一组$K$个标量$x_1, … , x_K$，我们可以通过$f(x)$得到另外一组$K$个标量$z_1, … , z_K$，<br>$$<br>z_k = f(x_k), ∀k = 1, … ,K \tag{1.4}<br>$$<br>为了简便起见，我们定义$x = [x_1, … , x_K]^T，z = [z_1, … , z_K]^T$，<br>$$<br>z = f(x) \tag{1.5}<br>$$<br>其中$f(x)$是按位运算的，即$[f(x)]_i = f(x_i)$。</p><p>当$x$为标量时，$f(x)$的导数记为$f′(x)$。当输入为$K$维向量$x = [x_1, … , x_K]^T$时，其导数为一个对角矩阵。<br>$$<br>\begin{eqnarray}<br>\frac{\partial f(x)}{\partial x} &amp;=&amp; [\frac{\partial f(x_j)}{\partial x_i}]_{K \times K} \ <br>&amp;=&amp; \begin {bmatrix}<br>&amp;f’(x_1)&amp; \quad &amp;0&amp; \quad &amp;…&amp; \quad &amp;0&amp; \\<br>&amp;0&amp; \quad &amp;f’(x_2)&amp; \quad &amp;…&amp; \quad &amp;0&amp; \\<br>&amp;\vdots&amp; \quad &amp;\vdots&amp; \quad &amp;\vdots&amp; \quad &amp;\vdots&amp; \quad \\<br>&amp;0&amp; \quad &amp;0&amp; \quad &amp;…&amp; \quad &amp;f’(x_K)&amp; \\<br>\end {bmatrix} \\<br>&amp;=&amp; diag(f’(x))<br>\end{eqnarray}  \tag{1.6}<br>$$</p><h1 id="3-Logistic函数的导数"><a href="#3-Logistic函数的导数" class="headerlink" title="3. Logistic函数的导数"></a>3. Logistic函数的导数</h1><p>关于logistic函数其实在博文<a href="https://buracagyang.github.io/2019/05/29/logistic-loss-function/">‘Logistic loss函数’</a>中已经有所介绍，接下来要说是更广义的logistic函数的定义：<br>$$<br>logistic(x) = \frac{L}{1 + exp(−k(x − x_0))} \tag{1.7}<br>$$<br>其中，$x_0$是中心点，$L$是最大值，$k$是曲线的倾斜度。下图给出了几种不同参数的Logistic函数曲线。当$x$趋向于$−\infty$时，logistic(x)接近于0；当$x$趋向于$+\infty$时，logistic(x) 接近于$L$。</p><p><img src="/2019/06/12/calculus-2/logistic.png" alt="logistic"></p><p>当参数为($k = 1,  x_0 = 0, L = 1$) 时，Logistic 函数称为标准Logistic 函数，记为f(x)。<br>$$<br>f(x) = \frac{1}{1 + exp(−x)} \tag{1.8}<br>$$<br>标准logistic函数有两个重要的性质如下：<br>$$<br>\begin{eqnarray}<br>f(x) &amp;=&amp; 1 - f(x) \tag{1.9} \\<br>f’(x) &amp;=&amp; f(x)(1 - f(x)) \tag{1.10}<br>\end{eqnarray}<br>$$<br>当输入为$K$维向量$x=[x_1, …, x_K]^T$时，其导数为：<br>$$<br>f’(x) = diag(f(x) \odot (1 − f(x))) \tag{1.11}<br>$$</p><h1 id="4-Softmax函数的导数"><a href="#4-Softmax函数的导数" class="headerlink" title="4. Softmax函数的导数"></a>4. Softmax函数的导数</h1><p>Softmax函数是将多个标量映射为一个概率分布。对于$K$个标量$x_1, … , x_K$，softmax 函数定义为<br>$$<br>z_k = softmax(x_k) = \frac{exp(x_k)}{\sum_{i=1}^{K}exp(x_i)} \tag{1.12}<br>$$<br>这样，我们可以将$K$个变量$x_1, … , x_K$转换为一个分布：$z_1, … , z_K$，满足<br>$$<br>z_k \in [0, 1], ∀k, \quad  \sum_{k=1}^{K}z_k = 1 \tag{1.13}<br>$$<br>当Softmax函数的输入为$K$维向量$x$时，<br>$$<br>\begin{eqnarray}<br>\hat{z} &amp;=&amp; softmax(x) \\<br>&amp;=&amp; \frac{1}{\sum_{k=1}^{K}exp(x_k)}\begin{bmatrix}<br>exp(x_1) \\<br>\vdots \\<br>exp(x_K) \\<br>\end{bmatrix} \\<br>&amp;=&amp; \frac{exp(x)}{\sum_{k=1}^{K}exp(x)} \ <br>&amp;=&amp; \frac{exp(x)}{1_K^Texp(x)} \\<br>\end{eqnarray} \tag{1.14}<br>$$<br>其中$1_K = [1, … , 1]_{K×1}$是$K$维的全1向量。</p><p>Softmax函数的导数为<br>$$<br>\begin{eqnarray}<br>\frac{\partial softmax(x)}{\partial x} &amp;=&amp; \frac{\partial(\frac{exp(x)}{1_K^Texp(x)})}{\partial x} \tag{1.15} \\<br>&amp;=&amp; \frac{1}{1_K^Texp(x)}\frac{\partial exp(x)}{\partial(x)} + \frac{\partial(\frac{1}{1_K^Texp(x)})}{\partial x}(exp(x))^T \tag{1.16} \\<br>&amp;=&amp; \frac{diag(exp(x))}{1_K^Texp(x)} - (\frac{1}{(1_K^Texp(x))^2})\frac{\partial(1_K^Texp(x))}{\partial x}(exp(x))^T \tag{1.17} \\<br>&amp;=&amp; \frac{diag(exp(x))}{1_K^Texp(x)} - (\frac{1}{(1_K^Texp(x))^2})diag(exp(x))1_K(exp(x))^T \tag{1.18} \\<br>&amp;=&amp; \frac{diag(exp(x))}{1_K^Texp(x)} - (\frac{1}{(1_K^Texp(x))^2})exp(x)(exp(x))^T \tag{1.19} \\<br>&amp;=&amp; diag(\frac{exp(x)}{1_K^Texp(x)}) - \frac{exp(x)}{1_K^Texp(x)}.\frac{(exp(x))^T}{1_K^Texp(x)} \tag{1.20} \\<br>&amp;=&amp; diag(softmax(x)) - softmax(x).softmax(x)^T \tag{1.21}<br>\end{eqnarray}<br>$$<br>其中式(1.16)请参考 <a href="https://buracagyang.github.io/2019/06/12/calculus-1/">‘微积分1-导数’</a> 式(1.13)。</p><p>主要参考<a href="https://github.com/nndl/nndl.github.io" target="_blank" rel="noopener">https://github.com/nndl/nndl.github.io</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在微积分1中已经附上了一个常见函数形式的导数，下文主要是关于向量函数及其导数，以及在机器学习和神经网络中常见的Logistic函数、Softmax函数的导数形式。&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术备忘" scheme="https://buracagyang.github.io/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/"/>
    
      <category term="基础知识" scheme="https://buracagyang.github.io/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>微积分1-导数</title>
    <link href="https://buracagyang.github.io/2019/06/12/calculus-1/"/>
    <id>https://buracagyang.github.io/2019/06/12/calculus-1/</id>
    <published>2019-06-12T09:23:57.000Z</published>
    <updated>2019-06-14T02:27:06.347Z</updated>
    
    <content type="html"><![CDATA[<p>同步于<a href="https://blog.csdn.net/buracag_mc" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/" target="_blank" rel="noopener">音尘杂记</a></p><p>微积分1，主要回顾关于微积分中关于导数的相关知识。错误之处，还望诸君不吝指教。</p><a id="more"></a><h1 id="1-导数基础"><a href="#1-导数基础" class="headerlink" title="1. 导数基础"></a>1. 导数基础</h1><p><strong>导数（Derivative）</strong> 是微积分学中重要的基础概念。<br>对于定义域和值域都是实数域的函数$f : \mathbb{R} \to \mathbb{R}$，若$f(x)$在点$x_0$的某个邻域$\triangle x$内，极限定义如下<br>$$<br>f’(x_0) = \lim_{\triangle x \to 0} \frac{f(x_0 + \triangle x) − f(x_0)}{\triangle x} \tag{1.1}<br>$$<br>若极限存在，则称函数$f(x)$在点$x_0$处可导，$f′(x_0)$称为其导数，或导函数，也可以记为$\frac{df(x_0)}{dx}$。在几何上，导数可以看做函数曲线上的切线斜率。</p><p>给定一个连续函数，计算其导数的过程称为微分（Differentiation）。微分的逆过程为积分（Integration）。函数$f(x)$的积分可以写为<br>$$<br>F(x) = \int f(x)dx \tag{1.2}<br>$$<br>其中$F(x)$称为$f(x)$的原函数。</p><p>若函数$f(x)$在其定义域包含的某区间内每一个点都可导，那么也可以说函数$f(x)$在这个区间内可导。如果一个函数$f(x)$在定义域中的所有点都存在导数，则$f(x)$为可微函数（Differentiable Function）。<strong>可微函数一定连续，但连续函数不一定可微</strong>。例如函数$|x|$为连续函数，但在点x = 0处不可导。下表是几个常见函数的导数：</p><table><thead><tr><th style="text-align:center">函数</th><th style="text-align:center">函数形式</th><th style="text-align:center">导数</th></tr></thead><tbody><tr><td style="text-align:center">常数函数</td><td style="text-align:center">$f(x) = C$，其中C为常数</td><td style="text-align:center">$f’(x) = 0$</td></tr><tr><td style="text-align:center">幂函数</td><td style="text-align:center">$f(x) = x^r$， 其中r是非零实数</td><td style="text-align:center">$f’(x) = rx^{r-1}$</td></tr><tr><td style="text-align:center">指数函数</td><td style="text-align:center">$f(x) = exp(x)$</td><td style="text-align:center">$f’(x) = exp(x)$</td></tr><tr><td style="text-align:center">对数函数</td><td style="text-align:center">$f(x) = log_ax$</td><td style="text-align:center">$f’(x) = \frac{1}{xlna}$</td></tr></tbody></table><p><strong>高阶导数</strong> 对一个函数的导数继续求导，可以得到高阶导数。函数$f(x)$的导数$f′(x)$称为一阶导数，$f′(x)$的导数称为二阶导数，记为$f′′(x)$或$\frac{d^2f(x)}{dx^2}$。</p><p><strong>偏导数</strong> 对于一个多元变量函数$f : \mathbb{R}^d \to \mathbb{R}$，它的偏导数（Partial Derivative ）是关于其中一个变量$x_i$的导数，而保持其他变量固定，可以记为$f’_{x_i} (x)，\bigtriangledown_{x_i}f(x)，\frac{∂f(x)}{∂x_i}或\frac{∂}{∂x_i}f(x)$。</p><h1 id="2-矩阵微积分"><a href="#2-矩阵微积分" class="headerlink" title="2. 矩阵微积分"></a>2. 矩阵微积分</h1><p>为了书写简便，我们通常把<strong>单个函数对多个变量</strong> 或者 <strong>多元函数对单个变量</strong>的偏导数写成向量和矩阵的形式，使其可以被当成一个整体被处理。<strong>矩阵微积分（Matrix Calculus）</strong>是多元微积分的一种表达方式，即使用矩阵和向量来表示因变量每个成分关于自变量每个成分的偏导数。</p><p>矩阵微积分的表示通常有两种符号约定：<strong>分子布局（Numerator Layout）</strong>和<strong>分母布局（Denominator Layout）</strong>。两者的区别是一个标量关于一个向量的导数是写成列向量还是行向量。</p><h2 id="2-1-标量关于向量的偏导数"><a href="#2-1-标量关于向量的偏导数" class="headerlink" title="2.1 标量关于向量的偏导数"></a>2.1 标量关于向量的偏导数</h2><p>对于一个$d$维向量$x \in \mathbb{R}^p$，函数$y = f(x) = f(x_1, … , x_p) \in \mathbb{R}$，则$y$关于$x$的偏导数为</p><p>分母布局 :<br>$$<br>\frac{\partial y}{\partial x} = [\frac{\partial y}{\partial x_1}, …, \frac{\partial y}{\partial x_p}]^T \qquad \in \mathbb{R}^{p \times 1} \tag{1.3}<br>$$<br>分子布局：<br>$$<br>\frac{\partial y}{\partial x} = [\frac{\partial y}{\partial x_1}, …, \frac{\partial y}{\partial x_p}] \qquad \in \mathbb{R}^{1 \times p} \tag{1.4}<br>$$<br>在分母布局中，$\frac{∂y}{∂x}$为列向量，而在分子布局中， $\frac{∂y}{∂x}$为行向量。下文如无特殊说明，均采用分母布局。</p><h2 id="2-2-向量关于标量的偏导数"><a href="#2-2-向量关于标量的偏导数" class="headerlink" title="2.2 向量关于标量的偏导数"></a>2.2 向量关于标量的偏导数</h2><p>对于一个标量$x \in \mathbb{R}$，函数$y = f(x) \in \mathbb{R}^q，则$y$关于$x$的偏导数为</p><p>分母布局：<br>$$<br>\frac{\partial y}{\partial x} = [\frac{\partial y_1}{\partial x}, …, \frac{\partial y_q}{\partial x}] \qquad \in \mathbb{R}^{1 \times q} \tag{1.5}<br>$$<br>分子布局：<br>$$<br>\frac{\partial y}{\partial x} = [\frac{\partial y_1}{\partial x}, …, \frac{\partial y_q}{\partial x}]^T \qquad \in \mathbb{R}^{q \times 1} \tag{1.6}<br>$$</p><p>在分母布局中，$\frac{∂y}{∂x}$为行向量，而在分子布局中， $\frac{∂y}{∂x}$为列向量。</p><h2 id="2-3-向量关于向量的偏导数"><a href="#2-3-向量关于向量的偏导数" class="headerlink" title="2.3 向量关于向量的偏导数"></a>2.3 向量关于向量的偏导数</h2><p>对于一个$d$维向量$x \in \mathbb{R}^p$，函数$y = f(x) \in \mathbb{R}^q$ 的值也为一个向量，则$f(x)$关于$x$的偏导数（分母布局）为<br>$$<br>\frac{\partial f(x)}{\partial x} =<br>\begin {bmatrix}<br>&amp;\frac{\partial y_1}{\partial x_1}&amp; &amp;…&amp; &amp;\frac{\partial y_q}{\partial x_1}&amp; \\<br>&amp;\vdots&amp; &amp;\vdots&amp; &amp;\vdots&amp; \\<br>&amp;\frac{\partial y_1}{\partial x_p}&amp; &amp;…&amp; &amp;\frac{\partial y_q}{\partial x_p}&amp; \\<br>\end {bmatrix} \in \mathbb{R}^{p \times q} \tag{1.7}<br>$$<br>称之为<strong>雅克比矩阵（Jacobian Matrix）</strong>。</p><h1 id="3-导数法则"><a href="#3-导数法则" class="headerlink" title="3. 导数法则"></a>3. 导数法则</h1><p>复合函数的导数的计算可以通过以下法则来简化。</p><h2 id="3-1-加-减-法则"><a href="#3-1-加-减-法则" class="headerlink" title="3.1 加(减)法则"></a>3.1 加(减)法则</h2><p>若$x \in \mathbb{R}^p，y = f(x) \in \mathbb{R}^q，z = g(x) \in \mathbb{R}^q$，则<br>$$<br>\frac{\partial(y+z)}{\partial x} = \frac{\partial y}{\partial x} + \frac{\partial z}{\partial x} \in \mathbb{R}^{p×q} \tag{1.10}<br>$$</p><h2 id="3-2-乘法法则"><a href="#3-2-乘法法则" class="headerlink" title="3.2 乘法法则"></a>3.2 乘法法则</h2><p>(1) 若$x \in \mathbb{R}^p，y = f(x) \in \mathbb{R}^q，z = g(x) \in \mathbb{R}^q$，则<br>$$<br>\frac{∂y^Tz}{∂x} = \frac{∂y}{∂x}z + \frac{∂z}{∂x}y \in \mathbb{R}^p \tag{1.11}<br>$$</p><p>(2) 若$x \in \mathbb{R}^p，y = f(x) \in \mathbb{R}^s，z = g(x) \in \mathbb{R}^t，A \in \mathbb{R}^{s×t}$ 和 $x$ 无关，则<br>$$<br>\frac{∂y^TAz}{∂x} = \frac{∂y}{∂x}Az + \frac{∂z}{∂x}A^Ty \in \mathbb{R}^p \tag{1.12}<br>$$<br>(3) 若$x \in \mathbb{R}^p，y = f(x) \in \mathbb{R}，z = g(x) \in \mathbb{R}^q$，则<br>$$<br>\frac{∂yz}{∂x} = y\frac{∂z}{∂x} + \frac{∂y}{∂x}z^T \in \mathbb{R}^{p×q} \tag{1.13}<br>$$</p><h2 id="3-3-链式法则"><a href="#3-3-链式法则" class="headerlink" title="3.3 链式法则"></a>3.3 链式法则</h2><p><strong>链式法则（Chain Rule）</strong>是在微积分中求复合函数导数的一种常用方法。</p><p>(1) 若$x \in \mathbb{R}，u = u(x) \in \mathbb{R}^s，g = g(u) \in \mathbb{R}^t$，则<br>$$<br>\frac{∂g}{∂x} = \frac{∂u}{∂x}\frac{∂g}{∂u} \in \mathbb{R}^{1×t} \tag{1.14}<br>$$</p><p>(2) 若$x \in \mathbb{R}^p，y = g(x) \in \mathbb{R}^s，z = f(y) \in \mathbb{R}^t$，则<br>$$<br>\frac{∂z}{∂x} = \frac{∂y}{∂x}\frac{∂z}{∂y} \in \mathbb{R}^{p×t} \tag{1.15}<br>$$</p><p>(3) 若$X \in \mathbb{R}^{p×q}$为矩阵，$y = g(X) \in \mathbb{R}^s，z = f(y) \in \mathbb{R}$，则<br>$$<br>\frac{∂z}{∂X_{ij}} = \frac{∂y}{∂X_{ij}}\frac{∂z}{∂y} \in \mathbb{R} \tag{1.16}<br>$$<br>主要参考<a href="https://github.com/nndl/nndl.github.io" target="_blank" rel="noopener">https://github.com/nndl/nndl.github.io</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;微积分1，主要回顾关于微积分中关于导数的相关知识。错误之处，还望诸君不吝指教。&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术备忘" scheme="https://buracagyang.github.io/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/"/>
    
      <category term="基础知识" scheme="https://buracagyang.github.io/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>线性代数2-矩阵</title>
    <link href="https://buracagyang.github.io/2019/06/12/linear-algebra-2/"/>
    <id>https://buracagyang.github.io/2019/06/12/linear-algebra-2/</id>
    <published>2019-06-12T06:47:09.000Z</published>
    <updated>2019-06-13T10:09:47.986Z</updated>
    
    <content type="html"><![CDATA[<p>同步于<a href="https://blog.csdn.net/buracag_mc" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/" target="_blank" rel="noopener">音尘杂记</a></p><p>线性代数2，主要回顾关于矩阵的相关知识。错误之处，还望诸君不吝指教。</p><a id="more"></a><h1 id="1-线性映射"><a href="#1-线性映射" class="headerlink" title="1. 线性映射"></a>1. 线性映射</h1><p><strong>线性映射（Linear Mapping）</strong>是指从线性空间V 到线性空间W的一个映射函数$f : V \to W$，并满足：对于$V$中任何两个向量$u$和$v$以及任何标量$c$，有<br>$$<br>\begin{eqnarray}<br>f(u+v) &amp;=&amp; f(u) + f(v), \tag{1.1} \\<br>f(cv) &amp;=&amp; cf(v). \tag{1.2}<br>\end{eqnarray}<br>$$</p><p>两个有限维欧式空间的映射函数$f: \mathbb{R}^n \to \mathbb{R}^m$可以表示为<br>$$<br>y = Ax \triangleq<br>\begin {bmatrix}<br>a_{11}x_1 + a_{12}x_2 + … + a_{1n}x_n \\<br>a_{21}x_1 + a_{22}x_2 + … + a_{2n}x_n \\<br>\vdots \\<br>a_{m1}x_1 + a_{m2}x_2 + … + a_{mn}x_n \\<br>\end {bmatrix}, \tag{1.3}<br>$$<br>其中$A$定义为$m × n$的<strong>矩阵（Matrix）</strong>，是一个由$m$行$n$列元素排列成的矩形阵列。一个矩阵A从左上角数起的第$i$行第$j$列上的元素称为第$i, j$项，通常记为$[A]_{ij}或 a _{ij}$。矩阵$A$定义了一个从$\mathbb{R}^n$ 到 $\mathbb{R}^m$ 的线性映射；向量 $x \in \mathbb{R}^n$ 和 $y \in \mathbb{R}^m$ 分别为两个空间中的<strong>列向量</strong>，即大小分别为$n \times 1$和$m \times 1$的矩阵。<br>$$<br>x =\begin {bmatrix}<br>x_1 \\<br>x_2 \\<br>\vdots \\<br>x_n \\<br>\end {bmatrix}, y = \begin {bmatrix}<br>y_1 \\<br>y_2 \\<br>\vdots \\<br>y_m \\<br>\end {bmatrix}, \tag{1.4}<br>$$<br>一般为方便起见，书籍中约定逗号隔离的向量表示$[x_1, x_2, … , x_n]$为行向量，列向量通常用分号隔开的表示$x =  [x_1; x_2; … ; x_n]$，或行向量的转置$[x_1, x_2, … , x_n]^T$。</p><h1 id="2-矩阵操作"><a href="#2-矩阵操作" class="headerlink" title="2. 矩阵操作"></a>2. 矩阵操作</h1><p><strong>加</strong> 如果$A$和$B$都为$m×n$的矩阵，则$A$和$B$的加也是$m×n$的矩阵，其每个元素是$A$和$B$相应元素相加。<br>$$<br>[A + B]_{ij} = a_{ij} + b_{ij} \tag{1.5}<br>$$</p><p><strong>乘积</strong> 假设有两个$A$和$B$分别表示两个线性映射$g : \mathbb{R}^m \to \mathbb{R}^k$ 和 $f: \mathbb{R}^n \to \mathbb{R}^m，则其复合线性映射为：<br>$$<br>(g \circ f)(x) = g(f(x)) = g(Bx) = A(Bx) = (AB)x, \tag{1.6}<br>$$<br>其中$AB$表示矩阵$A$和$B$的乘积，定义为<br>$$<br>[AB]_{ij} = \sum_{k=1}^na_{ik}b_{kj} \tag{1.7}<br>$$<br>两个矩阵的乘积仅当第一个矩阵的列数和第二个矩阵的行数相等时才能定义。如$A$是$k × m$矩阵和$B$是$m × n$矩阵，则乘积$AB$是一个$k × n$的矩阵。矩阵的乘法也满足结合律和分配律：</p><ul><li><p>结合律： $(AB)C = A(BC)$,</p></li><li><p>分配律： $(A + B)C = AC + BC，C(A + B) = CA + CB$.</p></li></ul><p><strong>Hadamard 积</strong> $A$和$B$的<em>Hadamard</em>积，也称为<em>逐点乘积</em>，为$A$和$B$中对应的元素相乘。<br>$$<br>[A \odot B]_{ij} = a_{ij}b_{ij} \tag{1.8}<br>$$<br>一个标量$c$与矩阵$A$乘积为$A$的每个元素是$A$的相应元素与$c$的乘积<br>$$<br>[cA]_{ij} = ca_{ij} \tag{1.9}<br>$$</p><p><strong>转置</strong> $m×n$矩阵$A$的<strong>转置（Transposition）</strong>是一个$n×m$的矩阵，记为$A^T$，$A^T$的第$i$行第$j$列的元素是原矩阵A的第$j$行第$i$列的元素<br>$$<br>[A^T]_{ij} = [A]_{ji} \tag{1.10}<br>$$<br><strong>向量化</strong> 矩阵的向量化是将矩阵表示为一个列向量。这里，<strong>vec</strong>是向量化算子。设$A = [a_{ij}]_{m×n}$，则<br>$$<br>vec(A) = [a_{11}, a_{21}, … , a_{m1}, a_{12}, a_{22}, … , a_{m2}, … , a_{1n}, a_{2n}, … , a_{mn}]^T \tag{1.11}<br>$$</p><p><strong>迹</strong> $n$ x $n$矩阵$A$的对角线元素之和称为它的<strong>迹（Trace）</strong>，记为tr(A)。尽管矩阵的乘法不满足交换律，但它们的迹相同，即tr(AB) = tr(BA)。</p><p><strong>行列式</strong> $n$ x $n$矩阵$A$的行列式是一个将其映射到标量的函数，通常记作$det(A)$或$|A|$。行列式可以看做是有向面积或体积的概念在欧氏空间中的推广。在$n$维欧氏空间中，行列式描述的是一个线性变换对“体积”所造成的影响。一个$n × n$的矩阵$A$的行列式定义为：<br>$$<br>det(A) = \sum_{\sigma \in S_n}(-1)^k\prod a_i,\sigma(i) \tag{1.12}<br>$$<br>解释一下，$S_n$是$\{1,2,…,n\}$的所有排列的集合，$\sigma$是其中一个排列，$\sigma(i)$是元素i在排列$\sigma$中的位置，k表示$\sigma$中的逆序对的数量。</p><p>其中逆序对的定义为：在排列$\sigma$中，如果有序数对$(i, j)$满足$1 \leq i &lt; j \leq n$但$\sigma(i) &gt; \sigma(j)$，则其为$\sigma$的一个逆序对。举个例子(左侧为排列， 右侧为逆序对数量)：<br>$$<br>eg：[4, 3, 1, 2, 5] \to 5<br>$$</p><p><strong>秩</strong> 一个矩阵$A$的列秩是$A$的线性无关的列向量数量，行秩是$A$的线性无关的行向量数量。一个矩阵的列秩和行秩总是相等的，简称为<strong>秩（Rank）</strong>。</p><p>一个$m × n$的矩阵$A$的秩最大为$min(m, n)$。若$rank(A) = min(m, n)$，则称矩阵为满秩的。如果一个矩阵不满秩，说明其包含线性相关的列向量或行向量，其行列式为0。两个矩阵的乘积$AB$的秩$rank(AB) \leq min(rank(A), rank(B))$。</p><p><strong>范数</strong> 在“线性代数1-向量和向量空间”中已经提及<br>$$<br>\ell_p(v) = \parallel v \parallel_p = {(\sum_{i=1}^{n}|v_i|^p)}^{1/p}, \tag{1.13}<br>$$</p><h1 id="3-矩阵类型"><a href="#3-矩阵类型" class="headerlink" title="3. 矩阵类型"></a>3. 矩阵类型</h1><p><strong>对称矩阵</strong> 对称矩阵（Symmetric Matrix）指其转置等于自己的矩阵，即满足$A = A^T$。</p><p><strong>对角矩阵</strong> 对角矩阵（Diagonal Matrix）是一个主对角线之外的元素皆为0 的矩阵。对角线上的元素可以为0 或其他值。一个$n × n$的对角矩阵$A$满足<br>$$<br>[A]_{ij} = 0 \qquad if \quad i\neq j \quad \forall i,j \in \{1, …, n\} \tag{1.14}<br>$$<br>对角矩阵A也可以记为diag(a)，a 为一个n维向量，并满足<br>$$<br>[A]_{ii} = a_i \tag{1.15}<br>$$</p><p>其中$n × n$的对角矩阵$A = diag(a)$和$n$维向量b的乘积为一个$n$维向量<br>$$<br>Ab = diag(a)b = a \odot b \tag{1.16}<br>$$<br>其中$\odot$表示点乘，即$(a \odot b)_i = a_ib_i$。</p><p><strong>单位矩阵</strong> 单位矩阵（Identity Matrix）是一种特殊的的对角矩阵，其主对角线元素为1，其余元素为0。$n$阶单位矩阵$I_n$，是一个$n × n$的方块矩阵。可以记为$I_n = diag(1, 1, …, 1)$。一个m × n的矩阵A和单位矩阵的乘积等于其本身。<br>$$<br>AI_n = I_mA = A \tag{1.17}<br>$$<br><strong>逆矩阵</strong> 对于一个$n × n$的方块矩阵$A$，如果存在另一个方块矩阵$B$使得<br>$$<br>AB = BA = I_n \tag{1.18}<br>$$<br>其中$I_n$为单位阵，则称$A$是可逆的。矩阵$B$称为矩阵A的逆矩阵（Inverse Matrix），记为$A^{−1}$。</p><blockquote><p>一个方阵的行列式等于0当且仅当该方阵不可逆时。</p></blockquote><p><strong>正定矩阵</strong> 对于一个$n×n$的对称矩阵$A$，如果对于所有的非零向量$x \in \mathbb{R}^n$都满足<br>$$<br>x^TAx &gt; 0 \tag{1.19}<br>$$<br>则$A$为<strong>正定矩阵（Positive-Definite Matrix）</strong>。如果$x^TAx \geq 0$，则$A$是<strong>半正定矩阵（Positive-Semidefinite Matrix）</strong>。</p><p><strong>正交矩阵</strong> 正交矩阵（Orthogonal Matrix）$A$为一个方块矩阵，其逆矩阵等于其转置矩阵。<br>$$<br>A^T = A^{-1} \tag{1.20}<br>$$<br>等价于$A^TA = AA^T = I_n$。</p><p><strong>Gram矩阵</strong> 向量空间中一组向量$v_1, v_2 , … , v_n$的Gram 矩阵（Gram Matrix）;G是内积的对称矩阵，其元素$G_{ij}为${v_i}^T v_j$。</p><h1 id="4-特征值与特征矢量"><a href="#4-特征值与特征矢量" class="headerlink" title="4. 特征值与特征矢量"></a>4. 特征值与特征矢量</h1><p>如果一个标量$\lambda$和一个非零向量v满足<br>$$<br>Av = \lambda v \tag{1.21}<br>$$</p><p>则$\lambda$和$v$分别称为矩阵$A$的<strong>特征值（Eigenvalue）</strong>和<strong>特征向量（Eigenvector）</strong>。</p><h1 id="5-矩阵分解"><a href="#5-矩阵分解" class="headerlink" title="5. 矩阵分解"></a>5. 矩阵分解</h1><p>一个矩阵通常可以用一些比较“简单”的矩阵来表示，称为<strong>矩阵分解（Matrix Decomposition, Matrix Factorization）</strong>。</p><p><strong>奇异值分解</strong> 一个$m×n$的矩阵$A$的奇异值分解（Singular Value Decomposition，SVD）定义为<br>$$<br>A = U\sum V^T \tag{1.22}<br>$$<br>其中$U$和$V$分别为$m × m$和$n × n$的正交矩阵，$\sum$为$m × n$的对角矩阵，其对角线上的元素称为奇异值（Singular Value）。</p><p><strong>特征分解</strong> 一个$n × n$的方块矩阵$A$的特征分解（Eigendecomposition）定义为<br>$$<br>A = Q\Lambda Q^{-1} \tag{1.23}<br>$$<br>其中$Q$为$n×n$的方块矩阵，其每一列都为$A$的特征向量，Λ为对角阵，其每一个对角元素为$A$的特征值。<br>如果$A$为对称矩阵，则A可以被分解为<br>$$<br>A = Q\Lambda Q^T \tag{1.24}<br>$$<br>其中Q为正交阵。</p><p>主要参考<a href="https://github.com/nndl/nndl.github.io" target="_blank" rel="noopener">https://github.com/nndl/nndl.github.io</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;线性代数2，主要回顾关于矩阵的相关知识。错误之处，还望诸君不吝指教。&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术备忘" scheme="https://buracagyang.github.io/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/"/>
    
      <category term="基础知识" scheme="https://buracagyang.github.io/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>线性代数1-向量和向量空间</title>
    <link href="https://buracagyang.github.io/2019/06/11/linear-algebra-1/"/>
    <id>https://buracagyang.github.io/2019/06/11/linear-algebra-1/</id>
    <published>2019-06-11T09:36:42.000Z</published>
    <updated>2019-06-14T07:28:41.766Z</updated>
    
    <content type="html"><![CDATA[<p>同步于<a href="https://blog.csdn.net/buracag_mc" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/" target="_blank" rel="noopener">音尘杂记</a></p><p>后续几篇笔记主要想回顾整理一下需要用到的数学基础知识，主要包括了线性代数、微积分、概念论、数学优化和信息论等内容。相对比较基础，权当复习回顾完善整个知识体系结构。错误之处，还望诸君不吝指教。</p><a id="more"></a><h1 id="1-向量"><a href="#1-向量" class="headerlink" title="1. 向量"></a>1. 向量</h1><p><strong>标量（Scalar）</strong>是一个实数，只有大小，没有方向。而<strong>向量（Vector）</strong>是由一组实数组成的有序数组，同时具有大小和方向。例，一个n维<strong>向量a</strong> 是由n个有序实数组成，表示为：<br>$$<br>a = [a_1, a_2, …, a_n], \tag{1.1}<br>$$<br>其中$a_i$称为向量a的第$i$个分量，或第$i$维。向量符号通常用黑体小写字母$a, b, c$或小写希腊字母$\alpha,\beta, \gamma$ 等来表示。</p><h1 id="2-向量空间"><a href="#2-向量空间" class="headerlink" title="2. 向量空间"></a>2. 向量空间</h1><p><strong>向量空间（Vector Space）</strong>，也称<strong>线性空间（Linear Space）</strong>，是指由向量组成的集合，并满足以下两个条件：</p><ul><li><p>向量加法：向量空间$V$中的两个向量<strong>a</strong>和<strong>b</strong>，它们的和<strong>a + b</strong>也属于空间$V$；</p></li><li><p>标量乘法：向量空间$V$中的任一向量<strong>a</strong>和任一标量$c$，它们的乘积$c · a$也属于空间$V$。</p></li></ul><p><strong>欧氏空间</strong> 一个常用的线性空间是<strong>欧氏空间（Euclidean Space）</strong>。一个欧氏空间表示通常为$\mathbb{R}^n$，其中n为空间<strong>维度（Dimension）</strong>。欧氏空间中向量的加法和标量乘法定义为：<br>$$<br>\begin{eqnarray}<br>[a_1, a_2, … , a_n] + [b_1, b_2, … , b_n] &amp;=&amp; [a_1 + b_1, a_2 + b_2, … , a_n + b_n], \tag{1.2} \\<br>c[a_1, a_2, … , a_n] &amp;=&amp; [ca_1, ca_2, … , ca_n] \tag{1.3}<br>\end{eqnarray}<br>$$<br>其中$a, b, c \in{\mathbb{R}}$为一个标量。</p><p><strong>线性子空间</strong> 向量空间$V$的线性子空间$U$是$V$的一个子集，并且满足向量空间的条件（向量加法和标量乘法）。</p><p><strong>线性无关</strong> 线性空间$V$中的一组向量${v_1, v_2, … , v_n}$，如果对任意的一组标量$\lambda_1, \lambda_2, … , \lambda_n$，满足$\lambda_1v_1 + \lambda_2v_2 + ·… + \lambda_nv_n = 0$，则必然$\lambda_1 = \lambda_2 = … =\lambda_n = 0$，那么${v_1, v_2, … , v_n}$是线性无关的，也称为线性独立的。</p><p><strong>基向量</strong> 向量空间$V$的<strong>基（Base）</strong>$B = {e_1, e_2, … , e_n}$ 是$V$的有限子集，其元素之间线性无关。向量空间$V$所有的向量都可以按唯一的方式表达为$B$中向量的线性组合。对任意$v \in V$，存在一组标量$(\lambda_1, \lambda_2, … , \lambda_n)$ 使得:<br>$$<br>v = \lambda_1e_1 + \lambda_2e_2 + … + \lambda_ne_n \tag{1.4}<br>$$<br>其中基$B$中的向量称为基向量（Base Vector）。如果基向量是有序的，则标量$(\lambda_1, \lambda_2, … , \lambda_n)$ 称为向量$v$关于基$B$的<strong>坐标（Coordinates）</strong>。</p><p>n维空间$V$的一组<strong>标准基（Standard Basis）</strong>为:<br>$$<br>\begin{eqnarray}<br>e_1 &amp;=&amp; [1, 0, …, 0], \tag{1.5} \\<br>e_2 &amp;=&amp; [0, 1, …, 0], \tag{1.6} \\<br>&amp;…&amp;, \tag{1.7} \\<br>e_n &amp;=&amp; [0, 0, …, 1], \tag{1.8}<br>\end{eqnarray}<br>$$</p><p>向量空间$V$中的任一向量$v = [v_1, v_2, … , v_n]$可以唯一的表示为:<br>$$<br>[v_1, v_2, … , v_n] = v_1e_1 + v_2e_2 + … + v_ne_n, \tag{1.9}<br>$$<br>其中$v_1, v_2, … , v_n$也称为向量$v$的<strong>笛卡尔坐标（Cartesian Coordinate）</strong>。向量空间中的每个向量可以看作是一个线性空间中的笛卡儿坐标。</p><p>内积<strong> 一个n维线性空间中的两个向量$a$和$b$，其内积为:<br>$$<br>⟨a, b⟩ = \sum_{i=1}^{n}a_ib_i, \tag{1.10}<br>$$</strong>正交<strong> 如果向量空间中两个向量的内积为0，则它们</strong>正交（Orthogonal）**。如果向量空间中一个向量$v$与子空间$U$中的每个向量都正交，那么向量$v$和子空间$U$正交。</p><h1 id="3-常见的向量"><a href="#3-常见的向量" class="headerlink" title="3. 常见的向量"></a>3. 常见的向量</h1><p><strong>全0向量</strong>指所有元素都为0的向量，用<strong>0</strong>表示。全0向量为笛卡尔坐标系中的原点。</p><p><strong>全1向量</strong>指所有值为1的向量，用<strong>1</strong>表示。</p><p><strong>one-hot向量</strong>为有且只有一个元素为1，其余元素都为0 的向量。one-hot向量是在数字电路中的一种状态编码，指对任意给定的状态，状态寄存器中只有1位为1，其余位都为0。</p><h1 id="4-范数"><a href="#4-范数" class="headerlink" title="4. 范数"></a>4. 范数</h1><p><strong>范数（Norm）</strong>是一个表示向量“长度”的函数，为向量空间内的所有向量赋予非零的正长度或大小。对于一个n维向量<strong>v</strong>，一个常见的范数函数为$\ell_p$范数<br>$$<br>\ell_p(v) = \parallel v \parallel_p = {(\sum_{i=1}^{n}|v_i|^p)}^{1/p}, \tag{1.11}<br>$$<br>其中$p \geq 0$为一个标量的参数。常见的$p$的取值有1，2，$\infty$等。</p><p>$\ell_1$<strong>范数 </strong>， $p = 1$<br>$$<br>\ell_1(v) = \sum_{i=1}^{n}|v_i|, \tag{1.12}<br>$$<br>$\ell_2$<strong>范数 </strong>， $p = 2$<br>$$<br>\ell_2(v) = \sqrt{\sum_{i=1}^{n}|v_i|^2} = \sqrt{v^Tv}, \tag{1.13}<br>$$<br>$\ell_2$范数又称为<strong>Euclidean范数</strong>或者<strong>Frobenius范数</strong>。从几何角度，向量也可以表示为从原点出发的一个有向线段，其$\ell_2$范数为线段的长度，也常称为向量的模。</p><p>$\ell_{\infty}$<strong>范数 </strong>， $p = \infty$,表示为各个元素的最大绝对值<br>$$<br>\ell_{\infty}(v) = ||v||_{\infty} = max\{v_1,v_2, …, v_n\}, \tag{1.14}<br>$$</p><p>主要参考<a href="https://github.com/nndl/nndl.github.io" target="_blank" rel="noopener">https://github.com/nndl/nndl.github.io</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;后续几篇笔记主要想回顾整理一下需要用到的数学基础知识，主要包括了线性代数、微积分、概念论、数学优化和信息论等内容。相对比较基础，权当复习回顾完善整个知识体系结构。错误之处，还望诸君不吝指教。&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术备忘" scheme="https://buracagyang.github.io/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/"/>
    
      <category term="基础知识" scheme="https://buracagyang.github.io/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>分位数回归简介</title>
    <link href="https://buracagyang.github.io/2019/06/03/quantile-regression/"/>
    <id>https://buracagyang.github.io/2019/06/03/quantile-regression/</id>
    <published>2019-06-03T06:24:43.000Z</published>
    <updated>2019-06-03T10:51:29.009Z</updated>
    
    <content type="html"><![CDATA[<p>同步于<a href="https://blog.csdn.net/buracag_mc" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/" target="_blank" rel="noopener">音尘杂记</a></p><p>最近在做一个比较有意思(难搞…)的项目。大致介绍一下相关背景：根据历史的一个工作情况(历史表现，也就是有多少人做了多少工作量)，以及未来的一个预估工作量(预测值)，我们需要预估一个<strong>合理的</strong>人员投入;一言概之，根据历史表现和预测件量预估人员投入。</p><a id="more"></a><p><strong>时序问题？</strong><br>咋一看，这不就是一个时序问题嘛！人力投入如下：<br>$$<br>Y_t = f(T_t, S_t, C_t, I_t)<br>$$<br>其中$T_t$代表长期趋势特征，$S_t$代表季节性或者季节变动，$C_t$代表周期性或循环波动，$I_t$代表随机性或不规则波动。接下来获取特征和历史人员投入，这不就可以预估得到了未来人力投入嘛。</p><p>但是，我们再仔细考虑一下。事情还不仅仅是如此简单。原因有两点：</p><ul><li>与常见的销量、件量等的预测不同，人力的投入不仅仅是一个时序数据，内生的跟工作量强相关；</li><li>预估人员投入的一个很重要的目标是，求得一个合理的人员投入(范围)。</li></ul><p><strong>常规机器学习问题？</strong><br>或者，再稍微拓展一下，由于人员投入是跟工作量是强相关的，我们可不可以用机器学习的思路来解决这个问题。也即：<br>$$<br>Y_t = f(workload, other_features)<br>$$<br>其实也是存在问题的，在上述的有监督学习中，对于每一个instance我们是需要有一个监督值的。对于该场景下，貌似每个instance都存在一个人力投入值；但是我们的目标是需要预估一个<strong>合理的</strong>人力投入，如果单纯地去拟合当前的人力投入，岂不是认为目前的投入即是最优的了，既然如此就没有做这个任务的必要了。</p><p><strong>经济学模型和其他尝试</strong><br>我们也曾尝试从经典的柯布-道格拉斯生产函数形式、<a href="https://doi.org/10.1080/03610926.2014.1001495" target="_blank" rel="noopener">部分随机人力规划系统</a>以及<a href="https://doi.org/10.1016/j.simpat.2015.07.004" target="_blank" rel="noopener">基于强化学习</a>等的的一些思路进行过分析过，均因效果不甚理想或者业务场景不相符而被pass掉。</p><p>最后，考虑到我们的主要目标是预估一个<strong>合理的</strong>人力投入，我们引入了衡量工作质量的一个变量。通过综合考虑质量和效能的关系，以保证预估出的人员数量，在保证工作量的情况或者说在降低人力投入量后工作质量不至于太差，反之亦然。最后，我们用了一个比较简单的方法来解决这个事情 – 分位数回归（Quantile Regression, QR）。</p><p>在介绍分为数回归的知识点之前，需要简要说一下推导过程不然显得太过突兀：<br>定义工作量为$W$,业务指标准时完成量为$W1$,员工数量为$P$，显然，<br>$$<br> \frac{W1}{W} = \frac{W1}{P}\frac{P}{W}<br>$$<br>这里的$\frac{W1}{W}$用来衡量质量情况，$\frac{P}{W}$的倒数$\frac{W}{P}$用来衡量效能情况。我们可以认为，在同一个类型下(工作场景、工作时间)，实际工作效能$\frac{W1}{P}$是一个相对客观的不变的值，令其为$k$。接下来我们便可以用分位数回归的方法求得系数也即$k$值，然后根据需要的质量情况，得到最终的效能范围，再结合预测件量情况，即可得到一个较为合理的人员投入范围。</p><p>首先，我们知道随机变量X的分布函数为：<br>$$<br>F(x) = P(X\leq x)<br>$$<br>则随机变量X的$\tau$分位数的定义为：<br>$$<br>Q_\tau(X) = arginf\{x\in R ; F(x)\geq\tau\}(0&lt;\tau&lt;1)<br>$$<br>若将分布函数F(x)的逆定义为：<br>$$<br>F_X^{-1}(\tau) = inf\{y\in R ; F(y)\geq\tau\}<br>$$</p><p>故：<br>$$<br>Q_\tau(X) = F_X^{-1}(\tau)<br>$$<br>和传统的线性回归估计方法不同的是，分位数回归估计的是一组自变量X与因变量Y的分位数之间线性关系的建模方法，偏向于条件分位数的变化。故OLS在数据出现尖峰(异常值)、长尾分布或者显著异方差等情况时，OLS结果不稳定,但是分位数的估计量确相对稳健。</p><p>设随机向量(X, Y),其中Y在X=x的情况下的条件累积分布函数为$F_{Y|X=x}$(y|x)，则其$\tau$条件分位数定义为：<br>$$<br>Q_\tau(Y|X=x) = arginf\{y\in R ; F(y|x)\geq\tau\}(0&lt;\tau&lt;1)<br>$$</p><p>这里直接附上对于OLS和分位数回归的相关对比：</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">OLS</th><th style="text-align:center">分位数回归估计</th></tr></thead><tbody><tr><td style="text-align:center">原理</td><td style="text-align:center">以平均数为基准，求解最短距离</td><td style="text-align:center">以不同的分位数为基准，求解最短距离</td></tr><tr><td style="text-align:center">前提条件</td><td style="text-align:center">独立、正态、同方差</td><td style="text-align:center">独立</td></tr><tr><td style="text-align:center">假设要求</td><td style="text-align:center">强假设</td><td style="text-align:center">弱假设</td></tr><tr><td style="text-align:center">求解方法</td><td style="text-align:center">OLS</td><td style="text-align:center">加权最小一乘估计</td></tr><tr><td style="text-align:center">检验类型</td><td style="text-align:center">参数检验</td><td style="text-align:center">非参数检验</td></tr><tr><td style="text-align:center">异方差</td><td style="text-align:center">影响大</td><td style="text-align:center">影响小</td></tr><tr><td style="text-align:center">拟合曲线</td><td style="text-align:center">一条拟合曲线</td><td style="text-align:center">一簇拟合曲线</td></tr></tbody></table><p><strong>分位数回归参数估计的思想</strong></p><hr><p>与线性回归不同的是，QR估计量的特点在于，是通过样本到回归曲线的垂直距离的加权和求得；其中权重设置为，在拟合曲线之下的样本权重为$1 - \tau$，拟合曲线之上的样本权重为$\tau$， 即：<br>$$<br>L(\theta) = \min_{\xi\subset{R}}\{\sum_{i:Y_i\ge\xi}\tau|Y_i - \xi| + \sum_{i:Y_i\le\xi}(1 - \tau)|Y_i - \xi|\}<br>$$</p><p>上式可等价为：<br>$$<br>L(\theta) = \min_{\xi\subset{R}}\sum_{i=1}^n\rho_\tau(Y_i - \xi)<br>$$<br>其中，$\rho_\tau(u)=u(\tau-I(u&lt;0))$, $I(Z)$为示性函数。</p><p>QR的损失函数$L(\theta)$不是对称的，是由两条从原点出发的分别位于第一和第二象限的射线组成，显然其斜率比为$\tau:1-\tau$。</p><p>以上，仅是关于分位数回归知识的大概简介，最主要的部分是关于损失函数的设计。</p><p>最后，应用到该项目中时，我们对原始数据进行了离散化的处理，以及经过斯皮尔曼检验后的数据进行训练。由于其是一个计算密集型的任务，应用到全国众多网点时(数万),可以开多个线程池进行并行处理。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;最近在做一个比较有意思(难搞…)的项目。大致介绍一下相关背景：根据历史的一个工作情况(历史表现，也就是有多少人做了多少工作量)，以及未来的一个预估工作量(预测值)，我们需要预估一个&lt;strong&gt;合理的&lt;/strong&gt;人员投入;一言概之，根据历史表现和预测件量预估人员投入。&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术备忘" scheme="https://buracagyang.github.io/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/"/>
    
      <category term="统计学运用" scheme="https://buracagyang.github.io/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E8%BF%90%E7%94%A8/"/>
    
  </entry>
  
  <entry>
    <title>Logistic loss函数</title>
    <link href="https://buracagyang.github.io/2019/05/29/logistic-loss-function/"/>
    <id>https://buracagyang.github.io/2019/05/29/logistic-loss-function/</id>
    <published>2019-05-29T09:16:09.000Z</published>
    <updated>2019-06-03T10:51:02.378Z</updated>
    
    <content type="html"><![CDATA[<p>同步于<a href="https://blog.csdn.net/buracag_mc" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/" target="_blank" rel="noopener">音尘杂记</a></p><p>前面在浏览sklearn中关于<a href="https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression" title="Logistic Regression" target="_blank" rel="noopener">Logistic Regression</a>部分，看到关于带正则项的LR目标损失函数的定义形式的时候，对具体表达式有点困惑，后查阅资料，将思路整理如下。</p><a id="more"></a><h1 id="1-sklearn文档中的LR损失函数"><a href="#1-sklearn文档中的LR损失函数" class="headerlink" title="1. sklearn文档中的LR损失函数"></a>1. sklearn文档中的LR损失函数</h1><p>先看sklearn对于LR目标损失函数(带L2)的定义：<br>$$<br>\min_{w, c} \frac{1}{2}w^T w + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1)<br>$$</p><p>看到这个表达形式，其实是有两个疑问：</p><ul><li><p>logistic loss的表达形式</p></li><li><p>正则项的惩罚系数</p></li></ul><p>对于第二个问题，其实比较容易解释。通常我们在最小化结构风险时，会给我们的惩罚项乘上一个惩罚系数λ(通常1 &lt; λ &lt; 0)，<br>$$<br>\min_{w, λ} \sum_{i=1}^nloss(y, y_i) + λw^T w<br>$$<br>一般，为方便处理，做一个技巧性地处理，对多项式乘上一个正数 1/2λ, 得到：<br>$$<br>\min_{w, λ} \frac{1}{2λ}\sum_{i=1}^nloss(y, y_i) + \frac{1}{2}w^T w<br>$$<br>令C = 1/2λ即可。</p><p>但是对于第一个形式，当时比较困惑；特意翻看了一下我以前记录的关于<a href="https://blog.csdn.net/buracag_mc/article/details/77620686" title="LR损失函数" target="_blank" rel="noopener">LR以及LR损失函数</a>的一些笔记。</p><h1 id="2-LR损失函数"><a href="#2-LR损失函数" class="headerlink" title="2. LR损失函数"></a>2. LR损失函数</h1><p>为了方便说明笔者当时的疑惑所在，便将当时脑海里存在的logistic loss函数形式 和 sklearn中LR损失函数的推导方法分别记为旧思路和新思路吧。</p><h2 id="2-1-logistic基础知识"><a href="#2-1-logistic基础知识" class="headerlink" title="2.1 logistic基础知识"></a>2.1 logistic基础知识</h2><p>如指数分布、高斯分布等分布一样，logistic是一种变量的分布，它也有自己的概率分布函数和概率密度函数，其中概率分布函数如下：<br>$$<br>F(x) = P(X \leq x) = \frac{1}{1+e^{-(x-\mu)/\gamma}}<br>$$</p><p>对概率分布函数求导，记得到对应的概率密度函数：<br>$$<br>f(x) = \frac{e^{-(x- \mu)/ \gamma}}{\gamma(1+e^{-(x-\mu)/\gamma})^2}<br>$$</p><p>其中，$\mu$就是分布对应的均值，$\gamma$是对应的形状参数。</p><p>下文，为简介方便起见，将$-(x-\mu)/\gamma$ 替换为 $-x$,故记为：<br>$$<br>F(x) = \frac{1}{1+exp(-x)}<br>$$</p><p>对应示例图如下：<br><img src="/2019/05/29/logistic-loss-function/20170827141819860.png" alt="这里写图片描述"></p><p>logistic有一个很重要的性质是：<br>$$<br>F(-x) = \frac{1}{1+exp(x)} = \frac{1}{1+\frac{1}{exp(-x)}} =<br>\frac{exp(-x)}{1+exp(-x)}=1-\frac{1}{1+exp(-x)}=1-F(x)<br>$$</p><p>通常，应用到LR中，有如下形式：</p><blockquote><p><strong>(1)</strong><br>$$<br>P(Y=1|\beta,x) = \frac{1}{1+exp(-\beta x)} = \frac{e^{\beta x}}{1+e^{\beta x}}<br>$$<br>$$<br>P(Y=0|\beta,x) = 1 - \frac{1}{1+exp(-\beta x)} = \frac{1}{1+e^{\beta x}}<br>$$</p></blockquote><blockquote><p>一个事件的几率(odds)，定义为该事件发生与不发生的概率比值，若事件发生概率为p：</p></blockquote><p>$$<br>odds = \frac{p}{1-p}<br>$$</p><p>那么该事件的对数几率（log odds或者logit）如下：<br>$$<br>logit(p)=log\frac{p}{1−p}<br>$$</p><p>那么，对于上述二项，Y=1的对数几率就是：<br>$$<br>log \frac{P(Y=1|\beta,x)}{1−P(Y=1|\beta,x)}=log \frac{P(Y=1|\beta,x)}{P(Y=0|\beta,x)}=\beta x<br>$$</p><p>也就是说，输出Y=1的对数几率是由输入x的线性函数表示的模型，这就是逻辑回归模型。易知，当 $\beta x$的值越大，$P(Y=1|\beta,x)$越接近1；$\beta x$越小,$P(Y=1|\beta,x)$ 越接近0。</p><p>其实，LR就是一个线性分类的模型。与线性回归不同的是：LR将线性方程输出的很大范围的数压缩到了[0,1]区间上；更优雅地说：<strong>LR就是一个被logistic方程归一化后的线性回归</strong>。</p><h2 id="2-2-旧思路"><a href="#2-2-旧思路" class="headerlink" title="2.2 旧思路"></a>2.2 旧思路</h2><p>旧思路要从LR的参数求解过程说起。</p><p>我们知道统计学中一种很常用的方法是根据最大化似然函数的值来估计总体参数。在机器学习领域，我们听到的更多是损失函数的概念，常通过构建损失函数，然后最小化损失函数估计目标参数。在这里，<strong>最大化对数似然函数与最小化对数似然损失函数其实是等价的</strong>，下面我们可以看到。</p><ul><li><p>假设我们有n个独立的训练样本$\{(x_1,y_1),(x_2,y_2),(x_3,y_3),…,(x_n,y_n)\},y={0,1}$,那么每一个观察到的样本$(x_i,y_i)$出现的概率是：<br>$$<br>P(y_i,x_i) = P(y_i=1 | x_i)^{y_i}(1-P(y_i=1 | x_i))^{1-y_i}<br>$$<br>显然，$y_i$为1时，保留前半部分；$y_i$为0时，保留后半部分。</p></li><li><p>构建似然函数：<br>$$<br>L(\beta) = \prod P(y_i=1|x_i)^{y_i}(1-P(y_i=1|x_i))^{1-y_i}<br>$$</p></li><li><p>OK,对似然函数取对数，得到对数似然函数：</p></li></ul><p>$$LL(\beta) = log(L(\beta))= log(\prod P(y_i=1|x_i)^{y_i}(1-P(y_i=1|x_i))^{1-y_i}) $$</p><p> $= \sum_{i=1}^{n}(y_i log P(y_i=1|x_i) + (1-y_i)log(1-P(y_i=1|x_i)))$</p><p> $= \sum_{i=1}^{n}y_i log \frac{P(y_i=1|x_i)}{1-P(y_i=1|x_i)} + \sum_{i=1}^{n}log(1-P(y_i=1|x_i))$</p><p> $= \sum_{i=1}^{n}y_i(\beta x) + \sum_{i=1}^{n}logP(y_i=0|x_i)$</p><p> $= \sum_{i=1}^{n}y_i(\beta x) - \sum_{i=1}^{n}log(1+e^{\beta x})$</p><ul><li><p>用 $LL(\beta)$ 对 $\beta$ 求偏导，得：<br>$\frac{\partial LL(\beta)}{\partial \beta}<br>= \sum_{i=1}^{n}y_ix_i - \sum_{i=1}^{n} \frac{e^{\beta x_i}}{1+e^{\beta x_i}}.x_i$</p><p>$= \sum_{i=1}^{n}(y_i - P(y_i=1|x_i))x_i$<br>该式是无法解析求解，故会用到一些优化算法进行求解(梯度下降、牛顿法等)，这不是本文重点，便不再赘述。</p></li></ul><p>咋一看的确与sklearn中的形式差别有点大，所以请看新思路。</p><h2 id="2-3-新思路"><a href="#2-3-新思路" class="headerlink" title="2.3 新思路"></a>2.3 新思路</h2><p>在式(1)中， $x$表示特征向量，$\beta$表示相应的超参数，此时$y\in({0, 1})$表示样本对应的标签(label)。</p><p>这里，特别要讲的是另一种表达形式，将标签与预测函数在形式上统一了：</p><blockquote><p><strong>(2)</strong><br>$$<br>P(g=\pm1 |\beta, x) = \frac{1}{1+exp(-g\beta x)}<br>$$</p></blockquote><p>此时的样本标签$g\in({1, -1})$。</p><p>虽然式(1)与式(2)看起来似乎不同，但是我们可以有如下证明：<br>$$<br>P(Y=1|\beta,x) = \frac{e^{\beta x}}{1+e^{\beta x}} =  \frac{1}{1+exp(-\beta x)} = P(g=1 |\beta, x)<br>$$<br>同理，我们可以证明$P(Y=0|\beta,x)$ 和 $P(g=-1|\beta,x)$是等价的。</p><p>既然两种形式是等价的，为了适应更加广泛的分类loss最小化的框架，故采用第二种形式来表示LR.毕竟<strong>Simple is better than complex.</strong></p><p>首先定义$x_i$为特征向量，$y_i$为样本标签,则目标损失函数可以表示为：<br>$$<br>arg\min_{\beta}\sum_{i=1}L(y_i, f(x_i))<br>$$<br>其中，f是我们的回归方程，L是目标损失函数。</p><p>对应到LR中，我们有<br>$$<br>f(x) = \beta x<br>$$<br>$$<br>L(y, f(x)) = log(1 + exp(-yf(x)))<br>$$<br>如果将LR的第二种表达形式带入到损失函数L中，可得：<br>$$<br>L(y, f(x)) = log(1 + exp(-yf(x))) = log(\frac{1}{P(y|\beta,x)})<br>$$</p><p>再进一步：<br>$$<br>arg\min_{\beta}\sum_{i=1}L(y_i, f(x_i)) = arg\min_{\beta}\sum_{i=1}log(\frac{1}{P(y_i|\beta,x_i)})<br>$$<br>$$<br>= arg\max_{\beta}\sum_{i=1}log(P(y_i|\beta,x_i))= arg\max_{\beta}\prod_{i=1}P(y_i|\beta,x_i)<br>$$<br><strong>等式最后即为极大似然估计的表达形式。</strong></p><h1 id="3-思考"><a href="#3-思考" class="headerlink" title="3. 思考"></a>3. 思考</h1><p>其实到这儿，我们不难发现在旧思路中，推导极大化对数似然函数中的第二步：<br>$= \sum_{i=1}^{n}(y_i log P(y_i=1|x_i) + (1-y_i)log(1-P(y_i=1|x_i)))$</p><p>与新思路中的：<br>$$<br>=arg\max_{\beta}\sum_{i=1}log(P(y_i|\beta,x_i))<br>$$<br><strong>本质是统一的。</strong></p><p>最后</p><blockquote><p><strong>“Simple is better than complex.”   – The Zen of Python, by Tim Peters</strong></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;前面在浏览sklearn中关于&lt;a href=&quot;https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression&quot; title=&quot;Logistic Regression&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Logistic Regression&lt;/a&gt;部分，看到关于带正则项的LR目标损失函数的定义形式的时候，对具体表达式有点困惑，后查阅资料，将思路整理如下。&lt;/p&gt;
    
    </summary>
    
    
      <category term="统计学运用" scheme="https://buracagyang.github.io/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E8%BF%90%E7%94%A8/"/>
    
      <category term="机器学习" scheme="https://buracagyang.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="算法备忘" scheme="https://buracagyang.github.io/tags/%E7%AE%97%E6%B3%95%E5%A4%87%E5%BF%98/"/>
    
  </entry>
  
  <entry>
    <title>Python中定义类的相关知识</title>
    <link href="https://buracagyang.github.io/2019/05/29/python-basic-about-class/"/>
    <id>https://buracagyang.github.io/2019/05/29/python-basic-about-class/</id>
    <published>2019-05-29T09:12:54.000Z</published>
    <updated>2019-06-03T10:51:24.181Z</updated>
    
    <content type="html"><![CDATA[<p>同步于<a href="https://blog.csdn.net/buracag_mc" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/" target="_blank" rel="noopener">音尘杂记</a></p><p>主要介绍了在python中，抽象类的定义、多态的概念、类中属性的封装以及类中常见的修饰器。</p><a id="more"></a><h1 id="1-抽象类"><a href="#1-抽象类" class="headerlink" title="1. 抽象类"></a>1. 抽象类</h1><p>与Java一样，Python也有抽象类的概念，抽象类是一个特殊的类。其特殊之处在于</p><ul><li>只能被继承，不能被实例化；</li><li>子类必须完全覆写(实现)其“抽象方法”和“抽象属性”后才能被实例化。</li></ul><p>可以有两种实现方式: 利用NotImplementedError实现和利用abctractmethod实现</p><h2 id="1-1-NotImplementedError"><a href="#1-1-NotImplementedError" class="headerlink" title="1.1 NotImplementedError"></a>1.1 NotImplementedError</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2018/11/20 10:11</span></span><br><span class="line"><span class="comment"># @File     : test_interface.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"><span class="comment"># @Desc     :</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#########################################</span></span><br><span class="line"><span class="comment"># 利用NotImplementedError</span></span><br><span class="line"><span class="comment">#########################################</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Payment</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pay</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChildPay</span><span class="params">(Payment)</span>:</span></span><br><span class="line">    <span class="comment"># 必须实现pay方法,否则报错NotImplementedError</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pay</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"TestPay pay"</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">payed</span><span class="params">(self, money)</span>:</span></span><br><span class="line">print(<span class="string">"Payed: &#123;&#125;"</span>.format(money))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">child_pay = ChildPay()</span><br><span class="line">child_pay.payed(<span class="number">20</span>)</span><br></pre></td></tr></table></figure><h2 id="1-2-abctractmethod"><a href="#1-2-abctractmethod" class="headerlink" title="1.2 abctractmethod"></a>1.2 abctractmethod</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2018/11/20 10:11</span></span><br><span class="line"><span class="comment"># @File     : test_interface.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"><span class="comment"># @Desc     :</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> abc <span class="keyword">import</span> ABCMeta, abstractmethod</span><br><span class="line"></span><br><span class="line"><span class="comment"># #########################################</span></span><br><span class="line"><span class="comment"># abstractmethod</span></span><br><span class="line"><span class="comment"># 子类必须全部重写父类的abstractmethod方法</span></span><br><span class="line"><span class="comment"># 非abstractmethod方法可以不实现重写</span></span><br><span class="line"><span class="comment"># 带abstractmethod方法的类不能实例化</span></span><br><span class="line"><span class="comment"># #########################################</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Payment</span><span class="params">(metaclass=ABCMeta)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span></span></span><br><span class="line">self.name = name</span><br><span class="line"></span><br><span class="line"><span class="meta">@abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pay</span><span class="params">(self, money)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span><span class="params">(self, money)</span>:</span></span><br><span class="line">        print(<span class="string">"Payment get &#123;&#125;"</span>.format(money))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">total</span><span class="params">(self, money)</span>:</span></span><br><span class="line">        print(<span class="string">"Payment total &#123;&#125;"</span>.format(money))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChildPay</span><span class="params">(Payment)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pay</span><span class="params">(self, money)</span>:</span></span><br><span class="line">        print(<span class="string">"ChildPay pay &#123;&#125;"</span>.format(money))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span><span class="params">(self, money)</span>:</span></span><br><span class="line">        print(<span class="string">"ChildPay get &#123;&#125;"</span>.format(money))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">child_pay = ChildPay(<span class="string">"safly"</span>)</span><br><span class="line">child_pay.pay(<span class="number">100</span>)</span><br><span class="line">child_pay.get(<span class="number">200</span>)</span><br><span class="line">child_pay.total(<span class="number">400</span>)</span><br><span class="line"><span class="comment"># 不能实例化</span></span><br><span class="line"><span class="comment"># TypeError: Can't instantiate abstract class Payment</span></span><br><span class="line"><span class="comment"># with abstract methods get, pay</span></span><br><span class="line"><span class="comment"># a = Payment("safly")</span></span><br></pre></td></tr></table></figure><h1 id="2-多态概念"><a href="#2-多态概念" class="headerlink" title="2. 多态概念"></a>2. 多态概念</h1><p>向不同的对象发送同一条消息(obj.func(): 是调用了obj的方法func, 又称向obj发送了一条消息func)，不同的对象在接受时会产生不同的行为（即不同的处理方法）。</p><p>也就是说，每个对象可以用自己的方式去响应共同的消息。所谓消息，就是调用函数，不同的对象可以执行不同的函数。</p><p>例： 男生.放松了()， 女生.放松了()，男生是打篮球，女生是看综艺，虽然二者消息一样，但是处理方法不同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2018/11/20 10:11</span></span><br><span class="line"><span class="comment"># @File     : test_interface.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"><span class="comment"># @Desc     :</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> abc <span class="keyword">import</span> ABCMeta, abstractmethod</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base</span><span class="params">(metaclass=ABCMeta)</span>:</span></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">relax</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Boy</span><span class="params">(Base)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">relax</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"playing basketball"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Girl</span><span class="params">(Base)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">relax</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"watching TV"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">boy = Boy()</span><br><span class="line">girl = Girl()</span><br><span class="line">boy.talk()  <span class="comment"># playing basketball</span></span><br><span class="line">girl.talk()  <span class="comment"># watching TV</span></span><br></pre></td></tr></table></figure><h1 id="3-属性封装"><a href="#3-属性封装" class="headerlink" title="3. __属性封装"></a>3. __属性封装</h1><h2 id="3-1-私有静态属性、私有方法"><a href="#3-1-私有静态属性、私有方法" class="headerlink" title="3.1 私有静态属性、私有方法"></a>3.1 私有静态属性、私有方法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2018/11/20 10:11</span></span><br><span class="line"><span class="comment"># @File     : test_interface.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"><span class="comment"># @Desc     :</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># #########################################</span></span><br><span class="line"><span class="comment"># __属性封装</span></span><br><span class="line"><span class="comment"># 私有静态属性、私有方法</span></span><br><span class="line"><span class="comment"># #########################################</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment"># 私有静态属性</span></span><br><span class="line">    __kind = <span class="string">"private kind"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 调用私有静态属性</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_kind</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> Dog.__kind</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 私有方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__func</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"__func"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 调用私有方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.__func()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"><span class="comment"># 如下调用错误,因为需要在类内调用</span></span><br><span class="line"><span class="comment"># print(Dog.__kind)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提倡如下调用方式</span></span><br><span class="line">d = Dog()</span><br><span class="line">print(d.get_kind())</span><br><span class="line">print(d.func())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不提倡如下调用方式</span></span><br><span class="line"><span class="comment"># d._Dog__func()</span></span><br><span class="line"><span class="comment"># print(Dog.__dict__)</span></span><br><span class="line"><span class="comment"># print(Dog._Dog__kind)</span></span><br><span class="line"><span class="comment"># print(Dog._Dog__func)</span></span><br></pre></td></tr></table></figure><h2 id="3-2-私有对象属性"><a href="#3-2-私有对象属性" class="headerlink" title="3.2 私有对象属性"></a>3.2 私有对象属性</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2018/11/20 10:11</span></span><br><span class="line"><span class="comment"># @File     : test_interface.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"><span class="comment"># @Desc     :</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># #########################################</span></span><br><span class="line"><span class="comment"># 私有对象属性</span></span><br><span class="line"><span class="comment"># #########################################</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, weight)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.__weight = weight</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_weight</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.__weight</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">room = Dog(<span class="string">"doggy"</span>, <span class="number">5</span>)</span><br><span class="line">print(room.name)  <span class="comment"># doggy</span></span><br><span class="line">print(room.get_weight())  <span class="comment"># 5</span></span><br><span class="line"><span class="comment"># 不能如下方法调用私有对象属性</span></span><br><span class="line"><span class="comment"># print(room.__weight)</span></span><br></pre></td></tr></table></figure><h2 id="3-3-私有属性不被继承"><a href="#3-3-私有属性不被继承" class="headerlink" title="3.3 私有属性不被继承"></a>3.3 私有属性不被继承</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2018/11/20 10:11</span></span><br><span class="line"><span class="comment"># @File     : test_interface.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"><span class="comment"># @Desc     :</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># #########################################</span></span><br><span class="line"><span class="comment"># 私有属性不能被继承</span></span><br><span class="line"><span class="comment"># #########################################</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DogParent</span><span class="params">(object)</span>:</span></span><br><span class="line">__private = <span class="string">'PRIVATE'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        self.__name = name</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__func</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"__DogParent func"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DogChild</span><span class="params">(DogParent)</span>:</span></span><br><span class="line">    <span class="comment"># 如下的方法是错误的</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_private</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> DogParent.__private</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">dog_parent = DogParent(<span class="string">"Tom"</span>)</span><br><span class="line">print(dir(dog_parent))</span><br><span class="line">print(<span class="string">"-------------"</span>)</span><br><span class="line">dog_child = DogChild(<span class="string">"Tommy"</span>)</span><br><span class="line">print(dir(dog_child))</span><br><span class="line"><span class="comment"># 调用报错AttributeError: type object 'DogChild' has no attribute '_DogChild__private'</span></span><br><span class="line"><span class="comment"># print(dog_child.get_private())</span></span><br></pre></td></tr></table></figure><h1 id="4-类中的常见修饰器"><a href="#4-类中的常见修饰器" class="headerlink" title="4. 类中的常见修饰器"></a>4. 类中的常见修饰器</h1><p>主要介绍最常见的装饰器，classmethod, staticmethod和property</p><h2 id="4-1-classmethod"><a href="#4-1-classmethod" class="headerlink" title="4.1 classmethod"></a>4.1 classmethod</h2><p>@classmethod<br>不需要self参数，但是classmethod方法的第一个参数是需要表示自身类的cls 参数；不管是从类本身调用还是从实例化后的对象调用，都用第一个参数把类传进来。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DogParent</span><span class="params">(object)</span>:</span></span><br><span class="line">__private = <span class="string">'PRIVATE'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        self.__name = name</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__func</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"__DogParent func"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 类方法</span></span><br><span class="line"><span class="meta">@classmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">change_name</span><span class="params">(cls, new_name)</span>:</span></span><br><span class="line">cls.__name = new_name</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_name</span><span class="params">(cls)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls.__name</span><br><span class="line"></span><br><span class="line"><span class="comment"># 普通方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">change_name2</span><span class="params">(self, new_name)</span>:</span></span><br><span class="line">        self.__name = new_name</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_name2</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.__name</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">DogParent.change_name(DogParent, <span class="string">"Tom2"</span>)</span><br><span class="line">print(DogParent.get_name(DogParent))</span><br><span class="line"></span><br><span class="line">DogParent.change_name2(<span class="string">"Tom3"</span>)</span><br><span class="line">print(DogParent.get_name2())</span><br></pre></td></tr></table></figure></p><h2 id="4-2-staticmethod"><a href="#4-2-staticmethod" class="headerlink" title="4.2 staticmethod"></a>4.2 staticmethod</h2><p>staticmethod不需要表示自身对象的self和自身类的cls参数，就跟使用普通的函数一样;这样有一个好处：</p><ul><li>有利于我们代码的优雅，把某些应该属于某个类的函数给放到那个类里去，同时有利于命名空间的整洁</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DogParent</span><span class="params">(object)</span>:</span></span><br><span class="line">__private = <span class="string">'PRIVATE'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        self.__name = name</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__func</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"__DogParent func"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 类方法</span></span><br><span class="line"><span class="meta">@classmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">change_name</span><span class="params">(cls, new_name)</span>:</span></span><br><span class="line">cls.__name = new_name</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_name</span><span class="params">(cls)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls.__name</span><br><span class="line"></span><br><span class="line"><span class="comment"># 普通方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">change_name2</span><span class="params">(self, new_name)</span>:</span></span><br><span class="line">        self.__name = new_name</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_name2</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.__name</span><br><span class="line"></span><br><span class="line"><span class="comment"># 静态方法</span></span><br><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_nickname</span><span class="params">(nickname)</span>:</span></span><br><span class="line">print(<span class="string">"nickname: &#123;&#125;"</span>.format(nickname))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">DogParent.set_nickname(<span class="string">"tom's nickname~"</span>)</span><br></pre></td></tr></table></figure><h2 id="4-3-property"><a href="#4-3-property" class="headerlink" title="4.3 property"></a>4.3 property</h2><p>@property 把一个方法伪装成一个属性,这个属性的值，是这个方法的返回值；这个方法不能有参数，类不能调用，只能对象调用。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, height, weight)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.height = height</span><br><span class="line">        self.weight = weight</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bmi</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.weight / (self.height ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">method</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"method"</span>)</span><br></pre></td></tr></table></figure></p><p>其实，property的作用不仅于此。简单点讲，@property的本质其实就是实现了get，set，delete三种方法。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, nickname)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.nickname = nickname</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">nickname</span><span class="params">(self)</span>:</span></span><br><span class="line"><span class="comment"># 相当于实现了get方法</span></span><br><span class="line">        print(<span class="string">"nickname: &#123;&#125;"</span>.self.nickname)</span><br><span class="line"></span><br><span class="line"><span class="meta">@property.setter</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nickname</span><span class="params">(self, new_nickname)</span>:</span></span><br><span class="line"><span class="comment"># 相当于实现了set方法</span></span><br><span class="line">self.nickname = new_nickname</span><br><span class="line">print(<span class="string">"new nickname: &#123;&#125;"</span>.format(new_nickname))</span><br><span class="line"></span><br><span class="line"><span class="meta">@property.deleter</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nickname</span><span class="params">(self)</span>:</span></span><br><span class="line"><span class="comment"># 相当于实现了delete方法</span></span><br><span class="line"><span class="keyword">del</span> Person.nickname</span><br><span class="line">print(<span class="string">"deleted nickname"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">person = Person(<span class="string">"Tom"</span>, <span class="string">'tommmy'</span>)</span><br><span class="line"><span class="comment"># get</span></span><br><span class="line">person.nickname()</span><br><span class="line"></span><br><span class="line"><span class="comment"># setter </span></span><br><span class="line">person.nickname = <span class="string">'new_tommmy'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># deleter</span></span><br><span class="line"><span class="keyword">del</span> person.nickname</span><br><span class="line"></span><br><span class="line"><span class="comment">#删除完毕后,再次调用报如下错误</span></span><br><span class="line"><span class="comment"># AttributeError: type object 'person' has no attribute 'nickname'</span></span><br><span class="line"><span class="comment"># person.nickname</span></span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;主要介绍了在python中，抽象类的定义、多态的概念、类中属性的封装以及类中常见的修饰器。&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术备忘" scheme="https://buracagyang.github.io/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/"/>
    
      <category term="Python" scheme="https://buracagyang.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>单层感知器为什么不能解决异或(XOR)问题</title>
    <link href="https://buracagyang.github.io/2019/05/29/single-layer-perceptron/"/>
    <id>https://buracagyang.github.io/2019/05/29/single-layer-perceptron/</id>
    <published>2019-05-29T09:05:20.000Z</published>
    <updated>2019-06-03T10:51:33.221Z</updated>
    
    <content type="html"><![CDATA[<p>同步于<a href="https://blog.csdn.net/buracag_mc" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/" target="_blank" rel="noopener">音尘杂记</a></p><p>单层感知器为什么不能解决异或问题(XOR)问题？给出两个思路去考虑这个小问题~</p><a id="more"></a><p>最近翻到了自己在印象笔记中学习记录的一些知识点，后续准备系统地整理放在自己的博客上，还请各位不吝指教。</p><h1 id="1-感知器模型"><a href="#1-感知器模型" class="headerlink" title="1. 感知器模型"></a>1. 感知器模型</h1><ul><li><p>感知器模型是美国学者罗森勃拉特（Frank Rosenblatt）为研究大脑的存储、学习和认知过程而提出的一类具有自学习能力的神经网络模型，它把神经网络的研究从纯理论探讨引向了从工程上的实现。</p></li><li><p>Rosenblatt提出的感知器模型是一个只有单层计算单元的前向神经网络，称为单层感知器。</p></li></ul><h1 id="2-单层感知器模型算法概述"><a href="#2-单层感知器模型算法概述" class="headerlink" title="2. 单层感知器模型算法概述"></a>2. 单层感知器模型算法概述</h1><p>在学习基础的NN知识的时候，单个神经元的结构必定是最先提出来的，单层感知器模型算法与神经元结构类似；</p><p>大概思想是：首先把<strong>连接权</strong>和<strong>阈值</strong>初始化为较小的非零随机数，然后把有n个连接权值的输入送入网络，经加权运算处理，得到的输出如果与所期望的输出有较大的差别(对比神经元模型中的激活函数)，就对连接权值参数进行自动调整，经过多次反复，直到所得到的输出与所期望的输出间的差别满足要求为止。</p><p>如下为简单起见，仅考虑只有一个输出的简单情况。设$x_i(t)$是时刻$t$感知器的输入（i=1,2,……,n），$ω_i(t)$是相应的连接权值，$y(t)$是实际的输出，$d(t)$是所期望的输出，且感知器的输出或者为1，或者为0。</p><h1 id="3-线性不可分问题"><a href="#3-线性不可分问题" class="headerlink" title="3. 线性不可分问题 "></a>3. 线性不可分问题 </h1><p>单层感知器不能表达的问题被称为线性不可分问题。 1969年，明斯基证明了“异或”问题是线性不可分问题。</p><h1 id="4-“与”、”或”、”非”问题的证明"><a href="#4-“与”、”或”、”非”问题的证明" class="headerlink" title="4. “与”、”或”、”非”问题的证明"></a>4. “与”、”或”、”非”问题的证明</h1><ul><li>由于单层感知器的输出为：</li></ul><p>$$ y(x1,x2) = f(ω1 <em> x1 + ω2 </em> x2 - θ) $$</p><p>所以，用感知器实现简单逻辑运算的情况如下：</p><ul><li><p>“与”运算（And, x1∧x2）<br>令 ω1 = ω2 = 1，θ = 1.5，则: y = f(1 <em> x1 + 1 </em> x2 - 1.5)<br>显然，当x1和x2均为1时，y的值1；而当x1和x2有一个为0时，y的值就为0.</p></li><li><p>“或”运算（Or, x1∨x2）<br>令ω1 = ω2=1, θ = 0.5，则: y=f(1 <em> x1 + 1 </em> x2 - 0.5)<br>显然，只要x1和x2中有一个为1，则y的值就为1；只有当x1和x2都为0时，y的值才为0。</p></li><li><p>“非”运算（Not, ～X1）<br>令ω1 = -1， ω2 = O， θ = -0.5，则:   y = f((-1) <em> x1 + 1 </em> x2 + 0.5)<br>显然，无论x2为何值，x1为1时，y的值都为0；x1为0时，y的值为1。即y总等于～x1。</p></li><li><p>“异或”运算（x1 XOR x2）</p></li></ul><h1 id="5-“异或”问题的证明"><a href="#5-“异或”问题的证明" class="headerlink" title="5. “异或”问题的证明"></a>5. “异或”问题的证明</h1><h2 id="5-1-单层感知机不能解决”异或”问题证明方法一"><a href="#5-1-单层感知机不能解决”异或”问题证明方法一" class="headerlink" title="5.1 单层感知机不能解决”异或”问题证明方法一"></a>5.1 单层感知机不能解决”异或”问题证明方法一</h2><p>如果“异或”（XOR）问题能用单层感知器解决，则由XOR的真值映射关系如下：</p><table><thead><tr><th style="text-align:center">(x1, x2)</th><th style="text-align:center">y</th></tr></thead><tbody><tr><td style="text-align:center">(0, 0)</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">(0, 1)</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">(1, 0)</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">(1, 1)</td><td style="text-align:center">0</td></tr></tbody></table><p>则ω1、 ω2 和θ 必须满足如下方程组：<br>1). ω1 + ω2 - θ ＜ 0    –&gt;   θ &gt; ω1 + ω2<br>2). ω1 + 0 - θ ≥ 0      –&gt;   0 ≥ θ - ω1<br>3). 0 + 0 - θ ＜ 0      –&gt;   θ &gt; 0<br>4). 0 + ω2 - θ ≥ 0      –&gt;   0 ≥ θ - ω2<br>显然，该方程组是矛盾的，无解！这就说明单层感知器是无法解决异或问题的。</p><h2 id="5-2-单层感知机不能解决”异或”问题证明方法二"><a href="#5-2-单层感知机不能解决”异或”问题证明方法二" class="headerlink" title="5.2 单层感知机不能解决”异或”问题证明方法二"></a>5.2 单层感知机不能解决”异或”问题证明方法二</h2><p>首先需要证明以下定理：</p><blockquote><p>样本集线性可分的充分必要条件是正实例点集所构成的凸壳与负实例点集所构成的凸壳互不相交    </p></blockquote><ul><li><p>必要性：假设样本集T线性可分，则存在一个超平面W将数据集正实例点和负实例点完全正确地划分到超平面两侧。显然两侧的点分别构成的凸壳互不相交；</p></li><li><p>充分性：假设存在两个凸壳A、B相交，且存在超平面W将A和B线性分割，令A在B的凸壳内部的点为a，因为线性可交，则A中不存在两点之间的连线与超平面W相交，而凸壳B中任意一点与A中的点的连线均与超平面W相交，则B内部的点a也与A中任一点之间的连线不与W相交，与B壳中任一点与A中的点的连线均与超平面W相交矛盾。</p></li></ul><p><strong>故：只有正负实例点所构成的两个凸壳不相交时样本集才线性可分。</strong></p><p>显然，对于此例，负实例样本集[(0, 0), (1, 1)] 和 正实例样本集[(0, 1), (1, 0)]是二维中是不能被线性分割的。<br><img src="/2019/05/29/single-layer-perceptron/1.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;单层感知器为什么不能解决异或问题(XOR)问题？给出两个思路去考虑这个小问题~&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术备忘" scheme="https://buracagyang.github.io/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/"/>
    
      <category term="算法备忘" scheme="https://buracagyang.github.io/tags/%E7%AE%97%E6%B3%95%E5%A4%87%E5%BF%98/"/>
    
  </entry>
  
  <entry>
    <title>AIC和BIC相关知识</title>
    <link href="https://buracagyang.github.io/2019/05/29/aic-and-bic/"/>
    <id>https://buracagyang.github.io/2019/05/29/aic-and-bic/</id>
    <published>2019-05-29T07:14:58.000Z</published>
    <updated>2019-06-11T09:48:46.932Z</updated>
    
    <content type="html"><![CDATA[<p>同步于<a href="https://blog.csdn.net/buracag_mc" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/" target="_blank" rel="noopener">音尘杂记</a></p><p>前面在回顾<a href="https://github.com/scikit-learn/scikit-learn" target="_blank" rel="noopener">sklearn</a>时，在广义线性模型中看到选择模型时可以采用AIC和BIC准则，特地复习了下统计学基础，简记如下，以抛砖引玉。</p><a id="more"></a><h2 id="1-模型拟合优度检验"><a href="#1-模型拟合优度检验" class="headerlink" title="1. 模型拟合优度检验"></a>1. 模型拟合优度检验</h2><p>最基础的一个模型拟合优度的检验量就是R square(方程的确定系数)。<br>已知一组样本观测值 $(X_i, Y_i)$,其中i=1,2,3,…,n得到如下样本回归方程：<br>$$<br>\hat{Y_i} = \hat{\beta_0} + \hat{\beta_1}X_i<br>$$<br>而Y的第i个观测值与样本均值的离差 $y_i = Y_i - \bar{Y}$，其可以分解为两部分之和：<br>$$<br>y_i = Y_i - \bar{Y} = (Y_i - \hat{Y_i}) + (\hat{Y_i} - \bar{Y}) = e_i + \hat{y_i}<br>$$<br>其中 $\hat{y_i} = (\hat{Y_i} - \bar{Y})$是样本拟合值与观测值的平均值之差，可认为是由回归直线解释的部分，通常称之为”离差”；</p><p>$e_i = (Y_i - \hat{Y_i})$是实际观测值与回归拟合值之差，是回归直线不能解释的部分，通常称之为”残差”。</p><p>如果 $Y_i = \hat{Y_i}$,即实际观测值落在样本回归”线”上，则拟合最好。</p><p>对于所有样本点，<strong>可以证明</strong>：<br>$$<br>\sum{y_i}^2 = \sum{\hat{y_i}^2} + \sum{e_i^2} + 2\sum{\hat{y_i}^2e_i} = \sum{\hat{y_i}^2} + \sum{e_i^2}<br>$$<br>记:<br>$TSS = \sum{y_i^2} = \sum{(Y_i - \bar{Y})^2}$为总体平方和(Total Sum of Squares)<br>$ESS = \sum{\hat{y_i}^2} = \sum{(\hat{Y_i} - \bar{Y})^2}$为回归平方和(Explained Sum of Squares, <strong>注意有的教材又称之为Regression Sum of Squares</strong>)<br>$RSS = \sum{e_i^2} = \sum{(Y_i - \hat{Y_i})^2}$为残差平方和(Residual Sum of Squares, <strong>注意有的教材又称之为Error Sum of Squares</strong>)<br>$$<br>TSS = ESS + RSS<br>$$<br>所以Y的观测值围绕其均值的总离差(total variation)可分解为两部分：一部分来自回归线(ESS)，另一部分则来自与随机误差(RSS)</p><blockquote><p>在给定样本中，TSS不变，如果实际观测点离样本回归线越近，则ESS在TSS中占的比重越大，因此定义<strong>拟合优度：回归平方和ESS与TSS的比值。</strong></p></blockquote><p>记 $R^2 = \frac{ESS}{TSS} = 1 - \frac{RSS}{TSS}$，称 $R^2$为(样本)可决系数/判定系数</p><p>对于回归方程来说，$R^2$有以下几个意义：</p><ol><li>R square可以作为选择不同模型的标准。在拟合数据之前，不能确定数据的确定模型关系，可以对变量的不同数学形式进行拟合，再看R square的大小。</li><li>在数据的关系存在非线性可能情况下：<br>a) R squared越大不一定拟合越好；<br>b) 如何一个模型的R square很小，不一定代表数据之间没有关系，而很有可能是选择的模型不对，或者存在有其他的函数关系。</li><li><strong>当自变量个数增加时，尽管有的自变量与的线性关系不显著，其R square也会增大</strong>，对于这种情况需采用Adjusted R squared进行调整。</li></ol><h2 id="2-调整R-square"><a href="#2-调整R-square" class="headerlink" title="2. 调整R square"></a>2. 调整R square</h2><p>由于在模型中增加变量时，$R^2$没有下降，所以存在一种过度拟合模型的内在趋势，即向模型中增加变量固然可以改善数据拟合程度，但这样也会导致预测的方差正大，这时就需要用到调整 $R^2$。<br>$$<br>\bar{R_2} = 1 - \frac{n-1}{n-k}(1-R^2)<br>$$<br>调整$R^2$用作拟合优度的度量，它能够适当消除在模型中增加变量所导致的自由度损失。</p><p>调整 $R^2$对模型扩张时自由度的损失进行了弥补，但又存在一个问题，随着样本容量的增大，这种弥补是否足以保证该准则肯定能让分析者得到正确的模型，所以提出了另外两个拟合度量指标，一个是赤池信息准则(Akaike Information Criterion, AIC)，另一个是施瓦茨或贝叶斯信息准则(Bayesian Information Criterion,BIC)。</p><h2 id="3-AIC和BIC"><a href="#3-AIC和BIC" class="headerlink" title="3. AIC和BIC"></a>3. AIC和BIC</h2><p>$$<br>AIC(K) = s_y^2(1-R^2)e^{2k/n}<br>$$</p><p>$$<br>BIC(K) = s_y^2(1-R^2)n^{k/n}<br>$$</p><p>$s_y^2$中没有对自由度进行修正，虽然随着$R^2$的提高，这两个指标都有所改善(下降),但在其他条件不变的情况下，模型规模扩大又会使这两个指标恶化。与$\bar{R^2}$一样，实现同样的拟合程度，这些指标在平均每次观测使用参数个数(K/n)较少时更有效。使用对数通常更方便，多数统计软件报告度量指标是：<br>$$<br>AIC(K) = ln(\frac{e^{\prime}e}{n}) + \frac{2K}{n}<br>$$</p><p>$$<br>BIC(K) = ln(\frac{e^{\prime}e}{n}) + \frac{Kln{n}}{n}<br>$$</p><p><u><strong>更一般地：</strong></u><br>$$<br>AIC(K) = 2K - 2ln(L)<br>$$<br>其中k是模型参数个数，L为似然函数。从一组可供选择的模型中选择最佳模型时，通常选择AIC最小的模型。</p><p>当两个模型之间存在较大差异时，差异主要体现在似然函数项，当似然函数差异不显著时，上市第一项，即模型复杂度则起作用，从而参数个数少的模型是较好的选择。</p><p>一般而言，当模型复杂度提高(k增大)时，似然函数L也会增大，从而使AIC变小，但是k过大时，似然函数增速减缓，导致AIC增大，模型过于复杂容易造成过拟合现象。目标是选取AIC最小的模型，AIC不仅要提高模型拟合度(极大似然)，而且引入了惩罚项，使模型参数尽可能少，有助于降低过拟合的可能性。<br>$$<br>BIC(K) = Kln{n} - 2ln(L)<br>$$<br>其中k是模型参数个数，n为样本数量，L为似然函数。与AIC类似地，引入了模型参数个数作为惩罚项，但是<strong>BIC的惩罚项比AIC的大</strong>，考虑了样本数量，样本数量过多时，可有效防止模型精度过高造成的模型复杂度过高；其中 $kln{n}$惩罚项在维度过大且训练样本数据相对较少的情况下，可以有效避免出现维度灾难现象。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;前面在回顾&lt;a href=&quot;https://github.com/scikit-learn/scikit-learn&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;sklearn&lt;/a&gt;时，在广义线性模型中看到选择模型时可以采用AIC和BIC准则，特地复习了下统计学基础，简记如下，以抛砖引玉。&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术备忘" scheme="https://buracagyang.github.io/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/"/>
    
      <category term="统计学运用" scheme="https://buracagyang.github.io/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E8%BF%90%E7%94%A8/"/>
    
  </entry>
  
  <entry>
    <title>利用numpy.vectorize提升计算速度</title>
    <link href="https://buracagyang.github.io/2019/05/09/numpy-vectorize/"/>
    <id>https://buracagyang.github.io/2019/05/09/numpy-vectorize/</id>
    <published>2019-05-09T09:11:56.441Z</published>
    <updated>2019-06-12T12:45:42.710Z</updated>
    
    <content type="html"><![CDATA[<hr><p>同步于<a href="https://blog.csdn.net/buracag_mc/article/details/88748607" title="https://blog.csdn.net/buracag_mc/article/details/88748607" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/2019/03/18/increase-calculation-speed-with-numpy-vectorize/" target="_blank" rel="noopener">音尘杂记</a></p><p>在实际项目中，对超大矩阵进行计算或者对超大的DataFrame进行计算是一个经常会出现的场景。这里先不考虑开发机本身内存等客观硬件因素，仅从设计上讨论一下不同实现方式带来的性能差异，抛砖引玉。</p><a id="more"></a><p>项目中有这样一个需求，需要根据历史销量数据计算SKU(Stock Keeping Unit)之间的相似度，或者更通俗一点说是根据历史销量数据求不同SKU之间出现的订单交集以及并集大小(注:SKU数量大概15k左右，订单数大概1000k左右)。</p><p>这里给几条示例数据，可以更直观形象地理解这个需求：</p><p><img src="/2019/05/09/numpy-vectorize/1.png" alt="1"></p><p>然后需要根据这些历史的orderno-sku(订单-商品)数据求解出sku的相似度矩阵。其中SKU1和SKU2之间的相似度定义为:</p><p><img src="/2019/05/09/numpy-vectorize/2.png" alt="2"></p><p>可以很快速地想到几种解决方案：</p><ul><li><p>直接for loops；</p></li><li><p>for loops稍微改进采用列表生成器；</p></li><li><p>采用多进程并行计算；</p></li><li><p><strong>采用numpy.vectorize</strong></p></li></ul><h1 id="1-for-loops计算相似度矩阵"><a href="#1-for-loops计算相似度矩阵" class="headerlink" title="1.for loops计算相似度矩阵"></a>1.for loops计算相似度矩阵</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@timer</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_corr_matrix_for_loops</span><span class="params">(order_df)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    for loops计算相似度矩阵</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    df = order_df.groupby([<span class="string">'sku'</span>]).agg(&#123;<span class="string">'orderno'</span>: <span class="keyword">lambda</span> x: set(x)&#125;).reset_index()</span><br><span class="line">    <span class="keyword">del</span> order_df</span><br><span class="line">    gc.collect()</span><br><span class="line">    l = len(df)</span><br><span class="line">    sku_series = df.sku.astype(str)</span><br><span class="line">    corr_matrix_arr = np.ones((l, l))</span><br><span class="line"></span><br><span class="line">    tbar = trange(l)</span><br><span class="line">    tbar.set_description(<span class="string">"compute corr matrix"</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tbar:</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i + <span class="number">1</span>, l):</span><br><span class="line">            corr_matrix_arr[j, i] = corr_matrix_arr[i, j] = len(df.iloc[i, <span class="number">1</span>] &amp; df.iloc[j, <span class="number">1</span>]) / len(</span><br><span class="line">                df.iloc[i, <span class="number">1</span>] | df.iloc[j, <span class="number">1</span>])</span><br><span class="line">    corr_matrix_df = pd.DataFrame(columns=sku_series, index=sku_series, data=corr_matrix_arr)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> corr_matrix_df</span><br></pre></td></tr></table></figure><p>计算耗时：2000s+<br><img src="/2019/05/09/numpy-vectorize/3.png" alt="3"></p><h1 id="2-list-generator计算相似度矩阵"><a href="#2-list-generator计算相似度矩阵" class="headerlink" title="2.list generator计算相似度矩阵"></a>2.list generator计算相似度矩阵</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@timer</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_corr_matrix_generator</span><span class="params">(order_df)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    列表生成器计算相似度矩阵</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    df = order_df.groupby([<span class="string">'sku'</span>]).agg(&#123;<span class="string">'orderno'</span>: <span class="keyword">lambda</span> x: set(x)&#125;).reset_index()</span><br><span class="line">    <span class="keyword">del</span> order_df</span><br><span class="line">    gc.collect()</span><br><span class="line">    l= len(df)</span><br><span class="line">    sku_series = df.sku.astype(str)</span><br><span class="line">    corr_matrix_arr = np.ones((l, l))</span><br><span class="line"></span><br><span class="line">    l1 = df.orderno</span><br><span class="line">    l2 = np.array(df[<span class="string">'orderno'</span>].apply(len), dtype=np.int8)</span><br><span class="line"></span><br><span class="line">    result_list = [[i, j, len(l1[i] &amp; l1[j])] <span class="keyword">for</span> i <span class="keyword">in</span> range(l)</span><br><span class="line">                   <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>, l) <span class="keyword">if</span> len(l1[i] &amp; l1[j]) &gt; <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, j, k <span class="keyword">in</span> result_list:</span><br><span class="line">        corr_matrix_arr[j, i] = corr_matrix_arr[i, j] = k * <span class="number">1.0</span> / (l2[i] + l2[j] - k)</span><br><span class="line">    corr_matrix_df = pd.DataFrame(columns=sku_series, index=sku_series, data=corr_matrix_arr)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> corr_matrix_df</span><br></pre></td></tr></table></figure><p>计算耗时：1296s<br><img src="/2019/05/09/numpy-vectorize/4.png" alt="4"></p><h1 id="3-多进程计算相似度矩阵"><a href="#3-多进程计算相似度矩阵" class="headerlink" title="3.多进程计算相似度矩阵"></a>3.多进程计算相似度矩阵</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@timer</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_corr_matrix_multiprocessing</span><span class="params">(order_df)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    多进程计算相似度矩阵</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    df = order_df.groupby([<span class="string">'sku'</span>]).agg(&#123;<span class="string">'orderno'</span>: <span class="keyword">lambda</span> x: set(x)&#125;).reset_index()</span><br><span class="line">    <span class="keyword">del</span> order_df</span><br><span class="line">    gc.collect()</span><br><span class="line">    l = len(df)</span><br><span class="line">    sku_series = df.sku.astype(str)</span><br><span class="line">    </span><br><span class="line">    l1 = df.orderno</span><br><span class="line">    l2 = np.array(df[<span class="string">'orderno'</span>].apply(len), dtype=np.int8)</span><br><span class="line">    <span class="keyword">del</span> df</span><br><span class="line">    gc.collect()</span><br><span class="line"></span><br><span class="line">    arr2 = np.zeros((l, l), dtype=np.float32)</span><br><span class="line">    pairs = [[i, j] <span class="keyword">for</span> i <span class="keyword">in</span> range(l - <span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> range(i + <span class="number">1</span>, l)]</span><br><span class="line"></span><br><span class="line">    loops = int(math.ceil((l ** <span class="number">2</span> - l) / <span class="number">10</span> ** <span class="number">6</span> / <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">    tbar = trange(loops)</span><br><span class="line">    tbar.set_description(<span class="string">"compute corr matrix"</span>)</span><br><span class="line">    pool = Pool(<span class="number">4</span>)</span><br><span class="line">    <span class="keyword">for</span> loop <span class="keyword">in</span> tbar:</span><br><span class="line">        temp_lists = [[i, j, l1[i], l1[j]] <span class="keyword">for</span> i, j <span class="keyword">in</span> pairs[(<span class="number">10</span> ** <span class="number">6</span> * loop): (<span class="number">10</span> ** <span class="number">6</span> * (loop + <span class="number">1</span>))]]</span><br><span class="line">        temp_results = pool.map(cal, temp_lists)</span><br><span class="line">        <span class="keyword">for</span> i, j, k <span class="keyword">in</span> temp_results:</span><br><span class="line">            arr2[i, j] = k</span><br><span class="line">    pool.close()</span><br><span class="line">    pool.join()</span><br><span class="line"></span><br><span class="line">    arr1 = l2 + l2.reshape((l, <span class="number">1</span>))</span><br><span class="line">    arr2 = arr2 + arr2.T  <span class="comment"># 变对称阵</span></span><br><span class="line">    arr3 = arr2 / (arr1 - arr2) + np.eye(l)</span><br><span class="line">    <span class="keyword">del</span> arr1</span><br><span class="line">    <span class="keyword">del</span> arr2</span><br><span class="line">    gc.collect()</span><br><span class="line"></span><br><span class="line">    corr_matrix_df = pd.DataFrame(columns=sku_series, index=sku_series, data=arr3)</span><br><span class="line">    <span class="keyword">return</span> corr_matrix_df</span><br></pre></td></tr></table></figure><p>计算耗时：1563s<br><img src="/2019/05/09/numpy-vectorize/5.png" alt="5"></p><h1 id="4-numpy-vectorize计算相似度矩阵"><a href="#4-numpy-vectorize计算相似度矩阵" class="headerlink" title="4.numpy.vectorize计算相似度矩阵"></a>4.numpy.vectorize计算相似度矩阵</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@timer</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_corr_matrix_vectorize</span><span class="params">(order_df)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    numpy.vectorice计算相似度矩阵</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    df = order_df.groupby([<span class="string">'sku'</span>]).agg(&#123;<span class="string">'orderno'</span>: <span class="keyword">lambda</span> x: set(x)&#125;).reset_index()</span><br><span class="line">    l = len(df)</span><br><span class="line">    sku_series = df.sku.astype(str)</span><br><span class="line">    arr = df.orderno.values</span><br><span class="line">    corr_matrix_arr = np.zeros((l, l))</span><br><span class="line">    f_vec = np.vectorize(len)</span><br><span class="line">    arr1 = f_vec(arr)</span><br><span class="line"></span><br><span class="line">    tbar = trange(l - <span class="number">1</span>)</span><br><span class="line">    tbar.set_description(<span class="string">"compute corr matrix"</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tbar(l - <span class="number">1</span>):</span><br><span class="line">        corr_matrix_arr[i, (i + <span class="number">1</span>): l] = f_vec(arr[(i + <span class="number">1</span>): l] &amp; arr[i])</span><br><span class="line">    corr_matrix_arr1 = np.add.outer(arr1, arr1)</span><br><span class="line">    temp = corr_matrix_arr / (corr_matrix_arr1 - corr_matrix_arr)</span><br><span class="line">    temp = temp + temp.T + np.eye(l)</span><br><span class="line">    <span class="keyword">return</span> pd.DataFrame(columns=sku_series, index=sku_series, data=temp)</span><br></pre></td></tr></table></figure><p>计算耗时：72s<br><img src="/2019/05/09/numpy-vectorize/6.png" alt="6"></p><p>可以看到，使用numpy.vectorize提升了20倍左右！</p><p><strong>思考：</strong><br>结合到实际业务中，其实有很多可以改进的地方：1. 并不需要计算所有SKU之间的相似度（提速）; 2. 可以只保存上三角阵或保存有效的相似SKU数据(降低内存)。这块儿就不展开赘述了。</p>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc/article/details/88748607&quot; title=&quot;https://blog.csdn.net/buracag_mc/article/details/88748607&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/2019/03/18/increase-calculation-speed-with-numpy-vectorize/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在实际项目中，对超大矩阵进行计算或者对超大的DataFrame进行计算是一个经常会出现的场景。这里先不考虑开发机本身内存等客观硬件因素，仅从设计上讨论一下不同实现方式带来的性能差异，抛砖引玉。&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术备忘" scheme="https://buracagyang.github.io/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/"/>
    
      <category term="Python" scheme="https://buracagyang.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>SVM推导过程注解</title>
    <link href="https://buracagyang.github.io/2019/05/08/svm-proving-process/"/>
    <id>https://buracagyang.github.io/2019/05/08/svm-proving-process/</id>
    <published>2019-05-08T09:51:18.299Z</published>
    <updated>2019-06-03T10:51:36.385Z</updated>
    
    <content type="html"><![CDATA[<hr><p>同步于<a href="https://blog.csdn.net/buracag_mc/article/details/76762249" title="https://blog.csdn.net/buracag_mc/article/details/76762249" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/2019/03/18/svm-process/" target="_blank" rel="noopener">音尘杂记</a></p><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>支持向量机(Support Vector Machine)的原理其实比较简单，它是基于结构风险最小化理论之上在特征空间中建构最优分割超平面。在二维中就是线，在三维中就是面，但我们统称为超平面。</p><a id="more"></a><p>就我所看到的相关书本、论文以及网上博文情况来看，其一般步骤通常如下：</p><ul><li>在二维平面中的线性可分情况开始讲解，求解硬间隔最优化</li><li>随后放宽条件，这时可以引入松弛向量，然后求解软间隔最优化</li><li>再后面拓展到线性不可分的情况，这时引入核函数方法（kernel trick），将低维数据映射到高维特征空间，在高维特征空间中，这些训练样本便是线性可分的了。</li></ul><p>SVM在数据挖掘与统计机器学习的书中是必讲的，网上优秀的教程也很多；故这里我只是将某些一笔带过或者模棱两可的推导步骤结合自己学习过程做一些补充，错误与不尽之处还望大家不吝指教！欢迎大家使劲儿拍砖耶！</p><h1 id="求解硬间隔最优化时的相关注解"><a href="#求解硬间隔最优化时的相关注解" class="headerlink" title="求解硬间隔最优化时的相关注解"></a>求解硬间隔最优化时的相关注解</h1><ul><li><p>首先我们回忆一下初中所学的知识,两条平行线的方程分别为：<br>$ax + by = c1$<br>$ax + by = c2$           (1)<br>两条平行线的距离d为：<br>$ d = \frac{|c_1-c_2|}{\sqrt(a^2+b^2)} $ (2)</p></li><li><p>范数(norm)相关知识：<br>p-范数 $||X||_p = (|x_1|^p + |x_2|^p+…+ |x_n|^p)^{1/p}$;也即:</p><ul><li><p>1-范数 =$|x_1| + |x_2|+…+ |x_n|$</p></li><li><p>2-范数 =$ (|x_1|^2 + |x_2|^2 + …+|x_n|^2)^{1/2}$</p></li><li><p>$\infty-范数 = MAX(|x_1|, |x_2|, …, |x_n|)$</p></li></ul></li></ul><p>跟博文<a href="http://blog.csdn.net/buracag_mc/article/details/75159437" title="http://blog.csdn.net/buracag_mc/article/details/75159437" target="_blank" rel="noopener">http://blog.csdn.net/buracag_mc/article/details/75159437</a>中所讲的闵可夫斯基距离是否有些似曾相识；的确是这样的，p-范数确实满足范数的定义。其中三角不等式的证明不是平凡的，这个结论通常称为闵可夫斯基不等式。</p><p>其中2-范数简单记为||X||,也就是我们通常意义上所说的欧式距离！</p><p>先描述一下，假设我们有N个训练样本${(x_1, y_1),(x_1, y_1), …, (x_n, y_n)}$，x是2维向量，而$y_i \in {+1, -1}$是训练样本的标签，分别代表两个不同的类。这里我们需要用这些样本去训练学习一个线性分类器：$f(x)=sgn(w^Tx + b)$，sgn函数就是一个符号函数，也就是说$w^Tx+ b$大于0的时候，输出f(x) = 1，小于0的时候，f(x) = -1。而$w^Tx + b=0$就是我们要寻找的分类超平面，如下图所示：<br><img src="http://img.blog.csdn.net/20170806110734659?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYnVyYWNhZ19tYw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p>我们需要这个超平面分隔这两类的效果最好，也就是说让这个超平面到这两个类的最近的那个样本的距离相同且最大。为了更好的说明，找到两个和这个超平面平行和距离相等的超平面，其实在平面几何中我们知道这就是平行线的移动，OK,如果各移动m个单位就达到要求，即：</p><p>$H_1: y = w^Tx + b=m$<br>$H_2: y = w^Tx + b=-m$</p><p>形式是不跟教材中的不一样？没关系，这里我们只是需要方程两边同时除以一个m即可：</p><p>$H_1: y = (\frac{w}{m})^Tx + \frac{b}{m}=1$<br>$H_2: y = (\frac{w}{m})^Tx + \frac{b}{m}=-1$(4)</p><p>这里为了统一起见，我们令w = w/m, b=b/m，注意与前面所说的$w^Tx + b=0$中的w和b是有区别的。(其实对于$w^Tx + b=0$,我们可以进行同样处理$H_1: y = (\frac{w}{m})^Tx + \frac{b}{m}=\frac{0}{m}$,再令w=w/m, b=b/m,即可完全统一了)</p><p>在H1左侧的函数值大于1，所有其分类为+1；在H2右侧的函数值小于1，所有其分类为-1，<br>可以统一记为记为$y_i(W^T.x_i + b) \geq 1$<br>这样便是我们熟悉的形式了！</p><hr><p>下面大家便可以猜想到了，求$H_1$和$H_2$之间的最大距离。当然如果是在二维平面中(当然，这里是以二维特征来说的，当然就是二维平面了)，易知便是两条平行线之间的距离，根据前面所所述平行线的距离即可求出，这里我们称之为margin。<br>即：margin = 2/||W||<br>这里对于二维特征$W^T = (w_1,w_2)$，||W||便是参数W的二范数(有的教科书又称之“模”)，将上式展开表示我们熟悉的平行线的距离了$margin = \frac{2}{\sqrt(w_1^2 + w_2^2)}$</p><p>但是，在统计机器学习中，我们要让它符合更多一般的情况，美其名曰便是“泛化能力”。将特征空间拓展到多维的情况，便是用向量来进行表示了，故在多维特征空间中，我们同样求margin= 2/||W||。</p><p>要使margin最大，即需W最小，故我们设我们的目标函数：<br>$min \frac{1}{2}||W||^2$<br>$s.t. yi(W^Tx_i + b) \geq 1, \forall x_i$                                              (5)</p><p>很多人会纠结W前面的系数1/2，这里加不加1/2其实没关系，这是为了求导时消去。其实在机器学习中， 我们常见的平方损失函数便是进行了同样的处理，在前面加了个常数系数1/2。</p><p>对于(5)式，准确的讲这是一个带有不等式约束的条件极值问题，根据高等数学和基础运筹学内容可以知道，我们可以用<strong>拉格朗日方法求解</strong>。</p><p>这里我必须要补充的一点是：通过查阅教科书以及在阅读网上的优秀教程，我发现不同教科书和网上不同的教程都有不同的说法，虽然实质是不变的，但当时我遇到的坑必须给大家给填了。</p><p>首先带不等式约束的条件极值问题中会有大于号约束、小于号约束两种(这里我们暂且先不说带等号，下文将KKT条件的时候一并补充)</p><ul><li><p>第一种说法如下：将所有不等式约束条件<strong>统一为小于号约束</strong>，然后拉格朗日方程的构建规则是用约束方程乘以非负的拉格朗日系数，然后再<strong>加上</strong>目标函数即可。</p></li><li><p>第二种说法如下：将所有不等式约束条件<strong>统一为大于号约束</strong>，然后拉格朗日方程的构建规则是用约束方程乘以非负的拉格朗日系数，然后再从目标函数中<strong>减去</strong>即可。</p></li></ul><p>其实我们可以发现这两种说法是等价的！事实确实如此，但是很多博文在讲解拉格朗日函数的构建时要么说用目标函数加上约束方程乘以非负的拉格朗日系数，要么说用目标函数减去约束方程乘以非负的拉格朗日系数。</p><p>可能某些文章作者完全没有申明大前提，他们准确的说法应该是，<strong><em>当统一成小于号约束时，拉格朗日函数的构建时是用目标函数加上约束方程乘以非负的拉格朗日系数；当统一成大于号约束时，拉格朗日函数的构建时是用目标函数减去约束方程乘以非负的拉格朗日系数。</em></strong>在不提前申明不同的大前提下，可能会误导不细心以及课程学的不仔细的读者(当时包括我=_=！)，导致某些人纳闷了，咦，这个拉格朗日咋一会儿是加上约束约束乘以拉格朗日系数，一会儿又是减去约束方程乘以拉格朗日系数啊？？？</p><p>为了统一与方便说明起见，故下文我们运用的第一种规则，将不等式约束条件统一成小于号约束。于是得到拉格朗日方程如下：<br>$L(w,b,a) = \frac{1}{2}||W||^2 + \sum_{i=1}^{n}a_i(1-y_i(wx_i+b)) = \frac{1}{2}||W||^2 - \sum_{i=1}^{n}a_i(y_i(wx_i+b)) + \sum_{i=1}^{n}a_i $<br>(6)</p><p>拉格朗日函数构建好后接下来便是简单的求解问题了，分别对W和b求偏导数并令其为零，得到如下结果：<br>$W = \sum_{i=1}^{n}a_iy_ix_i$                                    (7)<br>$\sum_{i=1}^{n}a_iy_i = 0$                                                 (8)    </p><p>带入(6)式即可得到:<br>$Max.W(a) =\sum_{i=1}^{n}a_i - \frac{1}{2}\sum_{i=1,j=1}^{n}a_ia_jy_iy_jx_i^Tx_j$<br>$s.t. a_i \geq 0, \sum_{i=1}^{n}a_iy_i = 0$(9)</p><p>为什么$min \frac{1}{2}||W||^2$问题变成了<br>$Max.W(a) =\sum_{i=1}^{n}a_i - \frac{1}{2}\sum_{i=1,j=1}^{n}a_ia_jy_iy_jx_i^Tx_j$<br>当然是对偶问题的求解了！对偶问题是怎么推导过来的？很多文章仅仅只是一笔带过了这么重要的推导内容。。。导致很多人有些小困惑哈~，为什么构建拉格朗日函数后就将求最小化问题变成求最大化问题？OK，既然本文的定位是SVM推导过程中的解析及注解，必定是要把这个问题完整给推导清楚的。</p><h2 id="SVM中对偶问题的注解"><a href="#SVM中对偶问题的注解" class="headerlink" title="SVM中对偶问题的注解"></a>SVM中对偶问题的注解</h2><p>再回看(6)式，<br>$L(w,b,a) = \frac{1}{2}||W||^2 + \sum_{i=1}^{n}a_i(1-y_i(W^Tx_i+b)) = \frac{1}{2}||W||^2 - \sum_{i=1}^{n}a_i(y_i(W^Tx_i+b)) + \sum_{i=1}^{n}a_i $<br>$s.t. a_i \geq 0$</p><p>我们要处理的最优化问题最正确的表达形式其实为：<br>        <img src="http://img.blog.csdn.net/20170806113012070?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYnVyYWNhZ19tYw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述">            (10)<br>上式才是严格带有不等式约束条件下的拉格朗日条件极值的表达式。我读的很多介绍SVM的文章(包括我看的书本)都是没说的！(10)式便是一个凸规划问题。</p><p>其意义是先对a求偏导，令其等于0消掉a，然后再对W和b求L的最小值。</p><p>要直接求解(10)式是有难度的，幸好这个问题可以通过拉格朗日对偶问题来解决。常说对偶问题对偶问题，现在就是真正发挥这把利器的时候了。对(10)式做一个简单的等价变换：<br>                        <img src="http://img.blog.csdn.net/20170806113254715?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYnVyYWNhZ19tYw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述">(11)</p><p><strong>上式即为对偶变换</strong>，这样就把这个凸规划问题转换成了对偶问题</p><p>其意义是：原凸规划问题可以转化为先对W和b求偏导，令两个偏导数都等于0消掉W和b，然后再对a求L的最大值。与(10)的意义是相反的，或者说是对偶的！不知我讲到这步，大家是否对对偶问题有了一个豁然开朗的感觉——啊！原来对偶问题就是这啊！！</p><p>然后将求得的(7)式和(8)式带入(6)式，得：<br>                            <img src="http://img.blog.csdn.net/20170806113534514?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYnVyYWNhZ19tYw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述">        (12)<br>将(12)式带入(11)式得：<br>                <img src="http://img.blog.csdn.net/20170806113608420?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYnVyYWNhZ19tYw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述">            (13)<br>再考虑到(8)式，对偶问题的完整表达为：<br><img src="http://img.blog.csdn.net/20170806113656054?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYnVyYWNhZ19tYw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述">                            (14)</p><p>到了这一步，我们便可以直接用数值方法计算求解拉格朗日乘数a了。求得a过后根据(7)式可以得到W，然后根据超平面方程可以求出b。最终便得到了我们想要的超平面和分类决策函数，也就是我们训练好的SVM分类器。那么对于待分类样本X，其分类为为：<br>                                      <img src="http://img.blog.csdn.net/20170806113836750?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYnVyYWNhZ19tYw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述">    (15)</p><p>我们根据(15)式可以发现，对于一个待分类样本，我们先计算待分类样本和训练样本的内积然后加权就和再加上b值即可。训练样本特别大的情况下，如果对所有训练样本做运算是否太耗时了啊？很多教科书以及网上教程都是直接说根据KKT条件可知，只有支持向量的乘子(拉格朗日乘数)$a_i$不等于0，其他训练样本的乘子都为0，这样便会大大减少运算量，也是后面SVM引入核函数(kernel)的铺垫。这又会引起新的疑惑，为什么只有支持向量对应的乘子不为0呢？</p><h2 id="SVM中KKT条件注解"><a href="#SVM中KKT条件注解" class="headerlink" title="SVM中KKT条件注解"></a>SVM中KKT条件注解</h2><p>这里还是继续讨论一下带等式和不等式约束的条件极值问题。任何极值问题的约束条件不外乎3种：等式、大于号和小于号，为了统一起见，我们将不等式约束统一为小于号。<br>例如：<br>$min(max)    f(x) $<br>$s.t.     g_i(x) \leq0,i=1,2…n_1$<br>$     h_j(x) = 0,j=1,2…n_2$</p><p>那么一个极值优化问题我们转化为：<br><img src="http://img.blog.csdn.net/20170806114231142?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYnVyYWNhZ19tYw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><ul><li>KKT条件就是函数的最优值必须满足以下条件：<ul><li>L对各个x的偏导为零</li><li>h(x) = 0</li><li>$\sum_{i=1}^{n_1}a_ig_i(x) =0 , a_i\geq0$</li></ul></li></ul><p>假设一个目标函数，3个不等式约束条件把自变量约束在一定范围，而目标函数是在这个范围内寻找最优解。<br><img src="http://img.blog.csdn.net/20170806114343592?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYnVyYWNhZ19tYw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><ul><li><p>1.函数开始也不知道该取哪一个值是吧，假设某一次取得自变量集合为x1*，发现不满足约束，然后再换呀换；</p></li><li><p>2.假设到x2<em>满足约束条件，但是这个时候函数值不是最优的，并且x2</em>使得g1(x)与g2(x)等于0了，而g3(x)还是小于0。这个时候，我们发现在x2*的基础上再寻找一组更优解要靠谁呢？当然是要靠约束条件g1(x)与g2(x)，因为他们等于0了，很极限呀，一不小心，走错了就不满足这两个约束的条件了，这个时候我们会选择g1(x)与g2(x)的梯度方向往下走，以寻找最优值解。</p></li><li><p>3.这个时候需不需要管约束条件g3(x)呢？正常来说管不管都可以，如果管，也取g3在x2<em>处的梯度的话，由于g3已经满足小于0的条件，这时候再取在x2</em>处的梯度，有可能更快得到结果，也有可能适得其反；如果不管g3，由于g1和g2已经在边缘了，只取g1和g2的梯度，是肯定会让目标函数接近解的；故我们这时候是不用考虑g3的；</p></li><li><p>4.再往下走，到了x3*处发现g2和g3等于0了，也就是说走到边了，而g1是满足约束小于0的，这时候我们重复上一步，取g2和g3的梯度方向作为变化方向，而不用管g1.</p></li><li><p>5.一直循环3(4)步，直到找到最优解。</p></li></ul><p>可以看到的是，如果如果g1、g2=0时，由于他们本身的条件是小于0的，我们是需要优化他们的，操作上便是乘以一个正常数a作为他们梯度增长的倍数(或者说学习效率)，那些暂且不需要考虑的约束，例如这里说的g3，我们可以乘以系数0，即在下一次的优化中是不用考虑这些约束的。综上所述的话：<br>$\sum_{i=1}^{n_1}a_ig_i(x) = 0, a_i\geq0$</p><p>如上，简单直观地说便是KKT条件中第三个式子的意义了。</p><p>回到SVM的推导上来，对于(6)式，我们知道其KKT条件中的第三个式子为:<br>$\sum_{i=1}^{n_1}a_i(1-y_i(W^T.x_i+b)) = 0$，</p><p>我们知道除了支持向量，对于其他训练样本有：</p><ul><li><p>$y_i(W^T.x_i + b) &gt; 1$ 也即$1 - y_i(W^T.x_i + b) &lt;0$根据前面所述的内容知道，其对应的乘子为0。</p></li><li><p>对于支持向量来说：$y_i(W^T.x_i + b) =1$ 也即$1 - y_i(W^T.x_i + b) =0$，其对应的乘子不为0。</p></li></ul><p>也就是说，新来的待分类样本只需与支持向量求内积即可，这便大大减少了计算量！这便是KKT条件在SVM关键推导中的应用。</p><p>这里我再补偿一下另外一种思路，其实本质还是KKT条件：<br>由于(5)式与(10)式等价，即：<br><img src="http://img.blog.csdn.net/20170806114946494?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYnVyYWNhZ19tYw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述">        (16)</p><p>故要使(16)式成立，只有令$a_i(1-y_i(W^T.x_i+b)) = 0$成立，由此得到KKT的第三个条件：<br>$\sum_{i=1}^{n_1}a_i(1-y_i(W^T.x_i+b)) = 0$<br>同样可出结论：支持向量对应的乘子为正系数；如果一个样本不是支持向量，则其对应的乘子为0。</p><hr>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc/article/details/76762249&quot; title=&quot;https://blog.csdn.net/buracag_mc/article/details/76762249&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/2019/03/18/svm-process/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;支持向量机(Support Vector Machine)的原理其实比较简单，它是基于结构风险最小化理论之上在特征空间中建构最优分割超平面。在二维中就是线，在三维中就是面，但我们统称为超平面。&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术备忘" scheme="https://buracagyang.github.io/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/"/>
    
      <category term="统计学运用" scheme="https://buracagyang.github.io/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E8%BF%90%E7%94%A8/"/>
    
      <category term="算法备忘" scheme="https://buracagyang.github.io/tags/%E7%AE%97%E6%B3%95%E5%A4%87%E5%BF%98/"/>
    
  </entry>
  
</feed>
