<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Buracag的博客</title>
  <icon>https://www.gravatar.com/avatar/5d6a8fbb9f799ea7bec71b36b635ce18</icon>
  <subtitle>Beautiful is better than ugly.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://buracagyang.github.io/"/>
  <updated>2019-07-17T12:31:15.013Z</updated>
  <id>https://buracagyang.github.io/</id>
  
  <author>
    <name>Buracag</name>
    <email>15591875898@163.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>深广度搜索手写实现与networkx对比</title>
    <link href="https://buracagyang.github.io/2019/07/14/breadth-depth-first-search/"/>
    <id>https://buracagyang.github.io/2019/07/14/breadth-depth-first-search/</id>
    <published>2019-07-14T05:48:06.000Z</published>
    <updated>2019-07-17T12:31:15.013Z</updated>
    
    <content type="html"><![CDATA[<p>同步于<a href="https://blog.csdn.net/buracag_mc" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/" target="_blank" rel="noopener">音尘杂记</a></p><p>前面项目在做一个遍历搜索的时候，有用到深度/广度搜索的相关知识；原理很简单，不再拾人牙慧了哈；这篇文章主要是将我自己简单实现的深广度搜索分享出来并与Python <code>networkx</code>模块中的已有实现做一个简单对比。</p><p><img src="/2019/07/14/breadth-depth-first-search/DFS.gif" alt="DFS">  <img src="/2019/07/14/breadth-depth-first-search/BFS.gif" alt="BFS"></p><a id="more"></a><h1 id="1-手写实现"><a href="#1-手写实现" class="headerlink" title="1. 手写实现"></a>1. 手写实现</h1><h2 id="1-1-网络的定义"><a href="#1-1-网络的定义" class="headerlink" title="1.1 网络的定义"></a>1.1 网络的定义</h2><p>这一步最主要的属性是<code>node_neighbors</code>， 理解成与一个node有连接边(edge)的所有nodes。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Graph</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    实现一个最基础的网络结构</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, *args, **kwargs)</span>:</span></span><br><span class="line">        self.node_neighbors = &#123;&#125;  <span class="comment"># 邻居节点</span></span><br><span class="line">        self.visited = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_node</span><span class="params">(self, node)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> node <span class="keyword">not</span> <span class="keyword">in</span> self.nodes():</span><br><span class="line">            self.node_neighbors[node] = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_nodes</span><span class="params">(self, nodelist)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> nodelist:</span><br><span class="line">            self.add_node(node)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_edge</span><span class="params">(self, edge)</span>:</span></span><br><span class="line">        u, v = edge</span><br><span class="line">        <span class="keyword">if</span> (v <span class="keyword">not</span> <span class="keyword">in</span> self.node_neighbors[u]) <span class="keyword">and</span> (u <span class="keyword">not</span> <span class="keyword">in</span> self.node_neighbors[v]):</span><br><span class="line">            self.node_neighbors[u].append(v)</span><br><span class="line">            <span class="keyword">if</span> u != v:</span><br><span class="line">                self.node_neighbors[v].append(u)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_node_neighbors</span><span class="params">(self, node_neighbors)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> node_neighbors.items():</span><br><span class="line">            self.add_node(k)</span><br><span class="line">            <span class="keyword">for</span> l <span class="keyword">in</span> v:</span><br><span class="line">                self.add_node(l)</span><br><span class="line">                self.add_edge((k, l))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">nodes</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.node_neighbors.keys()</span><br></pre></td></tr></table></figure><h2 id="1-2-深度优先搜索"><a href="#1-2-深度优先搜索" class="headerlink" title="1.2 深度优先搜索"></a>1.2 深度优先搜索</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">depth_first_search</span><span class="params">(self, root=None)</span>:</span></span><br><span class="line">    order = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(node_now)</span>:</span></span><br><span class="line">        self.visited[node_now] = <span class="literal">True</span></span><br><span class="line">        order.append(node_now)</span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> self.node_neighbors[node_now]:</span><br><span class="line">            <span class="keyword">if</span> n <span class="keyword">not</span> <span class="keyword">in</span> self.visited:</span><br><span class="line">                dfs(n)</span><br><span class="line">    <span class="keyword">if</span> root:</span><br><span class="line">        dfs(root)</span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> self.nodes():</span><br><span class="line">        <span class="keyword">if</span> node <span class="keyword">not</span> <span class="keyword">in</span> self.visited:</span><br><span class="line">            dfs(node)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> order</span><br></pre></td></tr></table></figure><p>输出深度优先搜索的结果：[0, 1, 3, 4, 2, 5, 6]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># 手写实现</span></span><br><span class="line">    node_edges = &#123;<span class="number">0</span>: [<span class="number">1</span>, <span class="number">2</span>], <span class="number">1</span>: [<span class="number">3</span>, <span class="number">4</span>], <span class="number">2</span>: [<span class="number">5</span>, <span class="number">6</span>]&#125;</span><br><span class="line">    root = <span class="number">0</span></span><br><span class="line">    g = Graph()</span><br><span class="line">    g.add_node_neighbors(node_edges)</span><br><span class="line">    print(g.depth_first_search(root))</span><br></pre></td></tr></table></figure><p><img src="/2019/07/14/breadth-depth-first-search/DFS.gif" alt="DFS"></p><h2 id="1-3-广度优先搜索"><a href="#1-3-广度优先搜索" class="headerlink" title="1.3 广度优先搜索"></a>1.3 广度优先搜索</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">breadth_first_search</span><span class="params">(self, root=None)</span>:</span></span><br><span class="line">    queue = []</span><br><span class="line">    order = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bfs</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">while</span> len(queue) &gt; <span class="number">0</span>:</span><br><span class="line">            node_now = queue.pop(<span class="number">0</span>)</span><br><span class="line">            self.visited[node_now] = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">if</span> node_now <span class="keyword">not</span> <span class="keyword">in</span> self.node_neighbors:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># 遍历其所有邻居节点(包含父节点和子节点)</span></span><br><span class="line">            <span class="keyword">for</span> n <span class="keyword">in</span> self.node_neighbors[node_now]:  </span><br><span class="line">                <span class="keyword">if</span> (n <span class="keyword">not</span> <span class="keyword">in</span> self.visited) <span class="keyword">and</span> (n <span class="keyword">not</span> <span class="keyword">in</span> queue):</span><br><span class="line">                    queue.append(n)</span><br><span class="line">                    order.append(n)</span><br><span class="line">    <span class="keyword">if</span> root:</span><br><span class="line">        queue.append(root)</span><br><span class="line">        order.append(root)</span><br><span class="line">        bfs()</span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> self.nodes():</span><br><span class="line">        <span class="keyword">if</span> node <span class="keyword">not</span> <span class="keyword">in</span> self.visited:</span><br><span class="line">            queue.append(node)</span><br><span class="line">            order.append(node)</span><br><span class="line">            bfs()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> order</span><br></pre></td></tr></table></figure><p>输出广度优先搜索的结果：[0, 1, 2, 3, 4, 5, 6]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># 手写实现</span></span><br><span class="line">    node_edges = &#123;<span class="number">0</span>: [<span class="number">1</span>, <span class="number">2</span>], <span class="number">1</span>: [<span class="number">3</span>, <span class="number">4</span>], <span class="number">2</span>: [<span class="number">5</span>, <span class="number">6</span>]&#125;</span><br><span class="line">    root = <span class="number">0</span></span><br><span class="line">    g = Graph()</span><br><span class="line">    g.add_node_neighbors(node_edges)</span><br><span class="line">    print(g.breadth_first_search(root))</span><br></pre></td></tr></table></figure><p><img src="/2019/07/14/breadth-depth-first-search/./BFS.gif" alt="BFS"></p><h1 id="2-networkx模块实现"><a href="#2-networkx模块实现" class="headerlink" title="2. networkx模块实现"></a>2. networkx模块实现</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bd_first_search</span><span class="params">(node_edges, mode, root)</span>:</span></span><br><span class="line">    <span class="comment"># 建立无向有序图</span></span><br><span class="line">    g = nx.OrderedGraph()</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> node_edges.items():</span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> v:</span><br><span class="line">            g.add_edge(k, l)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># BDFS</span></span><br><span class="line">    edges_list = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">'breadth'</span>:</span><br><span class="line">        edges_list = list(nx.traversal.bfs_edges(g, root))</span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">'depth'</span>:</span><br><span class="line">        edges_list = list(nx.traversal.dfs_edges(g, root))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">"please input mode correctly!"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 整理结果</span></span><br><span class="line">    nodes_list = <span class="literal">None</span></span><br><span class="line">    nodes_list = list(edges_list[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> edges_list[<span class="number">1</span>:]:</span><br><span class="line">        <span class="comment"># 可以不判断k值，定在nodes_list中</span></span><br><span class="line">        <span class="keyword">if</span> v <span class="keyword">not</span> <span class="keyword">in</span> nodes_list:</span><br><span class="line">            nodes_list.append(v)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nodes_list</span><br></pre></td></tr></table></figure><h2 id="2-1-输出深广度搜索结果"><a href="#2-1-输出深广度搜索结果" class="headerlink" title="2.1 输出深广度搜索结果"></a>2.1 输出深广度搜索结果</h2><p>同手写结果是一样的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(bd_first_search(node_edges, <span class="string">'depth'</span>, root))  <span class="comment"># [0, 1, 3, 4, 2, 5, 6]</span></span><br><span class="line">print(bd_first_search(node_edges, <span class="string">'breadth'</span>, root))  <span class="comment"># [0, 1, 2, 3, 4, 5, 6]</span></span><br></pre></td></tr></table></figure><h1 id="3-搜索效率对比"><a href="#3-搜索效率对比" class="headerlink" title="3. 搜索效率对比"></a>3. 搜索效率对比</h1><p>为了评估自己的手写实现和Python自带模块<code>networkx</code>的搜索效率，简单用Jupyter的Magic Commands <code>%%timeit</code>做评估。</p><p>首先，构建一个随机化一颗树：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">random.seed = <span class="number">2019</span></span><br><span class="line"></span><br><span class="line">node_edges = dict(zip(np.random.randint(<span class="number">1</span>, <span class="number">1000</span>, <span class="number">100</span>), [<span class="number">0</span>] * <span class="number">100</span>))</span><br><span class="line">root = list(node_edges.keys())[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> node_edges:</span><br><span class="line">    node_edges[k] = np.random.randint(<span class="number">1</span>, <span class="number">1000</span>, random.randint(<span class="number">1</span>, <span class="number">100</span>))</span><br></pre></td></tr></table></figure><h2 id="3-1-深度优先搜索对比结果"><a href="#3-1-深度优先搜索对比结果" class="headerlink" title="3.1 深度优先搜索对比结果"></a>3.1 深度优先搜索对比结果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">%%timeit</span><br><span class="line"><span class="comment"># 法一：手写实现</span></span><br><span class="line">g = Graph()</span><br><span class="line">g.add_node_neighbors(node_edges)</span><br><span class="line">result1 = g.depth_first_search(root)</span><br></pre></td></tr></table></figure><p>手写实现的结果是：7.08 ms ± 11.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">%%timeit</span><br><span class="line"><span class="comment"># 法二：networkx</span></span><br><span class="line">result2 = bd_first_search(node_edges, <span class="string">'depth'</span>, root)</span><br></pre></td></tr></table></figure><p>用<code>networkx</code>实现的结果是：15 ms ± 63 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)</p><h2 id="3-2-广度优先搜索对比结果"><a href="#3-2-广度优先搜索对比结果" class="headerlink" title="3.2 广度优先搜索对比结果"></a>3.2 广度优先搜索对比结果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">%%timeit -n <span class="number">100</span></span><br><span class="line"><span class="comment"># 法一：手写实现</span></span><br><span class="line">g = Graph()</span><br><span class="line">g.add_node_neighbors(node_edges)</span><br><span class="line">result1 = g.breadth_first_search(root)</span><br></pre></td></tr></table></figure><p>手写实现的结果是：24.2 ms ± 61.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">%%timeit -n <span class="number">100</span></span><br><span class="line"><span class="comment"># 法二：networkx</span></span><br><span class="line">result2 = bd_first_search(node_edges, <span class="string">'depth'</span>, root)</span><br></pre></td></tr></table></figure><p>用<code>networkx</code>实现的结果是：14 ms ± 129 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)</p><p>从上面的对比结果可以看出几个问题：</p><ul><li><p>手写实现的深/广度优先搜索，其耗时有较大差异，BFS的耗时是DFS的3倍以上；</p></li><li><p><code>networkx</code>模块的深/广度优先搜索的效率相差不大；</p></li><li><p>当采用DFS时：手写实现较<code>networkx</code>的要快，耗时大概是其1/2；当采用BFS时：手写实现较<code>networkx</code>的要慢，耗时大概是其2倍；</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;前面项目在做一个遍历搜索的时候，有用到深度/广度搜索的相关知识；原理很简单，不再拾人牙慧了哈；这篇文章主要是将我自己简单实现的深广度搜索分享出来并与Python &lt;code&gt;networkx&lt;/code&gt;模块中的已有实现做一个简单对比。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2019/07/14/breadth-depth-first-search/DFS.gif&quot; alt=&quot;DFS&quot;&gt;  &lt;img src=&quot;/2019/07/14/breadth-depth-first-search/BFS.gif&quot; alt=&quot;BFS&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术备忘" scheme="https://buracagyang.github.io/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/"/>
    
      <category term="算法备忘" scheme="https://buracagyang.github.io/tags/%E7%AE%97%E6%B3%95%E5%A4%87%E5%BF%98/"/>
    
  </entry>
  
  <entry>
    <title>信息论2-交叉熵和散度</title>
    <link href="https://buracagyang.github.io/2019/06/21/information-theory-2/"/>
    <id>https://buracagyang.github.io/2019/06/21/information-theory-2/</id>
    <published>2019-06-21T06:21:58.000Z</published>
    <updated>2019-07-14T06:52:21.445Z</updated>
    
    <content type="html"><![CDATA[<p>同步于<a href="https://blog.csdn.net/buracag_mc" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/" target="_blank" rel="noopener">音尘杂记</a></p><p>主要总结了交叉熵、KL散度、JS散度和wasserstein距离(也称推土机距离，EMD)的相关知识，其中EMD的直观表示可以参见下图：</p><p><img src="/2019/06/21/information-theory-2/EMD.png" alt="EMD"></p><h1 id="1-交叉熵"><a href="#1-交叉熵" class="headerlink" title="1. 交叉熵"></a>1. 交叉熵</h1><p>对应分布为$p(x)$的随机变量，熵$H(p)$表示其最优编码长度。<strong>交叉熵（Cross Entropy）</strong>是按照概率分布$q$的最优编码对真实分布为$p$的信息进行编码的长度，</p><a id="more"></a><p>交叉熵定义为<br>$$<br>H(p, q) = \Bbb{E}_p[−log q(x)] = −\sum_{x}p(x)logq(x) \tag{1}<br>$$<br>在给定$p$的情况下，如果$q$和$p$越接近，交叉熵越小；如果$q$和$p$越远，交叉熵就越大。</p><h1 id="2-KL散度"><a href="#2-KL散度" class="headerlink" title="2. KL散度"></a>2. KL散度</h1><p><strong>KL散度（Kullback-Leibler Divergence）</strong>，也叫<strong>KL距离</strong>或<strong>相对熵(Relative Entropy)</strong>，是用概率分布q来近似p时所造成的信息损失量。KL散度是按照概率分布q的最优编码对真实分布为p的信息进行编码，其平均编码长度$H(p, q)$和$p$的最优平均编码长度$H(p)$之间的差异。对于离散概率分布$p$和$q$，从$q$到$p$的KL散度定义为<br>$$<br>D_{KL}(p∥q) = H(p,q) − H(p) = \sum_{x}p(x)log\frac{p(x)}{q(x)} \tag{2}<br>$$<br>其中为了保证连续性，定义$0 log \frac{0}{0} = 0, 0 log \frac{0}{q} = 0$。</p><p>KL散度可以是衡量两个概率分布之间的距离。KL散度总是非负的，$D_{KL}(p∥q) ≥0$。只有当$p = q$时，$D_{KL}(p∥q) = 0$。如果两个分布越接近，KL散度越小；如果两个分布越远，KL散度就越大。但KL散度并不是一个真正的度量或距离，一是KL散度不满足距离的对称性，二是KL散度不满足距离的三角不等式性质。</p><h1 id="3-JS散度"><a href="#3-JS散度" class="headerlink" title="3. JS散度"></a>3. JS散度</h1><p><strong>JS散度（Jensen–Shannon Divergence）</strong>是一种对称的衡量两个分布相似度的度量方式，定义为<br>$$<br>D_{JS}(p∥q) = \frac{1}{2}D_{KL}(p∥m) + \frac{1}{2}D_{KL}(q∥m) \tag{3}<br>$$<br>其中$m = \frac{1}{2}(p + q)$。</p><p>JS 散度是KL散度一种改进。但两种散度都存在一个问题，即如果两个分布p, q 没有重叠或者重叠非常少时，KL散度和JS 散度都很难衡量两个分布的距离。</p><h1 id="4-Wasserstein距离"><a href="#4-Wasserstein距离" class="headerlink" title="4. Wasserstein距离"></a>4. Wasserstein距离</h1><p><strong>Wasserstein 距离（Wasserstein Distance）</strong>也用于衡量两个分布之间的距离。对于两个分布$q_1, q_2，p^{th}-Wasserstein$距离定义为<br>$$<br>W_p(q_1, q_2) =<br>\left (<br>\inf_{\gamma(x, y) \in \Gamma(q_1, q_2)}\Bbb{E}_{(x,y)\sim \gamma(x,y)}[d(x,y)^p]<br>\right )^{1/p} \tag{4}<br>$$</p><p>其中$Gamma(q_1, q_2)$是边际分布为$q_1$和$q_2$的所有可能的联合分布集合，$d(x, y)$为$x$和$y$的距离，比如$\ell_p$距离等。</p><p>如果将两个分布看作是两个土堆，联合分布$\gamma(x, y)$看作是从土堆$q_1$的位置$x$到土堆$q_2$的位置$y$的搬运土的数量，并有<br>$$<br>\begin{eqnarray}<br>\sum_{x}\gamma(x, y) = q_2(y) \tag{5} \\<br>\sum_{y}\gamma(x, y) = q_1(x) \tag{6}<br>\end{eqnarray}<br>$$<br>$q_1$和$q_2$为$\gamma(x, y)$的两个边际分布。</p><p>$\Bbb{E}_{(x,y) \sim \gamma(x,y)}[d(x, y)^p]$可以理解为在联合分布$\gamma(x, y)$下把形状为$q_1$的土堆搬运到形状为$q_2$的土堆所需的工作量，<br>$$<br>\Bbb{E}_{(x,y) \sim \gamma(x,y)}[d(x, y)^p] = \sum_{(x,y)}\gamma(x, y)d(x, y)^p \tag{7}<br>$$<br>其中从土堆$q_1$中的点$x$到土堆$q_2$中的点$y$的移动土的数量和距离分别为$\gamma(x, y)$和$d(x, y)^p$。因此，Wasserstein距离可以理解为搬运土堆的最小工作量，也称为<strong>推土机距离（Earth-Mover’s Distance，EMD）</strong>。</p><p>Wasserstein距离相比KL散度和JS 散度的优势在于：即使两个分布没有重叠或者重叠非常少，Wasserstein 距离仍然能反映两个分布的远近。</p><p>对于$\Bbb{R}^n$空间中的两个高斯分布$p = \cal{N}(\mu1,Σ1)$和$q = \cal{N}(\mu2,Σ2)$，它们的$2^{nd}-Wasserstein$距离为<br>$$<br>D_W(p∥q) = ||μ1 − μ2||_2^2 + tr<br>\left (<br>\begin {matrix}<br>\sum_1 + \sum_2 - 2(\sum_2^{1/2}\sum_1\sum_2^{1/2})^{1/2}<br>\end {matrix}<br>\right ) \tag{8}<br>$$<br>当两个分布的的方差为0时，$2^{nd}-Wasserstein$距离等价于欧氏距离($||μ1 − μ2||_2^2$)。</p><h2 id="4-1-EMD示例"><a href="#4-1-EMD示例" class="headerlink" title="4.1 EMD示例"></a>4.1 EMD示例</h2><p>求解两个分布的EMD可以通过一个<strong>Linear Programming（LP）</strong>问题来解决，可以将这个问题表达为一个规范的问题：寻找一个向量$x \in \Bbb{R}$，最小化损失$z = c^Tx, c\in \Bbb{R}^n$，使得$Ax = b, A \in \Bbb{R}^{m\times n},b \in \Bbb{R}^m, x \geq 0$，显然，在求解EMD时有：<br>$$<br>x = vec(\Gamma) \\<br>c = vec(D)<br>$$<br>其中$\Gamma$是$q_1$和$q_2$的联合概率分布，$D$是移动距离。</p><p>首先生成两个分布$q_1$和$q_2$：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.colors <span class="keyword">as</span> colors</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> linprog</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> cm</span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> linprog</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> cm</span><br><span class="line"></span><br><span class="line">l = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">q1 = np.array([<span class="number">13</span>, <span class="number">8</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">21</span>, <span class="number">15</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">15</span>])</span><br><span class="line">q2 = np.array([<span class="number">1</span>, <span class="number">6</span>, <span class="number">12</span>, <span class="number">17</span>, <span class="number">12</span>, <span class="number">10</span>, <span class="number">8</span>, <span class="number">15</span>, <span class="number">4</span>, <span class="number">2</span>])</span><br><span class="line">q1 = q1 / np.sum(q1)</span><br><span class="line">q2 = q2 / np.sum(q2)</span><br><span class="line"></span><br><span class="line">plt.bar(range(l), q1, <span class="number">1</span>, color=<span class="string">'blue'</span>, alpha=<span class="number">1</span>, edgecolor=<span class="string">'black'</span>)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">0.5</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.bar(range(l), q1, <span class="number">1</span>, color=<span class="string">'green'</span>, alpha=<span class="number">1</span>, edgecolor=<span class="string">'black'</span>)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">0.5</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2019/06/21/information-theory-2/q1.svg" alt="q1"></p><p><img src="/2019/06/21/information-theory-2/q2.svg" alt="q2"></p><p>计算其联合概率分布和距离矩阵：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">D = np.ndarray(shape=(l, l))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(l):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(l):</span><br><span class="line">        D[i, j] = abs(range(l)[i] - range(l)[j])</span><br><span class="line"></span><br><span class="line">A_1 = np.zeros((l, l, l))</span><br><span class="line">A_2 = np.zeros((l, l, l))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(l):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(l):</span><br><span class="line">        A_1[i, i, j] = <span class="number">1</span></span><br><span class="line">        A_2[i, j, i] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">A = np.concatenate((A_1.reshape((l, l**<span class="number">2</span>)), A_2.reshape((l, l**<span class="number">2</span>))), axis=<span class="number">0</span>)  <span class="comment"># 20x100</span></span><br><span class="line">b = np.concatenate((q1, q2), axis=<span class="number">0</span>)  <span class="comment"># 20x1</span></span><br><span class="line">c = D.reshape((l**<span class="number">2</span>))  <span class="comment"># 100x1</span></span><br><span class="line"></span><br><span class="line">opt_res = linprog(c, A_eq=A, b_eq=b, bounds=[<span class="number">0</span>, <span class="literal">None</span>])</span><br><span class="line">emd = opt_res.fun</span><br><span class="line">gamma = opt_res.x.reshape((l, l))</span><br><span class="line">print(<span class="string">"EMD: "</span>, emd)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Gamma</span></span><br><span class="line">plt.imshow(gamma, cmap=cm.gist_heat, interpolation=<span class="string">'nearest'</span>)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># D</span></span><br><span class="line">plt.imshow(D, cmap=cm.gist_heat, interpolation=<span class="string">'nearest'</span>)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2019/06/21/information-theory-2/gamma.svg" alt="gamma"></p><p><img src="/2019/06/21/information-theory-2/distances.svg" alt="distances"></p><p>最终得到EMD=0.8252404410039889</p><h2 id="4-2-利用对偶问题求解EMD"><a href="#4-2-利用对偶问题求解EMD" class="headerlink" title="4.2 利用对偶问题求解EMD"></a>4.2 利用对偶问题求解EMD</h2><p>事实上，4.1节说的求解方式在很多情形下是不适用的，在示例中我们只用了10个状态去描述分布，但是在很多应用中，输入的状态数很容易的就到达了上万维，甚至近似求$\gamma$都是不可能的。</p><p>但实际上我们并不需要关注$\gamma$，我们仅需要知道具体的EMD数值，我们必须能够计算梯度$\nabla_{P_1}EMD(P_1, P_2)$，因为$P_1$和$P_2$仅仅是我们的约束条件，这是不可能以任何直接的方式实现的。</p><p>但是，这里有另外一个更加方便的方法去求解EMD；任何LP问题都有两种表示问题的方法：原始问题(4.1所述)和对偶问题。所以刚才的问题转化成对偶问题如下：<br>$$<br>\begin {eqnarray}<br>maxmize \qquad &amp;\tilde{z}=b^T.y \\<br>st. \qquad &amp;A^T.y \leq c<br>\end {eqnarray} \tag{9}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">opt_res = linprog(-b, A.T, c, bounds=(<span class="literal">None</span>, <span class="literal">None</span>))</span><br><span class="line"></span><br><span class="line">emd = -opt_res.fun</span><br><span class="line">f = opt_res.x[<span class="number">0</span>:l]</span><br><span class="line">g = opt_res.x[l:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(dual_result)</span></span><br><span class="line">print(<span class="string">"dual EMD: "</span>, emd)</span><br></pre></td></tr></table></figure><p>得到其结果：EMD=0.8252404410039867</p><p>或者另一种方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">emd = np.sum(np.multiply(q1, f)) + np.sum(np.multiply(q2, g))</span><br><span class="line">print(<span class="string">"emd: "</span>, emd)</span><br></pre></td></tr></table></figure><p>得到其结果，EMD=0.8252404410039877</p><p>最后，再看一下两个分布的对应转换情况：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># q1</span></span><br><span class="line">r = range(l)</span><br><span class="line">current_bottom = np.zeros(l)</span><br><span class="line">cNorm = colors.Normalize(vmin=<span class="number">0</span>, vmax=l)</span><br><span class="line">colorMap = cm.ScalarMappable(norm=cNorm, cmap=cm.terrain)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> r:</span><br><span class="line">    plt.bar(r, gamma[r, i], <span class="number">1</span>, color=colorMap.to_rgba(r), bottom=current_bottom, edgecolor=<span class="string">'black'</span>)</span><br><span class="line">    current_bottom = current_bottom + gamma[r, i]</span><br><span class="line"></span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">0.5</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2019/06/21/information-theory-2/earth_move_q1.svg" alt="earth_move_q1"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># q2</span></span><br><span class="line">r = range(l)</span><br><span class="line">current_bottom = np.zeros(l)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> r:</span><br><span class="line">    plt.bar(r, gamma[i, r], <span class="number">1</span>, color=colorMap.to_rgba(i), bottom=current_bottom, edgecolor=<span class="string">'black'</span>)</span><br><span class="line">    current_bottom = current_bottom + gamma[i, r]</span><br><span class="line"></span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">0.5</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2019/06/21/information-theory-2/earth_move_q2.svg" alt="earth_move_q2"></p><p>主要参考:</p><ul><li><a href="https://github.com/nndl/nndl.github.io" target="_blank" rel="noopener">https://github.com/nndl/nndl.github.io</a></li><li><a href="https://vincentherrmann.github.io/blog/wasserstein/" target="_blank" rel="noopener">https://vincentherrmann.github.io/blog/wasserstein/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;主要总结了交叉熵、KL散度、JS散度和wasserstein距离(也称推土机距离，EMD)的相关知识，其中EMD的直观表示可以参见下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2019/06/21/information-theory-2/EMD.png&quot; alt=&quot;EMD&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;1-交叉熵&quot;&gt;&lt;a href=&quot;#1-交叉熵&quot; class=&quot;headerlink&quot; title=&quot;1. 交叉熵&quot;&gt;&lt;/a&gt;1. 交叉熵&lt;/h1&gt;&lt;p&gt;对应分布为$p(x)$的随机变量，熵$H(p)$表示其最优编码长度。&lt;strong&gt;交叉熵（Cross Entropy）&lt;/strong&gt;是按照概率分布$q$的最优编码对真实分布为$p$的信息进行编码的长度，&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术备忘" scheme="https://buracagyang.github.io/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/"/>
    
      <category term="基础知识" scheme="https://buracagyang.github.io/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>信息论1-熵</title>
    <link href="https://buracagyang.github.io/2019/06/21/information-theory-1/"/>
    <id>https://buracagyang.github.io/2019/06/21/information-theory-1/</id>
    <published>2019-06-21T05:52:58.000Z</published>
    <updated>2019-06-24T08:37:39.650Z</updated>
    
    <content type="html"><![CDATA[<p>同步于<a href="https://blog.csdn.net/buracag_mc" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/" target="_blank" rel="noopener">音尘杂记</a></p><p><strong>信息论（Information Theory）</strong>是数学、物理、统计、计算机科学等多个学科的交叉领域。信息论是由Claude Shannon 最早提出的，主要研究信息的量化、存储和通信等方法。这里，“信息”是指一组消息的集合。假设在一个噪声通道上发送消息，我们需要考虑如何对每一个信息进行编码、传输以及解码，使得接收者可以尽可能准确地重构出消息。</p><p>在机器学习相关领域，信息论也有着大量的应用。比如特征抽取、统计推断、自然语言处理等。</p><a id="more"></a><h1 id="1-自信息和熵"><a href="#1-自信息和熵" class="headerlink" title="1. 自信息和熵"></a>1. 自信息和熵</h1><p><strong>熵（Entropy）</strong>最早是物理学的概念，用于表示一个热力学系统的无序程度。在信息论中，熵用来衡量一个随机事件的不确定性。假设对一个随机变量$X$（取值集合为$\cal{X}$，概率分布为$p(x), x \in \cal{X}$）进行编码，<strong>自信息（Self Information）</strong> $I(x)$是变量$X = x$时的信息量或编码长度，定义为<br>$$<br>I(x) = −log(p(x)) \tag{1}<br>$$<br>那么随机变量$X$的平均编码长度，即熵定义为<br>$$<br>H(X) = \Bbb{E}_X[I(x)] = \Bbb{E}_X[−log(p(x))] = −\sum_{x \in \cal{X}}p(x) log p(x) \tag{2}<br>$$<br>其中当$p(x_i) = 0$时，我们定义$0 log 0 = 0$，与极限一致，$\lim_{p\to 0+} p log p = 0$。</p><p>熵是一个随机变量的平均编码长度，即自信息的数学期望。熵越高，则随机变量的信息越多，熵越低；则信息越少。如果变量$X$当且仅当在$x$时$p(x) = 1$，则熵为0。也就是说，对于一个<strong>确定的信息(不确定概率为0)</strong>，其熵为0，信息量也为0。如果其概率分布为一个均匀分布，则熵最大。假设一个随机变量X 有三种可能值$x_1, x_2, x_3$，不同概率分布对应的熵如下：</p><table><thead><tr><th style="text-align:center">p(x1)</th><th style="text-align:center">p(x2)</th><th style="text-align:center">p(x3)</th><th style="text-align:center">熵</th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">1/2</td><td style="text-align:center">1/4</td><td style="text-align:center">1/4</td><td style="text-align:center">$\frac{3}{2}(log2)$</td></tr><tr><td style="text-align:center">1/3</td><td style="text-align:center">1/3</td><td style="text-align:center">1/3</td><td style="text-align:center">log3</td></tr></tbody></table><h1 id="2-联合熵和条件熵"><a href="#2-联合熵和条件熵" class="headerlink" title="2. 联合熵和条件熵"></a>2. 联合熵和条件熵</h1><p>对于两个离散随机变量$X$和$Y$ ，假设$X$取值集合为$cal{X}$；$Y$取值集合为$\cal{Y}$，其联合概率分布满足为$p(x, y)$，</p><p>则$X$和$Y$的<strong>联合熵（Joint Entropy）</strong>为<br>$$<br>H(X, Y) = −\sum_{x \in \cal{X}} \sum_{y \in \cal{Y}}p(x, y) log p(x, y) \tag{3}<br>$$<br>$X$和$Y$的<strong>条件熵（Conditional Entropy）</strong>为<br>$$<br>H(X|Y) = −\sum_{x \in \cal{X}} \sum_{y \in \cal{Y}}p(x, y) log p(x|y) = −\sum_{x \in \cal{X}} \sum_{y \in \cal{Y}}p(x, y) log \frac{p(x,y)}{p(y)} \tag{4}<br>$$<br>根据其定义，条件熵也可以写为<br>$$<br>H(X|Y) = H(X, Y) − H(Y) \tag{5}<br>$$</p><h1 id="3-互信息"><a href="#3-互信息" class="headerlink" title="3. 互信息"></a>3. 互信息</h1><p><strong>互信息（Mutual Information）</strong>是衡量已知一个变量时，另一个变量不确定性的减少程度。两个离散随机变量X 和Y 的互信息定义为<br>$$<br>I(X; Y ) =\sum_{x \in \cal{X}} \sum_{y \in \cal{Y}}p(x, y) \frac{log p(x, y)}{p(x)p(y)} \tag{6}<br>$$<br>互信息的一个性质为<br>$$<br>\begin{eqnarray}<br>I(X;Y) &amp;=&amp; H(X) − H(X|Y) \tag{7} \\<br>&amp;=&amp; H(Y) − H(Y|X) \tag{8} \\<br>&amp;=&amp; H(X) + H(Y) - H(X, Y) \tag{9}<br>\end{eqnarray}<br>$$</p><p>如果X和Y相互独立，即X不对Y提供任何信息，反之亦然，因此它们的互信息最小， 即$I(X;Y)$为零。</p><p>主要参考<a href="https://github.com/nndl/nndl.github.io" target="_blank" rel="noopener">https://github.com/nndl/nndl.github.io</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;信息论（Information Theory）&lt;/strong&gt;是数学、物理、统计、计算机科学等多个学科的交叉领域。信息论是由Claude Shannon 最早提出的，主要研究信息的量化、存储和通信等方法。这里，“信息”是指一组消息的集合。假设在一个噪声通道上发送消息，我们需要考虑如何对每一个信息进行编码、传输以及解码，使得接收者可以尽可能准确地重构出消息。&lt;/p&gt;
&lt;p&gt;在机器学习相关领域，信息论也有着大量的应用。比如特征抽取、统计推断、自然语言处理等。&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术备忘" scheme="https://buracagyang.github.io/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/"/>
    
      <category term="基础知识" scheme="https://buracagyang.github.io/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>数学优化3-拉格朗日乘数法与KKT条件</title>
    <link href="https://buracagyang.github.io/2019/06/19/mathematical-optimization-3/"/>
    <id>https://buracagyang.github.io/2019/06/19/mathematical-optimization-3/</id>
    <published>2019-06-19T12:47:06.000Z</published>
    <updated>2019-06-21T06:32:47.303Z</updated>
    
    <content type="html"><![CDATA[<p>同步于<a href="https://blog.csdn.net/buracag_mc" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/" target="_blank" rel="noopener">音尘杂记</a></p><p>主要介绍一下数学优化中的拉格朗日乘数法和KKT条件，其实在 <a href="https://blog.csdn.net/buracag_mc/article/details/76762249" target="_blank" rel="noopener">拙文</a> 中已经有关于KKT条件的简要介绍和自己的个人总结，这里再一起回顾一下。</p><a id="more"></a><p><strong>拉格朗日乘数法（Lagrange Multiplier）</strong>是约束优化问题的一种有效求解方法。约束优化问题可以表示为<br>$$<br>\begin{eqnarray}<br>\min_{x} \qquad &amp;f(x) \\<br>subject \quad to \qquad &amp;h_i(x) = 0, i = 1, … ,m \\<br>\qquad &amp;g_j(x) ≤ 0, j = 1, . . . , n<br>\end{eqnarray} \tag{1}<br>$$</p><p>其中$h_i(x)$为等式约束函数，$g_j(x)$为不等式约束函数。x的可行域为<br>$$<br>\cal{D} = domf\cap \bigcap_{i=1}^{m} domh_i \cap \bigcap_{j=1}^{n} domg_j \subseteq \Bbb{R}^d \tag{2}<br>$$<br>其中$domf$是函数f的定义域。</p><h1 id="1-等式约束优化问题"><a href="#1-等式约束优化问题" class="headerlink" title="1. 等式约束优化问题"></a>1. 等式约束优化问题</h1><p>如果公式(1) 中只有等式约束，我们可以构造一个拉格朗日函数Λ(x, λ):<br>$$<br>\Lambda(x, \lambda) = f(x) + \sum_{i=1}^{m}\lambda_i h_i(x) \tag{3}<br>$$<br>其中$\lambda$为拉格朗日乘数。如果$f(x^∗)$是原始约束优化问题的局部最优值，那么存在一个$λ^∗$使得$(x^∗, λ^∗)$为拉格朗日函数$Λ(x, λ)$的平稳点（stationary point）。因此，只需要令$\frac{\partialΛ(x,λ)}{\partial x} = 0$和$\frac{\partialΛ(x,λ)}{\partial \lambda} = 0$，得到<br>$$<br>\nabla f(x) + \sum_{i=1}^{m}\lambda_i \nabla h_i(x) = 0 \tag{4}<br>$$</p><p>$$<br>h_i(x) = 0, \qquad i=0, …, m \tag{5}<br>$$</p><p>上面方程组的解即为原始问题的可能解。在实际应用中，需根据问题来验证是否为极值点。</p><p>拉格朗日乘数法是将一个有$d$个变量和$m$个等式约束条件的最优化问题转换为一个有$d + m$个变量的函数求平稳点的问题。拉格朗日乘数法所得的平稳点会包含原问题的所有极值点，但并不保证每个平稳点都是原问题的极值点。</p><h1 id="2-不等式约束优化问题"><a href="#2-不等式约束优化问题" class="headerlink" title="2. 不等式约束优化问题"></a>2. 不等式约束优化问题</h1><p>对于公式(1) 中定义的一般约束优化问题，其拉格朗日函数为<br>$$<br>\Lambda(x, a, b) = f(x) + \sum_{i=1}^{m}a_i h_i(x) + \sum_{j=1}^{n}b_j g_j(x) \tag{6}<br>$$<br>其中$a = [a_1, … , a_m]^T$为等式约束的拉格朗日乘数，$b = [b_1, … , b_n]^T$为不等式约束的拉格朗日乘数。</p><p>当约束条件不满足时，有$\max_{a,b} \Lambda(x, a, b) = \infty$；当约束条件满足时并且$b ≥ 0$时，$\max_{a,b} \Lambda(x, a, b) = f(x)$。因此原始约束优化问题等价于<br>$$<br>\min_x \max_{a,b} \Lambda(x, a, b) \tag{7}<br>$$</p><p>$$<br>subject \quad to \qquad b ≥ 0 \tag{8}<br>$$</p><p>这个min-max优化问题称为<strong>主问题（Primal Problem）</strong>。</p><p><strong>对偶问题</strong> 主问题的优化一般比较困难，我们可以通过交换min-max 的顺序来简化。定义拉格朗日对偶函数为<br>$$<br>\Gamma(a, b) = \inf_{x \in D}\Lambda (x, a, b) \tag{9}<br>$$</p><p>$\Gamma(a, b)$是一个凹函数，即使$f(x)$是非凸的。</p><p>当$b \geq 0$时，对于任意的$\tilde{x} \in \cal{D}$，有<br>$$<br>\Gamma(a, b) = \inf_{x\in D}\Lambda(x, a, b) \leq \Lambda(\tilde{x}, a, b) ≤ f(\tilde{x}) \tag{10}<br>$$<br>令$p^∗$是原问题的最优值，则有<br>$$<br>\Gamma(a, b) \leq p^∗ \tag{11}<br>$$<br>即拉格朗日对偶函数$Γ(a, b)$为原问题最优值的下界。</p><p>优化拉格朗日对偶函数$Γ(a, b)$并得到原问题的最优下界，称为<strong>拉格朗日对偶问题（Lagrange Dual Problem）</strong>。<br>$$<br>\begin{eqnarray}<br>\max_{a,b} \qquad &amp;\Gamma(a, b) \tag{12}  \\<br>subject \quad to \qquad &amp;b ≥ 0 \tag{13}<br>\end{eqnarray}<br>$$<br>拉格朗日对偶函数为凹函数，因此拉格朗日对偶问题为<strong>凸优化问题</strong>。</p><p>令$d^∗$表示拉格朗日对偶问题的最优值，则有$d^∗ \leq p^∗$，这个性质称为<strong>弱对偶性（Weak Duality）</strong>。如果$d^∗ = p^∗$，这个性质称为<strong>强对偶性（Strong Duality）</strong>。</p><p>当强对偶性成立时，令$x^∗$和$a^∗, b^∗$分别是原问题和对偶问题的最优解，那么它们满足以下条件：<br>$$<br>\begin{eqnarray}<br>&amp; \nabla f(x^∗) + \sum_{i=1}^ma_i^∗ \nabla h_i(x^∗) + \sum_{j=1}^{n}b_j^∗\nabla g_j(x^∗) = 0 \tag{14} \\<br>&amp; h_i(x^∗) = 0, \quad i = 0, … ,m \tag{15} \\<br>&amp; g_j(x^∗) \leq 0, \quad j = 0, … , n \tag{16} \\<br>&amp; b_j^∗ g_j(x^∗) = 0, \quad j = 0, … , n \tag{17} \\<br>&amp; b_j^∗ \geq 0, \quad j = 0, … , n \tag{18}<br>\end{eqnarray}<br>$$<br>称为不等式约束优化问题的<strong>KKT条件（Karush-Kuhn-Tucker Conditions）</strong>。KKT条件是拉格朗日乘数法在不等式约束优化问题上的泛化。当原问题是凸优化问题时，满足KKT条件的解也是原问题和对偶问题的最优解。</p><p>KKT条件中需要关注的是公式(17)，称为互补松弛条件（Complementary Slackness）。如果最优解$x^∗$出现在不等式约束的边界上$g_j(x) = 0$，则$b_j^∗ &gt; 0$；如果$x^∗$出现在不等式约束的内部$g_j(x) &lt; 0$，则$b_j^∗$= 0$。互补松弛条件说明当最优解出现在不等式约束的内部，则约束失效。</p><p>主要参考<a href="https://github.com/nndl/nndl.github.io" target="_blank" rel="noopener">https://github.com/nndl/nndl.github.io</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;主要介绍一下数学优化中的拉格朗日乘数法和KKT条件，其实在 &lt;a href=&quot;https://blog.csdn.net/buracag_mc/article/details/76762249&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;拙文&lt;/a&gt; 中已经有关于KKT条件的简要介绍和自己的个人总结，这里再一起回顾一下。&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术备忘" scheme="https://buracagyang.github.io/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/"/>
    
      <category term="基础知识" scheme="https://buracagyang.github.io/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>数学优化2-优化算法</title>
    <link href="https://buracagyang.github.io/2019/06/18/mathematical-optimization-2/"/>
    <id>https://buracagyang.github.io/2019/06/18/mathematical-optimization-2/</id>
    <published>2019-06-18T05:17:45.000Z</published>
    <updated>2019-06-18T09:45:50.821Z</updated>
    
    <content type="html"><![CDATA[<p>同步于<a href="https://blog.csdn.net/buracag_mc" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/" target="_blank" rel="noopener">音尘杂记</a></p><p>优化问题一般都是通过迭代的方式来求解：通过猜测一个初始的估计$x_0$，然后不断迭代产生新的估计$x_1, x_2, …  x_t$，希望$x_t$最终收敛到期望的最优解$x^∗$。</p><p>一个好的优化算法应该是在一定的时间或空间复杂度下能够快速准确地找到最优解。同时，好的优化算法受初始猜测点的影响较小，通过迭代能稳定地找到最优解$x^∗$的邻域，然后迅速收敛于$x^∗$。</p><p>优化算法中常用的迭代方法有线性搜索和置信域方法等。线性搜索的策略是寻找方向和步长，具体算法有梯度下降法、牛顿法、共轭梯度法等。在<a href="https://blog.csdn.net/buracag_mc/article/details/77620686" target="_blank" rel="noopener">文章</a>中也简要介绍过梯度下降的概念，这里为使得整个体系完整故重新记录一下。</p><a id="more"></a><h1 id="1-全局最优和局部最优"><a href="#1-全局最优和局部最优" class="headerlink" title="1. 全局最优和局部最优"></a>1. 全局最优和局部最优</h1><p>对于很多非线性优化问题，会存在若干个局部的极小值。<strong>局部最小值</strong>，或<strong>局部最优解</strong>$x^∗$定义为：存在一个$\delta &gt; 0$，对于所有的满足$∥x−x^*∥ \leq \delta$的x，公式$f(x^∗) \leq f(x)$成立。也就是说，在$x^∗$的附近区域内，所有的函数值都大于或者等于$f(x^∗)$。</p><p>对于所有的$x \in A$，都有$f(x^∗) \leq f(x)$成立，则$x^∗$为全局最小值，或全局最优解。一般的，求局部最优解是容易的，但很难保证其为全局最优解。对于线性规划或凸优化问题，局部最优解就是全局最优解。</p><p>要确认一个点$x^∗$是否为局部最优解，通过比较它的邻域内有没有更小的函数值是不现实的。如果函数$f(x)$是二次连续可微的，我们可以通过检查目标函数在点$x^∗$的梯度$∇f(x^∗)$和Hessian矩阵$\nabla^2f(x^∗)$来判断。</p><blockquote><p><strong>定理1 局部最小值的一阶必要条件：</strong> 如果$x^∗$为局部最优解并且函数$f$在$x^∗$的邻域内一阶可微，则在$∇f(x^∗) = 0$。</p></blockquote><p><strong>证明.</strong> 如果函数$f(x)$是连续可微的，根据泰勒展开公式（Taylor’s Formula），函数$f(x)$的一阶展开可以近似为<br>$$<br>f(x^∗ + \Delta x) = f(x^∗) + \Delta x^T \nabla f(x^∗) \tag{1}<br>$$<br>假设$∇f(x^∗) \neq 0$，则可以找到一个$\Delta x$（比如$\Delta x = −\alpha ∇f(x^∗)$，$\alpha$为很小的正数），使得<br>$$<br>f(x^∗ + \Delta x) − f(x^∗) = \Delta x^T \nabla f(x^∗) \leq 0 \tag{2}<br>$$</p><p>这和局部最优的定义矛盾。</p><blockquote><p><strong>定理2 局部最优解的二阶必要条件：</strong> 如果$x^∗$为局部最优解并且函数$f$在$x^∗$的领域内二阶可微，则$\nabla f(x^∗)=0, \nabla^2 f(x^*)$为半正定矩阵。</p></blockquote><p><strong>证明.</strong> 如果函数$f(x)$是二次连续可微的，函数$f(x)$的二阶展开可以近似为<br>$$<br>f(x^∗ + \Delta x) = f(x^∗) + \Delta x^T \nabla f(x^∗) + \frac{1}{2}\Delta x^T (\nabla^2f(x^∗))\Delta x \tag{3}<br>$$<br>由一阶必要性定理可知$\nabla f(x^∗) = 0$，则<br>$$<br>f(x^∗ + \Delta x) − f(x^∗) = \frac{1}{2}\Delta x^T (\nabla^2 f(x^∗))\Delta x \geq 0 \tag{4}<br>$$<br>即Hessian矩阵$\nabla^2f(x^∗)$为半正定矩阵。</p><h1 id="2-梯度下降法"><a href="#2-梯度下降法" class="headerlink" title="2. 梯度下降法"></a>2. 梯度下降法</h1><p><strong>梯度下降法（Gradient Descent Method）</strong>，也叫<strong>最速下降法（Steepest Descend Method）</strong>，经常用来求解无约束优化的极小值问题。</p><p>对于函数$f(x)$，如果$f(x)$在点$x_t$附近是连续可微的，那么$f(x)$下降最快的方向是$f(x)$在$x_t$点的梯度方法的反方向。根据泰勒一阶展开公式，<br>$$<br>f(x_{t+1}) = f(x_t + \Delta x) \approx f(x_t) + \Delta x^T \nabla f(x_t) \tag{5}<br>$$<br>要使得$f(x_{t+1}) &lt; f(x_t)$，就得使$\Delta x^T\nabla f(x_t) &lt; 0$。我们取$\Delta x = −\alpha \nabla f(x_t)$。如果$\alpha &gt; 0$为一个够小数值时，那么$f(x_{t+1}) &lt; f(x_t)$ 成立。</p><p>这样我们就可以从一个初始值$x_0$出发，通过迭代公式<br>$$<br>x_{t+1} = x_t − \alpha_t\nabla f(x_t), t \geq 0 \tag{6}<br>$$<br>生成序列$x_0, x_1, x_2, …$ 使得<br>$$<br>f(x_0) \geq f(x_1) \geq f(x_2) \geq … \tag{7}<br>$$<br>如果顺利的话，序列($x_n$) 收敛到局部最优解$x^∗$。注意每次迭代步长$\alpha$可以改变，但其取值必须合适，如果过大就不会收敛，如果过小则收敛速度太慢。</p><p>梯度下降法的示例过程可以参见下图：</p><p><img src="/2019/06/18/mathematical-optimization-2/sample.gif" alt="sample"></p><p>梯度下降法为一阶收敛算法，当靠近极小值时梯度变小，收敛速度会变慢，并且可能以“之字形”的方式下降。如果目标函数为二阶连续可微，我们可以采用牛顿法。牛顿法为二阶收敛算法，收敛速度更快，但是每次迭代需要计算Hessian矩阵的逆矩阵，复杂度较高。相反，如果我们要求解一个最大值问题，就需要向梯度正方向迭代进行搜索，逐渐接近函数的局部极大值点，这个过程则被称为梯度上升法（GradientAscent）。</p><p>后面准备专门整理一份梯度下降中关于批量梯度下降(BGD)、随机梯度下降(SGD)、小批量梯度下降(MBGD)、Momentum、Adagrad等资料。</p><p>主要参考<a href="https://github.com/nndl/nndl.github.io" target="_blank" rel="noopener">https://github.com/nndl/nndl.github.io</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;优化问题一般都是通过迭代的方式来求解：通过猜测一个初始的估计$x_0$，然后不断迭代产生新的估计$x_1, x_2, …  x_t$，希望$x_t$最终收敛到期望的最优解$x^∗$。&lt;/p&gt;
&lt;p&gt;一个好的优化算法应该是在一定的时间或空间复杂度下能够快速准确地找到最优解。同时，好的优化算法受初始猜测点的影响较小，通过迭代能稳定地找到最优解$x^∗$的邻域，然后迅速收敛于$x^∗$。&lt;/p&gt;
&lt;p&gt;优化算法中常用的迭代方法有线性搜索和置信域方法等。线性搜索的策略是寻找方向和步长，具体算法有梯度下降法、牛顿法、共轭梯度法等。在&lt;a href=&quot;https://blog.csdn.net/buracag_mc/article/details/77620686&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;文章&lt;/a&gt;中也简要介绍过梯度下降的概念，这里为使得整个体系完整故重新记录一下。&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术备忘" scheme="https://buracagyang.github.io/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/"/>
    
      <category term="基础知识" scheme="https://buracagyang.github.io/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>数学优化1-数学优化的类型</title>
    <link href="https://buracagyang.github.io/2019/06/17/mathematical-optimization-1/"/>
    <id>https://buracagyang.github.io/2019/06/17/mathematical-optimization-1/</id>
    <published>2019-06-17T11:45:42.000Z</published>
    <updated>2019-06-17T12:05:14.902Z</updated>
    
    <content type="html"><![CDATA[<p>同步于<a href="https://blog.csdn.net/buracag_mc" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/" target="_blank" rel="noopener">音尘杂记</a></p><p><strong>数学优化（Mathematical Optimization）</strong>问题，也叫最优化问题，是指在一定约束条件下，求解一个目标函数的最大值（或最小值）问题。</p><p>数学优化问题的定义为：给定一个目标函数（也叫代价函数）$f : \cal{A} → \Bbb{R}$，寻找一个变量（也叫参数）$x^* \in \cal{D}$，使得对于所有$\cal{D}$中的$x，f(x^∗) ≤ f(x)$（最小化）；或者$f(x^∗) \geq f(x)$（最大化），其中$\cal{D}$为变量$x$的约束集，也叫可行域；$\cal{D}$中的变量被称为是可行解。</p><a id="more"></a><h1 id="1-离散优化和连续优化"><a href="#1-离散优化和连续优化" class="headerlink" title="1. 离散优化和连续优化"></a>1. 离散优化和连续优化</h1><p>根据输入变量$x$的值域是否为实数域，数学优化问题可以分为离散优化问题和连续优化问题。</p><h2 id="1-1-离散优化问题"><a href="#1-1-离散优化问题" class="headerlink" title="1.1 离散优化问题"></a>1.1 离散优化问题</h2><p><strong>离散优化（Discrete Optimization）</strong>问题是目标函数的输入变量为离散变量，比如为整数或有限集合中的元素。离散优化问题主要有两个分支：</p><ol><li><strong>组合优化（Combinatorial Optimization）</strong>：其目标是从一个有限集合中找出使得目标函数最优的元素。在一般的组合优化问题中，集合中的元素之间存在一定的关联，可以表示为图结构。典型的组合优化问题有旅行商问题、最小生成树问题、图着色问题等。很多机器学习问题都是组合优化问题，比如特征选择、聚类问题、超参数优化问题以及<strong>结构化学习（Structured Learning）</strong>中标签预测问题等。</li><li><strong>整数规划（Integer Programming）</strong>：输入变量$x \in \Bbb{Z}^d$为整数。一般常见的整数规划问题为<strong>整数线性规划（Integer Linear Programming，ILP）</strong>。整数线性规划的一种最直接的求解方法是：（1）去掉输入必须为整数的限制，将原问题转换为一般的线性规划问题，这个线性规划问题为原问题的松弛问题；（2）求得相应松弛问题的解；（3）把松弛问题的解四舍五入到最接近的整数。但是这种方法得到的解一般都不是最优的，因此原问题的最优解不一定在松弛问题最优解的附近。另外，这种方法得到的解也不一定满足约束条件。</li></ol><p>离散优化问题的求解一般都比较困难，优化算法的复杂度都比较高。</p><h2 id="1-2-连续优化问题"><a href="#1-2-连续优化问题" class="headerlink" title="1.2 连续优化问题"></a>1.2 连续优化问题</h2><p><strong>连续优化（Continuous Optimization）</strong>问题是目标函数的输入变量为连续变量$x \in \Bbb{R}^d$，即目标函数为实函数。下文的内容主要以连续优化为主。</p><h1 id="2-无约束优化和约束优化"><a href="#2-无约束优化和约束优化" class="headerlink" title="2. 无约束优化和约束优化"></a>2. 无约束优化和约束优化</h1><p>在连续优化问题中，根据是否有变量的约束条件，可以将优化问题分为无约束优化问题和约束优化问题。</p><p><strong>无约束优化问题（Unconstrained Optimization）</strong>的可行域为整个实数域$\cal{D} = \Bbb{R}^d$，可以写为<br>$$<br>\min_{x} f(x) \tag{1}<br>$$<br>其中$x \in \Bbb{R}^d$为输入变量，$f : \Bbb{R}^d \to \Bbb{R}$为目标函数。</p><p><strong>约束优化问题（Constrained Optimization）</strong>中变量x需要满足一些等式或不等式的约束。约束优化问题通常使用拉格朗日乘数法来进行求解。</p><h1 id="3-线性优化和非线性优化"><a href="#3-线性优化和非线性优化" class="headerlink" title="3. 线性优化和非线性优化"></a>3. 线性优化和非线性优化</h1><p>如果在公式(1) 中，目标函数和所有的约束函数都为线性函数，则该问题为<strong>线性规划问题（Linear Programming）</strong>。相反，如果目标函数或任何一个约束函数为非线性函数，则该问题为<strong>非线性规划问题（Nonlinear Programming）</strong>。</p><p>在非线性优化问题中，有一类比较特殊的问题是<strong>凸优化问题（Convex Programming）</strong>。在凸优化问题中，变量x 的可行域为凸集，即对于集合中任意两点，它们的连线全部位于在集合内部。目标函数f也必须为凸函数，即满足<br>$$<br>f(\alpha x + (1 − \alpha)y) \leq \alpha f(x) + (1 − \alpha)f(y), ∀\alpha \in [0, 1] \tag{2}<br>$$</p><p>凸优化问题是一种特殊的约束优化问题，需满足目标函数为凸函数，并且等式约束函数为线性函数，不等式约束函数为凹函数。</p><p>主要参考<a href="https://github.com/nndl/nndl.github.io" target="_blank" rel="noopener">https://github.com/nndl/nndl.github.io</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;数学优化（Mathematical Optimization）&lt;/strong&gt;问题，也叫最优化问题，是指在一定约束条件下，求解一个目标函数的最大值（或最小值）问题。&lt;/p&gt;
&lt;p&gt;数学优化问题的定义为：给定一个目标函数（也叫代价函数）$f : \cal{A} → \Bbb{R}$，寻找一个变量（也叫参数）$x^* \in \cal{D}$，使得对于所有$\cal{D}$中的$x，f(x^∗) ≤ f(x)$（最小化）；或者$f(x^∗) \geq f(x)$（最大化），其中$\cal{D}$为变量$x$的约束集，也叫可行域；$\cal{D}$中的变量被称为是可行解。&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术备忘" scheme="https://buracagyang.github.io/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/"/>
    
      <category term="基础知识" scheme="https://buracagyang.github.io/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>概率论2-随机过程</title>
    <link href="https://buracagyang.github.io/2019/06/14/probability-theory-2/"/>
    <id>https://buracagyang.github.io/2019/06/14/probability-theory-2/</id>
    <published>2019-06-14T12:09:53.000Z</published>
    <updated>2019-06-21T05:53:35.092Z</updated>
    
    <content type="html"><![CDATA[<p>同步于<a href="https://blog.csdn.net/buracag_mc" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/" target="_blank" rel="noopener">音尘杂记</a></p><p><strong>随机过程（Stochastic Process）</strong> 是一组随机变量$X_t$的集合，其中$t$属于一个索引（index）集合$\cal{T}$。索引集合$\cal{T}$可以定义在时间域或者空间域，但一般为时间域，以实数或正数表示。当t为实数时，随机过程为连续随机过程；当t为整数时，为离散随机过程。</p><p>日常生活中的很多例子包括股票的波动、语音信号、身高的变化等都可以看作是随机过程。常见的和时间相关的随机过程模型包括<strong>伯努利过程、随机游走（Random Walk）、马尔可夫过程</strong>等。和空间相关的随机过程通常称为<strong>随机场（Random Field）</strong>。比如一张二维的图片，每个像素点（变量）通过空间的位置进行索引，这些像素就组成了一个随机过程。</p><a id="more"></a><h1 id="1-马尔可夫过程"><a href="#1-马尔可夫过程" class="headerlink" title="1. 马尔可夫过程"></a>1. 马尔可夫过程</h1><p><strong>马尔可夫性质</strong> 在随机过程中，<strong>马尔可夫性质（Markov Property）</strong>是指一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态。以离散随机过程为例，假设随机变量$X_0,X_1, … ,X_T$构成一个随机过程。这些随机变量的所有可能取值的集合被称为<strong>状态空间（State Space）</strong>。如果$X_{t+1}$对于过去状态的条件概率分布仅是$X_t$的一个函数，则<br>$$<br>P(X_{t+1} = x_{t+1}|X_{0:t} = x_{0:t}) = P(X_{t+1} = x_{t+1}|X_t = x_t) \tag{1}<br>$$<br>其中$X{0:t}$表示变量集合$X_0,X_1, … ,X_t，x_{0:t}$表示为在状态空间中的状态序列。</p><blockquote><p>马尔可夫性质也可以描述为给定当前状态时，将来的状态与过去状态是条件独立的。</p></blockquote><h2 id="1-1-马尔可夫链"><a href="#1-1-马尔可夫链" class="headerlink" title="1.1 马尔可夫链"></a>1.1 马尔可夫链</h2><p>离散时间的马尔可夫过程也称为<strong>马尔可夫链（Markov Chain）</strong>。如果一个马尔可夫链的条件概率<br>$$<br>P(X_{t+1} = s_i|X_t = s_j) = T(s_i, s_j) \tag{2}<br>$$<br>在不同时间都是不变的，即和时间$t$无关，则称为<strong>时间同质的马尔可夫链（Time-Homogeneous Markov Chains）</strong>。如果状态空间是有限的，$T(s_i, s_j)$也可以用一个矩阵$T$表示，称为<strong>状态转移矩阵（Transition Matrix）</strong>，其中元素$t_{ij}$表示状态$s_i$转移到状态$s_j$的概率。</p><p><strong>平稳分布</strong> 假设状态空间大小为$M$，向量$\pi = [\pi_1, … , 、\pi_M]^T$ 为状态空间中的一个分布，满足$0 ≤ \pi_i ≤ 1$ 和$\sum_{i=1}^{M}\pi_i =1$。</p><p>对于状态转移矩阵为$T$的时间同质的马尔可夫链，如果存在一个分布$\pi$满足<br>$$<br>\pi = T \pi \tag{3}<br>$$<br>即分布$\pi$就称为该马尔可夫链的<strong>平稳分布（Stationary Distribution）</strong>。根据特征向量的定义可知，$\pi$为矩阵$T$的（归一化）的对应特征值为1的特征向量。</p><p>如果一个马尔可夫链的状态转移矩阵T满足<strong>所有状态可遍历性</strong>以及<strong>非周期性</strong>，那么对于任意一个初始状态分布$\pi^{(0)}$，将经过一定时间的状态转移之后，都会收敛到平稳分布，即<br>$$<br>\pi = \lim_{N \to \infty}T^Nπ^{(0)} \tag{4}<br>$$</p><blockquote><p>定理1 - <strong>细致平稳条件（Detailed Balance Condition）</strong>： 如果一个马尔科夫链满足<br>$$<br>\pi_it_{ij} = \pi_jt_{ji} \tag{5}<br>$$<br>则一定会收敛到平稳分布$\pi$。</p><p>细致平稳条件保证了从状态$i$转移到状态$j$的数量和从状态$j$转移到状态$i$的数量相一致，相互抵消，所以数量不发生改变。</p><p>细致平稳条件只是马尔科夫链收敛的充分条件，不是必要条件。</p></blockquote><h1 id="2-高斯过程"><a href="#2-高斯过程" class="headerlink" title="2. 高斯过程"></a>2. 高斯过程</h1><p><strong>高斯过程（Gaussian Process）</strong>也是一种应用广泛的随机过程模型。假设有一组连续随机变量$X_0,X_1, … ,X_T$ ，如果由这组随机变量构成的任一有限集合$X_{t_1,… ,t_k} = [X_{t_1} , … ,X_{t_n}]^T$都服从一个多元正态分布，那么这组随机变量为一个随机过程。高斯过程也可以定义为：如果$X_{t_1, … ,t_n}$ 的任一线性组合都服从一元正态分布，那么这组随机变量为一个随机过程。</p><p><strong>高斯过程回归</strong> 高斯过程回归（Gaussian Process Regression）是利用高斯过程来对一个函数分布进行建模。和机器学习中参数化建模（比如贝叶斯线性回归）相比，高斯过程是一种非参数模型，可以拟合一个黑盒函数，并给出拟合结果的置信度[Rasmussen, 2004]。</p><p>假设一个未知函数$f(x)$服从高斯过程，且为平滑函数。如果两个样本$x_1, x_2$比较接近，那么对应的$f(x_1), f(x_2)$也比较接近。假设从函数$f(x)$中采样有限个样本$X = [x_1, x_2, … , x_N]$，这$N$个点服从一个多元正态分布，<br>$$<br>[f(x_1), f(x_2), … , f(x_N)]^T \sim N(\mu(X),K(X,X)) \tag{6}<br>$$<br>其中$\mu(X) = [\mu(x_1), \mu(x_2), … , \mu(x_N)]^T$是均值向量，$K(X,X) = [k(x_i, x_j )]_{N×N}$是协方差矩阵，$k(x_i, x_j)$为核函数，可以衡量两个样本的相似度。</p><p>在高斯过程回归，一个常用的核函数是平方指数（Squared Exponential）函数<br>$$<br>k(x_i, x_j) = exp(\frac{−∥x_i − x_j∥^2}{2l^2}) \tag{7}<br>$$<br>其中$l$为超参数。当$x_i$和$x_j$越接近，其核函数的值越大，表明$f(x_i)$和$f(x_j)$越相关。</p><p>假设$f(x)$的一组带噪声的观测值为${(x_n, y_n)}_{n=1}^N$，其中$y_n \sim N(f(x_n), \sigma^2)$为正态分布，$\sigma$为噪声方差。</p><p>对于一个新的样本点$x^∗$，我们希望预测函数$y^∗ = f(x^∗)$。令$y = [y_1, y_2, … , y_n]$为已有的观测值，根据高斯过程的假设，$[y; y^∗]$ 满足</p><p><img src="/2019/06/14/probability-theory-2/fig1.png" alt="fig1"></p><p>其中$K(x^*,X) = [k(x^∗, x_1), … , k(x^∗, x^n)]$。</p><p>根据上面的联合分布，$y^∗$的后验分布为<br>$$<br>p(y^∗|X, y) = N(\hat{\mu}, \hat{\sigma}^2) \tag{9}<br>$$<br>其中均值$\hat{\mu}$和方差$\hat{\sigma}$为</p><p><img src="/2019/06/14/probability-theory-2/fig2.png" alt="fig2"></p><p>从公式(10) 可以看出，均值函数$\mu(x)$可以近似地互相抵消。在实际应用中，一般假设$\mu(x) = 0$，均值$\hat{\mu}$可以将简化为<br>$$<br>\hat{\mu} = K(x^∗,X)(K(X,X) + \sigma^2 I)^{−1}y \tag{12}<br>$$<br>高斯过程回归可以认为是一种有效的贝叶斯优化方法，广泛地应用于机器学习中。</p><p>主要参考<a href="https://github.com/nndl/nndl.github.io" target="_blank" rel="noopener">https://github.com/nndl/nndl.github.io</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;随机过程（Stochastic Process）&lt;/strong&gt; 是一组随机变量$X_t$的集合，其中$t$属于一个索引（index）集合$\cal{T}$。索引集合$\cal{T}$可以定义在时间域或者空间域，但一般为时间域，以实数或正数表示。当t为实数时，随机过程为连续随机过程；当t为整数时，为离散随机过程。&lt;/p&gt;
&lt;p&gt;日常生活中的很多例子包括股票的波动、语音信号、身高的变化等都可以看作是随机过程。常见的和时间相关的随机过程模型包括&lt;strong&gt;伯努利过程、随机游走（Random Walk）、马尔可夫过程&lt;/strong&gt;等。和空间相关的随机过程通常称为&lt;strong&gt;随机场（Random Field）&lt;/strong&gt;。比如一张二维的图片，每个像素点（变量）通过空间的位置进行索引，这些像素就组成了一个随机过程。&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术备忘" scheme="https://buracagyang.github.io/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/"/>
    
      <category term="基础知识" scheme="https://buracagyang.github.io/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>概率论1-随机事件和概率</title>
    <link href="https://buracagyang.github.io/2019/06/14/probability-theory-1/"/>
    <id>https://buracagyang.github.io/2019/06/14/probability-theory-1/</id>
    <published>2019-06-14T02:15:42.000Z</published>
    <updated>2019-06-21T05:53:25.791Z</updated>
    
    <content type="html"><![CDATA[<p>同步于<a href="https://blog.csdn.net/buracag_mc" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/" target="_blank" rel="noopener">音尘杂记</a></p><p>主要回顾概率论中关于样本空间、随机事件和常见概率分布的基础知识。</p><h1 id="1-样本空间"><a href="#1-样本空间" class="headerlink" title="1. 样本空间"></a>1. 样本空间</h1><p><strong>样本空间</strong> 是一个随机试验所有可能结果的集合。例如，如果抛掷一枚硬币，那么样本空间就是集合{正面，反面}。如果投掷一个骰子，那么样本空间就是{1, 2, 3, 4, 5, 6}。随机试验中的每个可能结果称为样本点。</p><p>有些试验有两个或多个可能的样本空间。例如，从52 张扑克牌中随机抽出一张，样本空间可以是数字（A到K），也可以是花色（黑桃，红桃，梅花，方块）。如果要完整地描述一张牌，就需要同时给出数字和花色，这时样本空间可以通过构建上述两个样本空间的笛卡儿乘积来得到。</p><a id="more"></a><h1 id="2-随机事件"><a href="#2-随机事件" class="headerlink" title="2. 随机事件"></a>2. 随机事件</h1><p><strong>随机事件</strong>（或简称<strong>事件</strong>） 指的是一个被赋予概率的事物集合，也就是样本空间中的一个子集。<strong>概率(Probability)</strong>表示一个随机事件发生的可能性大小，为0 到1 之间的一个非负实数。比如，一个0.5 的概率表示一个事件有50%的可能性发生。</p><p>对于一个机会均等的抛硬币动作来说，其样本空间为“正面”或“反面”。我们可以定义各个随机事件，并计算其概率。比如，</p><ul><li>{正面}，其概率为0.5；</li><li>{反面}，其概率为0.5；</li><li>空集∅，不是正面也不是反面，其概率为0；</li><li>{正面| 反面}，不是正面就是反面，其概率为1</li></ul><h1 id="3-随机变量"><a href="#3-随机变量" class="headerlink" title="3. 随机变量"></a>3. 随机变量</h1><p>在随机试验中，试验的结果可以用一个数$X$来表示，这个数$X$是随着试验结果的不同而变化的，是样本点的一个函数。我们把这种数称为<strong>随机变量（Random Variable）</strong>。例如，随机掷一个骰子，得到的点数就可以看成一个随机变量$X$，$X$的取值为{1, 2, 3, 4, 5, 6}。</p><p>如果随机掷两个骰子，整个事件空间Ω可以由36 个元素组成：<br>$$<br>Ω = \{(i, j)|i = 1, … , 6; j = 1, … , 6\} \tag{1}<br>$$<br>一个随机事件也可以定义多个随机变量。比如在掷两个骰子的随机事件中，可以定义随机变量$X$为获得的两个骰子的点数和，也可以定义随机变量$Y$为获得的两个骰子的点数差。随机变量$X$可以有11个整数值，而随机变量Y 只有6个。<br>$$<br>\begin{eqnarray}<br>X(i, j) &amp;:=&amp; i + j, x = 2, 3, … , 12 \tag{2} \\<br>Y (i, j) &amp;:=&amp; | i − j |, y = 0, 1, 2, 3, 4, 5 \tag{3}<br>\end{eqnarray}<br>$$</p><p>其中$i, j$分别为两个骰子的点数。</p><h2 id="3-1-离散随机变量"><a href="#3-1-离散随机变量" class="headerlink" title="3.1 离散随机变量"></a>3.1 离散随机变量</h2><p>如果随机变量$X$所有可能取的值为有限可列举的，有$n$个有限取值${x_1, … , x_n}$,则称$X$为离散随机变量</p><p>要了解$X$的统计规律，就必须知道它取每种可能值$x_i$的概率，即<br>$$<br>P(X = x_i) = p(x_i), \qquad ∀i \in [1, n] \tag{4}<br>$$<br>$p(x_1), … , p(x_n)$称为离散型随机变量$X$的<strong>概率分布（Probability Distribution）</strong>或<strong>分布</strong>，并且满足<br>$$<br>\begin{eqnarray}<br>\sum_{i=1}^{n}p(x_i) &amp;=&amp; 1 \\<br>p(x_i) &amp;\geq&amp; 0, \qquad \forall i \in [1, n]<br>\end{eqnarray}\tag{5}<br>$$<br>常见的离散随机变量的概率分布有：</p><p><strong>伯努利分布</strong> 在一次试验中，事件A出现的概率为$\mu$，不出现的概率为$1−\mu$。若用变量$X$表示事件A出现的次数，则$X$的取值为0和1，其相应的分布为:<br>$$<br>p(x) = μ^x(1 − μ)^{(1−x)} \tag{6}<br>$$<br>这个分布称为<strong>伯努利分布（Bernoulli Distribution）</strong>,又名两点分布或者<strong>0-1分布</strong>。</p><p><strong>二项分布</strong> 在n次伯努利分布中，若以变量$X$表示事件A出现的次数，则$X$的取值为{0, · · · , n}，其相应的分布为<strong>二项分布（Binomial Distribution）</strong>。<br>$$<br>P(X = k) = \tbinom{n}{k}μ^k(1 − μ)^{n−k}, \quad k = 1, … , n \tag{7}<br>$$<br>其中$\tbinom{n}{k}$为二项式系数（这就是二项分布的名称的由来），表示从$n$个元素中取出$k$个元素而不考虑其顺序的<strong>组合</strong>的总数。</p><h2 id="3-2-连续随机变量"><a href="#3-2-连续随机变量" class="headerlink" title="3.2 连续随机变量"></a>3.2 连续随机变量</h2><p>与离散随机变量不同，一些随机变量$X$的取值是不可列举的，由全部实数或者由一部分区间组成，比如<br>$$<br>X = \{x|a ≤ x ≤ b\}, -\infty &lt; a &lt; b &lt; \infty \tag{8}<br>$$<br>则称$X$为<strong>连续随机变量</strong>。连续随机变量的值是不可数及无穷尽的。</p><p>对于连续随机变量$X$，它取一个具体值$x_i$的概率为0，这个离散随机变量截然不同。因此用列举连续随机变量取某个值的概率来描述这种随机变量不但做不到，也毫无意义。</p><p>连续随机变量$X$的概率分布一般用<strong>概率密度函数（Probability Density Function，PDF）</strong> p(x)来描述。p(x)为可积函数，并满足<br>$$<br>\begin{eqnarray}<br>\int_{-\infty}^{\infty} p(x)dx &amp;=&amp; 1 \\<br>p(x) &amp;≥&amp; 0<br>\end{eqnarray} \tag{9}<br>$$<br>给定概率密度函数p(x)，便可以计算出随机变量落入某一个区间的概率，而p(x)本身反映了随机变量取落入x的非常小的邻近区间中的概率大小。常见的连续随机变量的概率分布有：</p><p><strong>均匀分布</strong> 若a, b为有限数，[a, b]上的<strong>均匀分布（Uniform Distribution）</strong>的概率密度函数定义为<br>$$<br>p(x) = \begin{cases}<br>\frac{1}{b-a} &amp; a\leq x \leq b \\<br>0 &amp; x&gt;b或x&lt;a<br>\end{cases} \tag{10}<br>$$</p><p><strong>正态分布</strong> 正态分布（Normal Distribution），又名<strong>高斯分布（Gaussian Distribution）</strong>，是自然界最常见的一种分布，并且具有很多良好的性质，在很多领域都有非常重要的影响力，其概率密度函数为<br>$$<br>p(x) = \frac{1}{\sqrt{2\pi}\sigma}exp(− \frac{(x − μ)^2}{2\sigma^2}) \tag{11}<br>$$<br>其中$\sigma &gt; 0$，$\mu$和$\sigma$均为常数。若随机变量$X$服从一个参数为$\mu$和$\sigma$的概率分布，简记为<br>$$<br>X \sim \cal N(\mu, \sigma^2) \tag{12}<br>$$<br>当$\mu = 0，\sigma = 1$时，称为<strong>标准正态分布（Standard Normal Distribution）</strong>。</p><h2 id="3-3-累积分布函数"><a href="#3-3-累积分布函数" class="headerlink" title="3.3 累积分布函数"></a>3.3 累积分布函数</h2><p>对于一个随机变量$X$，其<strong>累积分布函数（Cumulative Distribution Function，CDF）</strong>是随机变量$X$的取值小于等于$x$的概率。<br>$$<br>cdf(x) = P(X \leq x) \tag{13}<br>$$<br>以连续随机变量$X$为例，累积分布函数定义为<br>$$<br>cdf(x) =\int_{-\infty}^{x}p(t)dt \tag{14}<br>$$<br>其中p(x)为概率密度函数。下图给出了标准正态分布的累计分布函数和概率密度函数。</p><p><img src="/2019/06/14/probability-theory-1/pdf-cdf.png" alt="pdf-cdf"></p><h1 id="4-随机向量"><a href="#4-随机向量" class="headerlink" title="4. 随机向量"></a>4. 随机向量</h1><p><strong>随机向量</strong> 是指一组随机变量构成的向量。如果$X_1,X_2, … ,X_n$ 为$n$个随机变量, 那么称$[X_1,X_2, … ,X_n]$为一个$n$维随机向量。一维随机向量称为随机变量。随机向量也分为<strong>离散随机向量</strong>和<strong>连续随机向量</strong>。</p><h2 id="4-1离散随机向量"><a href="#4-1离散随机向量" class="headerlink" title="4.1离散随机向量"></a>4.1离散随机向量</h2><p>离散随机向量的<strong>联合概率分布（Joint Probability Distribution）</strong>为<br>$$<br>P(X_1 = x_1,X_2 = x_2, … ,X_n = x_n) = p(x_1, x_2, … , x_n) \tag{15}<br>$$<br>其中$x_i \in \omega_i$为变量$X_i$的取值，$\omega_i$为变量$X_i$的样本空间。和离散随机变量类似，离散随机向量的概率分布满足<br>$$<br>\begin{eqnarray}<br>&amp;p(x_1, x_2, … , x_n) \geq 0, \quad ∀x_1 \in \omega_1, x_2 \in \omega_2, … , x_n \in \omega_n \tag{16} \\<br>&amp;\sum_{x_1 \in \omega_1}\sum_{x_2 \in \omega_2}…\sum_{x_n \in \omega_n}p(x_1, x_2, … , x_n) = 1 \tag{17}<br>\end{eqnarray}<br>$$<br><strong>多项分布</strong> 一个常见的离散向量概率分布为<strong>多项分布（Multinomial Distribution）</strong>。多项分布是二项分布在随机向量的推广。假设一个袋子中装了很多球，总共有$K$个不同的颜色。我们从袋子中取出$n$个球。每次取出一个时，就在袋子中放入一个同样颜色的球（或者说有放回的抽样）。这样保证同一颜色的球在不同试验中被取出的概率是相等的。令$X$为一个$K$维随机向量，每个元素$X_k(k = 1, … ,K)$为取出的$n$个球中颜色为$k$的球的数量，则$X$服从多项分布，其概率分布为<br>$$<br>p(x_1, … , x_K|\mu) = \frac{n!}{x_1! … x_K!}μ_1^{x_1} … μ_K^{x_K} \tag{18}<br>$$<br>其中$\mu = [\mu_1, … , \mu_K]^T$分别为每次抽取的球的颜色为1, … ,K的概率；$x_1, … , x_K$为非负整数，并且满足$\sum_{k=1}^{K}x_k = n$。</p><p>多项分布的概率分布也可以用gamma函数表示：<br>$$<br>p(x_1, … , x_K|\mu) = \frac{\Gamma(\sum_k x_k+1)}{\prod_k \Gamma(x_k+1)}\prod_{k=1}^{K}\mu_k^{x_k} \tag{19}<br>$$<br>其中$\Gamma(z) = \int_{0}^{\infty}\frac{t^{z−1}}{exp(t)}dt$为gamma函数。这种表示形式和狄利克雷分布(  Dirichlet Distribution)类似，而狄利克雷分布可以作为多项分布的共轭先验。</p><h2 id="4-2-连续随机向量"><a href="#4-2-连续随机向量" class="headerlink" title="4.2 连续随机向量"></a>4.2 连续随机向量</h2><p>连续随机向量的其<strong>联合概率密度函数（Joint Probability Density Function）</strong>满足<br>$$<br>\begin{eqnarray}<br>p(x) = p(x_1, … , x_n) ≥ 0 \tag{20} \\<br>\int_{-\infty}^{+\infty} … \int_{-\infty}^{+\infty}p(x_1, … , x_n)dx_1 … dx_n = 1 \tag{21}<br>\end{eqnarray}<br>$$<br><strong>多元正态分布</strong> 一个常见的连续随机向量分布为<strong>多元正态分布（Multivariate Normal</strong><br><strong>Distribution）</strong>，也称为<strong>多元高斯分布（Multivariate Gaussian Distribution）</strong>。若$n$维随机向量$X = [X_1, … ,X_n]^T$服从$n$元正态分布，其密度函数为<br>$$<br>p(x) = \frac{1}{(2π)^{n/2}|\sum|^{1/2}} exp(-\frac{1}{2}(x−\mu)^T\sum^{−1}(x−\mu)) \tag{22}<br>$$<br>其中$\mu$为多元正态分布的均值向量，$\sum$为多元正态分布的协方差矩阵，$|\sum|$表示$\sum$的行列式。</p><p><strong>各项同性高斯分布</strong> 如果一个多元高斯分布的协方差矩阵简化为$\sum = \sigma^2I$，即每一个维随机变量都独立并且方差相同，那么这个多元高斯分布称为<strong>各项同性高斯分布（Isotropic Gaussian Distribution）</strong>。</p><p><strong>Dirichlet 分布</strong> 一个$n$维随机向量$X$的Dirichlet 分布为<br>$$<br>p(x|\alpha) = \frac{\Gamma(\alpha_0)}{\Gamma(\alpha_1) … \Gamma(\alpha_n)} \prod_{i=1}^{n}x_i^{\alpha_i - 1} \tag{23}<br>$$</p><p>其中$\alpha = [\alpha_1, … , \alpha_K]^T$为Dirichlet分布的参数。</p><h1 id="5-边际分布"><a href="#5-边际分布" class="headerlink" title="5. 边际分布"></a>5. 边际分布</h1><p>对于二维离散随机向量$(X, Y)$，假设$X$取值空间为$\Omega_x$，$Y$取值空间为$\Omega_y$。其联合概率分布满足<br>$$<br>p(x, y) \geq 0,\sum_{x\in \Omega_x}\sum_{y \in \Omega_y}p(x_i, y_j) = 1 \tag{24}<br>$$<br>对于联合概率分布p(x, y)，我们可以分别对x和y进行求和。</p><p>(1) 对于固定的x，<br>$$<br>\sum_{y\in \Omega_y}p(x, y) = P(X = x) = p(x) \tag{25}<br>$$<br>(2) 对于固定的y，<br>$$<br>\sum_{x \in \Omega_x}p(x, y) = P(Y = y) = p(y) \tag{26}<br>$$<br>由离散随机向量$(X, Y)$的联合概率分布，对$Y$的所有取值进行求和得到$X$的概率分布；而对$X$的所有取值进行求和得到$Y$的概率分布。这里p(x)和p(y)就称为p(x, y)的<strong>边际分布（Marginal Distribution）</strong>。</p><p>对于二维连续随机向量(X, Y)，其边际分布为：<br>$$<br>\begin{eqnarray}<br>p(x) = \int_{-\infty}^{+\infty}p(x, y)dy \tag{27} \\<br>p(y) = \int_{-\infty}^{+\infty}p(x, y)dx \tag{28}<br>\end{eqnarray}<br>$$<br>一个二元正态分布的边际分布仍为正态分布。</p><h1 id="6-条件概率分布"><a href="#6-条件概率分布" class="headerlink" title="6. 条件概率分布"></a>6. 条件概率分布</h1><p>对于离散随机向量$(X, Y)$，已知$X = x$的条件下，随机变量$Y = y$的<strong>条件概率（Conditional Probability）</strong>为：<br>$$<br>p(y|x) = P(Y = y|X = x) = \frac{p(x, y)}{p(x)} \tag{29}<br>$$<br>这个公式定义了随机变量$Y$关于随机变量X的条件概率分布（Conditional Probability Distribution），简称条件分布。</p><p>对于二维连续随机向量$(X, Y)$，已知$X = x$的条件下，随机变量$Y = y$的<strong>条件概率密度函数（Conditional Probability Density Function）</strong>为<br>$$<br>p(y|x) = \frac{p(x, y)}{p(x)} \tag{30}<br>$$<br>同理，已知$Y = y$的条件下，随机变量$X = x$的条件概率密度函数为<br>$$<br>p(x|y) = \frac{p(x, y)}{p(y)} \tag{31}<br>$$</p><p>通过公式(30) 和(31)，我们可以得到两个条件概率p(y|x) 和p(x|y) 之间的关系。<br>$$<br>p(y|x) = \frac{p(x|y)p(y)}{p(x)} \tag{32}<br>$$<br>这个公式称为<strong>贝叶斯定理（Bayes’ Theorem）</strong>，或贝叶斯公式。</p><h1 id="7-独立与条件独立"><a href="#7-独立与条件独立" class="headerlink" title="7. 独立与条件独立"></a>7. 独立与条件独立</h1><p>对于两个离散（或连续）随机变量$X$和$Y$，如果其联合概率（或联合概率密度函数）p(x, y) 满足<br>$$<br>p(x, y) = p(x)p(y) \tag{33}<br>$$<br>则称X 和Y相互<strong>独立（independence）</strong>，记为$X \perp !!! \perp Y$。</p><p>对于三个离散（或连续）随机变量X、Y 和Z，如果条件概率（或联合概率密度函数）p(x, y|z) 满足<br>$$<br>p(x, y|z) = P(X = x, Y = y|Z = z) = p(x|z)p(y|z) \tag{34}<br>$$</p><p>则称在给定变量$Z$时，$X$和$Y$<strong>条件独立（conditional independence）</strong>，记为$X \perp !!! \perp Y|Z$。</p><h1 id="8-期望和方差"><a href="#8-期望和方差" class="headerlink" title="8. 期望和方差"></a>8. 期望和方差</h1><p><strong>期望</strong> 对于离散变量$X$，其概率分布为$p(x_1), … , p(x_n)$，$X$的期望（Expectation）或均值定义为<br>$$<br>\Bbb{E}[X] = \sum_{i=1}^{n}x_ip(x_i) \tag{35}<br>$$<br>对于连续随机变量$X$，概率密度函数为$p(x)$，其期望定义为<br>$$<br>\Bbb{E}[X] = \int_{\Bbb{R}}xp(x) dx \tag{36}<br>$$<br><strong>方差</strong> 随机变量$X$的方差（Variance）用来定义它的概率分布的离散程度，定义为<br>$$<br>var(X) = \Bbb{E}[X − \Bbb{E}(X)]^2 \tag{37}<br>$$<br>随机变量$X$的方差也称为它的二阶矩。$\sqrt{var(X)}$则称为$X$的根方差或标准差。</p><p><strong>协方差</strong> 两个连续随机变量X和Y的<strong>协方差（Covariance）</strong>用来衡量两个随机变量的分布之间的总体变化性，定义为<br>$$<br>cov(X, Y) = \Bbb{E}[(X − \Bbb(X))((Y − \Bbb{E}(Y))] \tag{38}<br>$$<br>协方差经常也用来衡量两个随机变量之间的线性相关性。如果两个随机变量的协方差为0，那么称这两个随机变量是<strong>线性不相关</strong>。两个随机变量之间没有这里的线性相关性，并非表示它们之间独立的，可能存在某种非线性的函数关系。反之，如果X 与Y是统计独立的，那么它们之间的协方差一定为0。</p><p><strong>协方差矩阵</strong> 两个m和n维的连续随机向量X和Y，它们的协方差（Covariance）为m × n的矩阵，定义为<br>$$<br>cov(X,Y) = \Bbb{E}[(X − \Bbb{E}(X))(Y − \Bbb{E}(Y))^T] \tag{39}<br>$$<br>协方差矩阵$cov(X,Y)$的第$(i, j)$个元素等于随机变量$X_i$和$Y_j$的协方差。两个向量变量的协方差$cov(X,Y)$与$cov(Y,X)$互为转置关系。如果两个随机向量的协方差矩阵为对角阵，那么称这两个随机向量是无关的。</p><p>单个随机向量X的协方差矩阵定义为<br>$$<br>cov(X) = cov(X,X) \tag{40}<br>$$</p><h2 id="8-1-Jensen不等式"><a href="#8-1-Jensen不等式" class="headerlink" title="8.1 Jensen不等式"></a>8.1 Jensen不等式</h2><p>如果$X$是随机变量，$g$是凸函数，则<br>$$<br>g(\Bbb{E}[X]) \leq \Bbb{E}[g(X)] \tag{41}<br>$$</p><p>等式当且仅当$X$是一个常数或$g$是线性时成立。</p><h2 id="8-2-大数定律和中心极限定理"><a href="#8-2-大数定律和中心极限定理" class="headerlink" title="8.2 大数定律和中心极限定理"></a>8.2 大数定律和中心极限定理</h2><p><strong>大数定律（Law Of Large Numbers）</strong> 是指$n$个样本$X_1, … ,X_n$是独立同分布的，即$E[X_1] = … = E[X_n] = \mu$，那么其均值收敛于期望值$\mu$<br>$$<br>\lim_{n \to \infty} \bar{X}_n = \lim_{n\to \infty} \frac{1}{n}(X_1 + … + X_n) \to \mu \tag{42}<br>$$<br><strong>中心极限定理(Central Limit Theorem)</strong> 是指$n$个样本$X_1, … ,X_n$是独立同分布的，则对任意x，分布函数<br>$$<br>F_n(x) = P(\frac{\sum_{i=1}^{n}X_i - n\mu}{\sigma \sqrt{n}} \leq x)<br>$$<br>满足：</p><p>$\lim_{n \to \infty}  F_n(x)$ 近似服从标准正态分布 $\cal{N}(0, 1)$。</p><p>主要参考<a href="https://github.com/nndl/nndl.github.io" target="_blank" rel="noopener">https://github.com/nndl/nndl.github.io</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;主要回顾概率论中关于样本空间、随机事件和常见概率分布的基础知识。&lt;/p&gt;
&lt;h1 id=&quot;1-样本空间&quot;&gt;&lt;a href=&quot;#1-样本空间&quot; class=&quot;headerlink&quot; title=&quot;1. 样本空间&quot;&gt;&lt;/a&gt;1. 样本空间&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;样本空间&lt;/strong&gt; 是一个随机试验所有可能结果的集合。例如，如果抛掷一枚硬币，那么样本空间就是集合{正面，反面}。如果投掷一个骰子，那么样本空间就是{1, 2, 3, 4, 5, 6}。随机试验中的每个可能结果称为样本点。&lt;/p&gt;
&lt;p&gt;有些试验有两个或多个可能的样本空间。例如，从52 张扑克牌中随机抽出一张，样本空间可以是数字（A到K），也可以是花色（黑桃，红桃，梅花，方块）。如果要完整地描述一张牌，就需要同时给出数字和花色，这时样本空间可以通过构建上述两个样本空间的笛卡儿乘积来得到。&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术备忘" scheme="https://buracagyang.github.io/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/"/>
    
      <category term="基础知识" scheme="https://buracagyang.github.io/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>微积分2-常见函数的导数</title>
    <link href="https://buracagyang.github.io/2019/06/12/calculus-2/"/>
    <id>https://buracagyang.github.io/2019/06/12/calculus-2/</id>
    <published>2019-06-12T12:44:46.000Z</published>
    <updated>2019-07-28T08:55:50.444Z</updated>
    
    <content type="html"><![CDATA[<p>同步于<a href="https://blog.csdn.net/buracag_mc" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/" target="_blank" rel="noopener">音尘杂记</a></p><p>在微积分1中已经附上了一个常见函数形式的导数，下文主要是关于向量函数及其导数，以及在机器学习和神经网络中常见的Logistic函数、Softmax函数的导数形式。</p><a id="more"></a><h1 id="1-向量函数及其导数"><a href="#1-向量函数及其导数" class="headerlink" title="1. 向量函数及其导数"></a>1. 向量函数及其导数</h1><p>$$<br>\begin{eqnarray}<br>\frac{\partial x}{\partial x} &amp;=&amp; I \tag{1.1} \\<br>\frac{\partial Ax}{\partial x} &amp;=&amp; A^T \tag{1.2} \\<br>\frac{\partial x^TA}{\partial x} &amp;=&amp; A \tag{1.3}<br>\end{eqnarray}<br>$$</p><h1 id="2-按位计算的向量函数及其导数"><a href="#2-按位计算的向量函数及其导数" class="headerlink" title="2. 按位计算的向量函数及其导数"></a>2. 按位计算的向量函数及其导数</h1><p>假设一个函数$f(x)$的输入是标量$x$。对于一组$K$个标量$x_1, … , x_K$，我们可以通过$f(x)$得到另外一组$K$个标量$z_1, … , z_K$，<br>$$<br>z_k = f(x_k), ∀k = 1, … ,K \tag{1.4}<br>$$<br>为了简便起见，我们定义$x = [x_1, … , x_K]^T，z = [z_1, … , z_K]^T$，<br>$$<br>z = f(x) \tag{1.5}<br>$$<br>其中$f(x)$是按位运算的，即$[f(x)]_i = f(x_i)$。</p><p>当$x$为标量时，$f(x)$的导数记为$f′(x)$。当输入为$K$维向量$x = [x_1, … , x_K]^T$时，其导数为一个对角矩阵。<br>$$<br>\begin{eqnarray}<br>\frac{\partial f(x)}{\partial x} &amp;=&amp; [\frac{\partial f(x_j)}{\partial x_i}]_{K \times K} \ <br>&amp;=&amp; \begin {bmatrix}<br>&amp;f’(x_1)&amp; \quad &amp;0&amp; \quad &amp;…&amp; \quad &amp;0&amp; \\<br>&amp;0&amp; \quad &amp;f’(x_2)&amp; \quad &amp;…&amp; \quad &amp;0&amp; \\<br>&amp;\vdots&amp; \quad &amp;\vdots&amp; \quad &amp;\vdots&amp; \quad &amp;\vdots&amp; \quad \\<br>&amp;0&amp; \quad &amp;0&amp; \quad &amp;…&amp; \quad &amp;f’(x_K)&amp; \\<br>\end {bmatrix} \\<br>&amp;=&amp; diag(f’(x))<br>\end{eqnarray}  \tag{1.6}<br>$$</p><h1 id="3-Logistic函数的导数"><a href="#3-Logistic函数的导数" class="headerlink" title="3. Logistic函数的导数"></a>3. Logistic函数的导数</h1><p>关于logistic函数其实在博文<a href="https://buracagyang.github.io/2019/05/29/logistic-loss-function/">‘Logistic loss函数’</a>中已经有所介绍，接下来要说是更广义的logistic函数的定义：<br>$$<br>logistic(x) = \frac{L}{1 + exp(−k(x − x_0))} \tag{1.7}<br>$$<br>其中，$x_0$是中心点，$L$是最大值，$k$是曲线的倾斜度。下图给出了几种不同参数的Logistic函数曲线。当$x$趋向于$−\infty$时，logistic(x)接近于0；当$x$趋向于$+\infty$时，logistic(x) 接近于$L$。</p><p><img src="/2019/06/12/calculus-2/logistic.png" alt="logistic"></p><p>当参数为($k = 1,  x_0 = 0, L = 1$) 时，Logistic 函数称为标准Logistic 函数，记为f(x)。<br>$$<br>f(x) = \frac{1}{1 + exp(−x)} \tag{1.8}<br>$$<br>标准logistic函数有两个重要的性质如下：<br>$$<br>\begin{eqnarray}<br>f(x) &amp;=&amp; 1 - f(x) \tag{1.9} \\<br>f’(x) &amp;=&amp; f(x)(1 - f(x)) \tag{1.10}<br>\end{eqnarray}<br>$$<br>当输入为$K$维向量$x=[x_1, …, x_K]^T$时，其导数为：<br>$$<br>f’(x) = diag(f(x) \odot (1 − f(x))) \tag{1.11}<br>$$</p><h1 id="4-Softmax函数的导数"><a href="#4-Softmax函数的导数" class="headerlink" title="4. Softmax函数的导数"></a>4. Softmax函数的导数</h1><p>Softmax函数是将多个标量映射为一个概率分布。对于$K$个标量$x_1, … , x_K$，softmax 函数定义为<br>$$<br>z_k = softmax(x_k) = \frac{exp(x_k)}{\sum_{i=1}^{K}exp(x_i)} \tag{1.12}<br>$$<br>这样，我们可以将$K$个变量$x_1, … , x_K$转换为一个分布：$z_1, … , z_K$，满足<br>$$<br>z_k \in [0, 1], ∀k, \quad  \sum_{k=1}^{K}z_k = 1 \tag{1.13}<br>$$<br>当Softmax函数的输入为$K$维向量$x$时，<br>$$<br>\begin{eqnarray}<br>\hat{z} &amp;=&amp; softmax(x) \\<br>&amp;=&amp; \frac{1}{\sum_{k=1}^{K}exp(x_k)}\begin{bmatrix}<br>exp(x_1) \\<br>\vdots \\<br>exp(x_K) \\<br>\end{bmatrix} \\<br>&amp;=&amp; \frac{exp(x)}{\sum_{k=1}^{K}exp(x)} \ <br>&amp;=&amp; \frac{exp(x)}{1_K^Texp(x)} \\<br>\end{eqnarray} \tag{1.14}<br>$$<br>其中$1_K = [1, … , 1]_{K×1}$是$K$维的全1向量。</p><p>Softmax函数的导数为<br>$$<br>\begin{eqnarray}<br>\frac{\partial softmax(x)}{\partial x} &amp;=&amp; \frac{\partial(\frac{exp(x)}{1_K^Texp(x)})}{\partial x} \tag{1.15} \\<br>&amp;=&amp; \frac{1}{1_K^Texp(x)}\frac{\partial exp(x)}{\partial(x)} + \frac{\partial(\frac{1}{1_K^Texp(x)})}{\partial x}(exp(x))^T \tag{1.16} \\<br>&amp;=&amp; \frac{diag(exp(x))}{1_K^Texp(x)} - (\frac{1}{(1_K^Texp(x))^2})\frac{\partial(1_K^Texp(x))}{\partial x}(exp(x))^T \tag{1.17} \\<br>&amp;=&amp; \frac{diag(exp(x))}{1_K^Texp(x)} - (\frac{1}{(1_K^Texp(x))^2})diag(exp(x))1_K(exp(x))^T \tag{1.18} \\<br>&amp;=&amp; \frac{diag(exp(x))}{1_K^Texp(x)} - (\frac{1}{(1_K^Texp(x))^2})exp(x)(exp(x))^T \tag{1.19} \\<br>&amp;=&amp; diag(\frac{exp(x)}{1_K^Texp(x)}) - \frac{exp(x)}{1_K^Texp(x)}.\frac{(exp(x))^T}{1_K^Texp(x)} \tag{1.20} \\<br>&amp;=&amp; diag(softmax(x)) - softmax(x).softmax(x)^T \tag{1.21}<br>\end{eqnarray}<br>$$<br>其中式(1.16)请参考 <a href="https://buracagyang.github.io/2019/06/12/calculus-1/">‘微积分1-导数’</a> 式(1.13)。</p><p>主要参考<a href="https://github.com/nndl/nndl.github.io" target="_blank" rel="noopener">https://github.com/nndl/nndl.github.io</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在微积分1中已经附上了一个常见函数形式的导数，下文主要是关于向量函数及其导数，以及在机器学习和神经网络中常见的Logistic函数、Softmax函数的导数形式。&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术备忘" scheme="https://buracagyang.github.io/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/"/>
    
      <category term="基础知识" scheme="https://buracagyang.github.io/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>微积分1-导数</title>
    <link href="https://buracagyang.github.io/2019/06/12/calculus-1/"/>
    <id>https://buracagyang.github.io/2019/06/12/calculus-1/</id>
    <published>2019-06-12T09:23:57.000Z</published>
    <updated>2019-07-28T08:55:43.551Z</updated>
    
    <content type="html"><![CDATA[<p>同步于<a href="https://blog.csdn.net/buracag_mc" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/" target="_blank" rel="noopener">音尘杂记</a></p><p>微积分1，主要回顾关于微积分中关于导数的相关知识。错误之处，还望诸君不吝指教。</p><a id="more"></a><h1 id="1-导数基础"><a href="#1-导数基础" class="headerlink" title="1. 导数基础"></a>1. 导数基础</h1><p><strong>导数（Derivative）</strong> 是微积分学中重要的基础概念。<br>对于定义域和值域都是实数域的函数$f : \mathbb{R} \to \mathbb{R}$，若$f(x)$在点$x_0$的某个邻域$\triangle x$内，极限定义如下<br>$$<br>f’(x_0) = \lim_{\triangle x \to 0} \frac{f(x_0 + \triangle x) − f(x_0)}{\triangle x} \tag{1.1}<br>$$<br>若极限存在，则称函数$f(x)$在点$x_0$处可导，$f′(x_0)$称为其导数，或导函数，也可以记为$\frac{df(x_0)}{dx}$。在几何上，导数可以看做函数曲线上的切线斜率。</p><p>给定一个连续函数，计算其导数的过程称为微分（Differentiation）。微分的逆过程为积分（Integration）。函数$f(x)$的积分可以写为<br>$$<br>F(x) = \int f(x)dx \tag{1.2}<br>$$<br>其中$F(x)$称为$f(x)$的原函数。</p><p>若函数$f(x)$在其定义域包含的某区间内每一个点都可导，那么也可以说函数$f(x)$在这个区间内可导。如果一个函数$f(x)$在定义域中的所有点都存在导数，则$f(x)$为可微函数（Differentiable Function）。<strong>可微函数一定连续，但连续函数不一定可微</strong>。例如函数$|x|$为连续函数，但在点x = 0处不可导。下表是几个常见函数的导数：</p><table><thead><tr><th style="text-align:center">函数</th><th style="text-align:center">函数形式</th><th style="text-align:center">导数</th></tr></thead><tbody><tr><td style="text-align:center">常数函数</td><td style="text-align:center">$f(x) = C$，其中C为常数</td><td style="text-align:center">$f’(x) = 0$</td></tr><tr><td style="text-align:center">幂函数</td><td style="text-align:center">$f(x) = x^r$， 其中r是非零实数</td><td style="text-align:center">$f’(x) = rx^{r-1}$</td></tr><tr><td style="text-align:center">指数函数</td><td style="text-align:center">$f(x) = exp(x)$</td><td style="text-align:center">$f’(x) = exp(x)$</td></tr><tr><td style="text-align:center">对数函数</td><td style="text-align:center">$f(x) = log_ax$</td><td style="text-align:center">$f’(x) = \frac{1}{xlna}$</td></tr></tbody></table><p><strong>高阶导数</strong> 对一个函数的导数继续求导，可以得到高阶导数。函数$f(x)$的导数$f′(x)$称为一阶导数，$f′(x)$的导数称为二阶导数，记为$f′′(x)$或$\frac{d^2f(x)}{dx^2}$。</p><p><strong>偏导数</strong> 对于一个多元变量函数$f : \mathbb{R}^d \to \mathbb{R}$，它的偏导数（Partial Derivative ）是关于其中一个变量$x_i$的导数，而保持其他变量固定，可以记为$f’_{x_i} (x)，\bigtriangledown_{x_i}f(x)，\frac{∂f(x)}{∂x_i}或\frac{∂}{∂x_i}f(x)$。</p><h1 id="2-矩阵微积分"><a href="#2-矩阵微积分" class="headerlink" title="2. 矩阵微积分"></a>2. 矩阵微积分</h1><p>为了书写简便，我们通常把<strong>单个函数对多个变量</strong> 或者 <strong>多元函数对单个变量</strong>的偏导数写成向量和矩阵的形式，使其可以被当成一个整体被处理。<strong>矩阵微积分（Matrix Calculus）</strong>是多元微积分的一种表达方式，即使用矩阵和向量来表示因变量每个成分关于自变量每个成分的偏导数。</p><p>矩阵微积分的表示通常有两种符号约定：<strong>分子布局（Numerator Layout）</strong>和<strong>分母布局（Denominator Layout）</strong>。两者的区别是一个标量关于一个向量的导数是写成列向量还是行向量。</p><h2 id="2-1-标量关于向量的偏导数"><a href="#2-1-标量关于向量的偏导数" class="headerlink" title="2.1 标量关于向量的偏导数"></a>2.1 标量关于向量的偏导数</h2><p>对于一个$d$维向量$x \in \mathbb{R}^p$，函数$y = f(x) = f(x_1, … , x_p) \in \mathbb{R}$，则$y$关于$x$的偏导数为</p><p>分母布局 :<br>$$<br>\frac{\partial y}{\partial x} = [\frac{\partial y}{\partial x_1}, …, \frac{\partial y}{\partial x_p}]^T \qquad \in \mathbb{R}^{p \times 1} \tag{1.3}<br>$$<br>分子布局：<br>$$<br>\frac{\partial y}{\partial x} = [\frac{\partial y}{\partial x_1}, …, \frac{\partial y}{\partial x_p}] \qquad \in \mathbb{R}^{1 \times p} \tag{1.4}<br>$$<br>在分母布局中，$\frac{∂y}{∂x}$为列向量，而在分子布局中， $\frac{∂y}{∂x}$为行向量。下文如无特殊说明，均采用分母布局。</p><h2 id="2-2-向量关于标量的偏导数"><a href="#2-2-向量关于标量的偏导数" class="headerlink" title="2.2 向量关于标量的偏导数"></a>2.2 向量关于标量的偏导数</h2><p>对于一个标量$x \in \mathbb{R}$，函数$y = f(x) \in \mathbb{R}^q，则$y$关于$x$的偏导数为</p><p>分母布局：<br>$$<br>\frac{\partial y}{\partial x} = [\frac{\partial y_1}{\partial x}, …, \frac{\partial y_q}{\partial x}] \qquad \in \mathbb{R}^{1 \times q} \tag{1.5}<br>$$<br>分子布局：<br>$$<br>\frac{\partial y}{\partial x} = [\frac{\partial y_1}{\partial x}, …, \frac{\partial y_q}{\partial x}]^T \qquad \in \mathbb{R}^{q \times 1} \tag{1.6}<br>$$</p><p>在分母布局中，$\frac{∂y}{∂x}$为行向量，而在分子布局中， $\frac{∂y}{∂x}$为列向量。</p><h2 id="2-3-向量关于向量的偏导数"><a href="#2-3-向量关于向量的偏导数" class="headerlink" title="2.3 向量关于向量的偏导数"></a>2.3 向量关于向量的偏导数</h2><p>对于一个$d$维向量$x \in \mathbb{R}^p$，函数$y = f(x) \in \mathbb{R}^q$ 的值也为一个向量，则$f(x)$关于$x$的偏导数（分母布局）为<br>$$<br>\frac{\partial f(x)}{\partial x} =<br>\begin {bmatrix}<br>&amp;\frac{\partial y_1}{\partial x_1}&amp; &amp;…&amp; &amp;\frac{\partial y_q}{\partial x_1}&amp; \\<br>&amp;\vdots&amp; &amp;\vdots&amp; &amp;\vdots&amp; \\<br>&amp;\frac{\partial y_1}{\partial x_p}&amp; &amp;…&amp; &amp;\frac{\partial y_q}{\partial x_p}&amp; \\<br>\end {bmatrix} \in \mathbb{R}^{p \times q} \tag{1.7}<br>$$<br>称之为<strong>雅克比矩阵（Jacobian Matrix）</strong>。</p><h1 id="3-导数法则"><a href="#3-导数法则" class="headerlink" title="3. 导数法则"></a>3. 导数法则</h1><p>复合函数的导数的计算可以通过以下法则来简化。</p><h2 id="3-1-加-减-法则"><a href="#3-1-加-减-法则" class="headerlink" title="3.1 加(减)法则"></a>3.1 加(减)法则</h2><p>若$x \in \mathbb{R}^p，y = f(x) \in \mathbb{R}^q，z = g(x) \in \mathbb{R}^q$，则<br>$$<br>\frac{\partial(y+z)}{\partial x} = \frac{\partial y}{\partial x} + \frac{\partial z}{\partial x} \in \mathbb{R}^{p×q} \tag{1.8}<br>$$</p><h2 id="3-2-乘法法则"><a href="#3-2-乘法法则" class="headerlink" title="3.2 乘法法则"></a>3.2 乘法法则</h2><p>(1) 若$x \in \mathbb{R}^p，y = f(x) \in \mathbb{R}^q，z = g(x) \in \mathbb{R}^q$，则<br>$$<br>\frac{∂y^Tz}{∂x} = \frac{∂y}{∂x}z + \frac{∂z}{∂x}y \in \mathbb{R}^p \tag{1.9}<br>$$</p><p>(2) 若$x \in \mathbb{R}^p，y = f(x) \in \mathbb{R}^s，z = g(x) \in \mathbb{R}^t，A \in \mathbb{R}^{s×t}$ 和 $x$ 无关，则<br>$$<br>\frac{∂y^TAz}{∂x} = \frac{∂y}{∂x}Az + \frac{∂z}{∂x}A^Ty \in \mathbb{R}^p \tag{1.10}<br>$$<br>(3) 若$x \in \mathbb{R}^p，y = f(x) \in \mathbb{R}，z = g(x) \in \mathbb{R}^q$，则<br>$$<br>\frac{∂yz}{∂x} = y\frac{∂z}{∂x} + \frac{∂y}{∂x}z^T \in \mathbb{R}^{p×q} \tag{1.11}<br>$$</p><h2 id="3-3-链式法则"><a href="#3-3-链式法则" class="headerlink" title="3.3 链式法则"></a>3.3 链式法则</h2><p><strong>链式法则（Chain Rule）</strong>是在微积分中求复合函数导数的一种常用方法。</p><p>(1) 若$x \in \mathbb{R}，u = u(x) \in \mathbb{R}^s，g = g(u) \in \mathbb{R}^t$，则<br>$$<br>\frac{∂g}{∂x} = \frac{∂u}{∂x}\frac{∂g}{∂u} \in \mathbb{R}^{1×t} \tag{1.12}<br>$$</p><p>(2) 若$x \in \mathbb{R}^p，y = g(x) \in \mathbb{R}^s，z = f(y) \in \mathbb{R}^t$，则<br>$$<br>\frac{∂z}{∂x} = \frac{∂y}{∂x}\frac{∂z}{∂y} \in \mathbb{R}^{p×t} \tag{1.13}<br>$$</p><p>(3) 若$X \in \mathbb{R}^{p×q}$为矩阵，$y = g(X) \in \mathbb{R}^s，z = f(y) \in \mathbb{R}$，则<br>$$<br>\frac{∂z}{∂X_{ij}} = \frac{∂y}{∂X_{ij}}\frac{∂z}{∂y} \in \mathbb{R} \tag{1.14}<br>$$<br>主要参考<a href="https://github.com/nndl/nndl.github.io" target="_blank" rel="noopener">https://github.com/nndl/nndl.github.io</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;微积分1，主要回顾关于微积分中关于导数的相关知识。错误之处，还望诸君不吝指教。&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术备忘" scheme="https://buracagyang.github.io/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/"/>
    
      <category term="基础知识" scheme="https://buracagyang.github.io/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>线性代数2-矩阵</title>
    <link href="https://buracagyang.github.io/2019/06/12/linear-algebra-2/"/>
    <id>https://buracagyang.github.io/2019/06/12/linear-algebra-2/</id>
    <published>2019-06-12T06:47:09.000Z</published>
    <updated>2019-06-19T12:54:22.167Z</updated>
    
    <content type="html"><![CDATA[<p>同步于<a href="https://blog.csdn.net/buracag_mc" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/" target="_blank" rel="noopener">音尘杂记</a></p><p>线性代数2，主要回顾关于矩阵的相关知识。错误之处，还望诸君不吝指教。</p><a id="more"></a><h1 id="1-线性映射"><a href="#1-线性映射" class="headerlink" title="1. 线性映射"></a>1. 线性映射</h1><p><strong>线性映射（Linear Mapping）</strong>是指从线性空间V 到线性空间W的一个映射函数$f : V \to W$，并满足：对于$V$中任何两个向量$u$和$v$以及任何标量$c$，有<br>$$<br>\begin{eqnarray}<br>f(u+v) &amp;=&amp; f(u) + f(v), \tag{1.1} \\<br>f(cv) &amp;=&amp; cf(v). \tag{1.2}<br>\end{eqnarray}<br>$$</p><p>两个有限维欧式空间的映射函数$f: \mathbb{R}^n \to \mathbb{R}^m$可以表示为<br>$$<br>y = Ax \triangleq<br>\begin {bmatrix}<br>a_{11}x_1 + a_{12}x_2 + … + a_{1n}x_n \\<br>a_{21}x_1 + a_{22}x_2 + … + a_{2n}x_n \\<br>\vdots \\<br>a_{m1}x_1 + a_{m2}x_2 + … + a_{mn}x_n \\<br>\end {bmatrix}, \tag{1.3}<br>$$<br>其中$A$定义为$m × n$的<strong>矩阵（Matrix）</strong>，是一个由$m$行$n$列元素排列成的矩形阵列。一个矩阵A从左上角数起的第$i$行第$j$列上的元素称为第$i, j$项，通常记为$[A]_{ij}或 a _{ij}$。矩阵$A$定义了一个从$\mathbb{R}^n$ 到 $\mathbb{R}^m$ 的线性映射；向量 $x \in \mathbb{R}^n$ 和 $y \in \mathbb{R}^m$ 分别为两个空间中的<strong>列向量</strong>，即大小分别为$n \times 1$和$m \times 1$的矩阵。<br>$$<br>x =\begin {bmatrix}<br>x_1 \\<br>x_2 \\<br>\vdots \\<br>x_n \\<br>\end {bmatrix}, y = \begin {bmatrix}<br>y_1 \\<br>y_2 \\<br>\vdots \\<br>y_m \\<br>\end {bmatrix}, \tag{1.4}<br>$$<br>一般为方便起见，书籍中约定逗号隔离的向量表示$[x_1, x_2, … , x_n]$为行向量，列向量通常用分号隔开的表示$x =  [x_1; x_2; … ; x_n]$，或行向量的转置$[x_1, x_2, … , x_n]^T$。</p><h1 id="2-矩阵操作"><a href="#2-矩阵操作" class="headerlink" title="2. 矩阵操作"></a>2. 矩阵操作</h1><p><strong>加</strong> 如果$A$和$B$都为$m×n$的矩阵，则$A$和$B$的加也是$m×n$的矩阵，其每个元素是$A$和$B$相应元素相加。<br>$$<br>[A + B]_{ij} = a_{ij} + b_{ij} \tag{1.5}<br>$$</p><p><strong>乘积</strong> 假设有两个$A$和$B$分别表示两个线性映射$g : \mathbb{R}^m \to \mathbb{R}^k$ 和 $f: \mathbb{R}^n \to \mathbb{R}^m，则其复合线性映射为：<br>$$<br>(g \circ f)(x) = g(f(x)) = g(Bx) = A(Bx) = (AB)x, \tag{1.6}<br>$$<br>其中$AB$表示矩阵$A$和$B$的乘积，定义为<br>$$<br>[AB]_{ij} = \sum_{k=1}^na_{ik}b_{kj} \tag{1.7}<br>$$<br>两个矩阵的乘积仅当第一个矩阵的列数和第二个矩阵的行数相等时才能定义。如$A$是$k × m$矩阵和$B$是$m × n$矩阵，则乘积$AB$是一个$k × n$的矩阵。矩阵的乘法也满足结合律和分配律：</p><ul><li><p>结合律： $(AB)C = A(BC)$,</p></li><li><p>分配律： $(A + B)C = AC + BC，C(A + B) = CA + CB$.</p></li></ul><p><strong>Hadamard 积</strong> $A$和$B$的<em>Hadamard</em>积，也称为<em>逐点乘积</em>，为$A$和$B$中对应的元素相乘。<br>$$<br>[A \odot B]_{ij} = a_{ij}b_{ij} \tag{1.8}<br>$$<br>一个标量$c$与矩阵$A$乘积为$A$的每个元素是$A$的相应元素与$c$的乘积<br>$$<br>[cA]_{ij} = ca_{ij} \tag{1.9}<br>$$</p><p><strong>转置</strong> $m×n$矩阵$A$的<strong>转置（Transposition）</strong>是一个$n×m$的矩阵，记为$A^T$，$A^T$的第$i$行第$j$列的元素是原矩阵A的第$j$行第$i$列的元素<br>$$<br>[A^T]_{ij} = [A]_{ji} \tag{1.10}<br>$$<br><strong>向量化</strong> 矩阵的向量化是将矩阵表示为一个列向量。这里，<strong>vec</strong>是向量化算子。设$A = [a_{ij}]_{m×n}$，则<br>$$<br>vec(A) = [a_{11}, a_{21}, … , a_{m1}, a_{12}, a_{22}, … , a_{m2}, … , a_{1n}, a_{2n}, … , a_{mn}]^T \tag{1.11}<br>$$</p><p><strong>迹</strong> $n$ x $n$矩阵$A$的对角线元素之和称为它的<strong>迹（Trace）</strong>，记为tr(A)。尽管矩阵的乘法不满足交换律，但它们的迹相同，即tr(AB) = tr(BA)。</p><p><strong>行列式</strong> $n$ x $n$矩阵$A$的行列式是一个将其映射到标量的函数，通常记作$det(A)$或$|A|$。行列式可以看做是有向面积或体积的概念在欧氏空间中的推广。在$n$维欧氏空间中，行列式描述的是一个线性变换对“体积”所造成的影响。一个$n × n$的矩阵$A$的行列式定义为：<br>$$<br>det(A) = \sum_{\sigma \in S_n}(-1)^k\prod a_i,\sigma(i) \tag{1.12}<br>$$<br>解释一下，$S_n$是$\{1,2,…,n\}$的所有排列的集合，$\sigma$是其中一个排列，$\sigma(i)$是元素i在排列$\sigma$中的位置，k表示$\sigma$中的逆序对的数量。</p><p>其中逆序对的定义为：在排列$\sigma$中，如果有序数对$(i, j)$满足$1 \leq i &lt; j \leq n$但$\sigma(i) &gt; \sigma(j)$，则其为$\sigma$的一个逆序对。举个例子(左侧为排列， 右侧为逆序对数量)：<br>$$<br>eg：[4, 3, 1, 2, 5] \to 5<br>$$</p><p><strong>秩</strong> 一个矩阵$A$的列秩是$A$的线性无关的列向量数量，行秩是$A$的线性无关的行向量数量。一个矩阵的列秩和行秩总是相等的，简称为<strong>秩（Rank）</strong>。</p><p>一个$m × n$的矩阵$A$的秩最大为$min(m, n)$。若$rank(A) = min(m, n)$，则称矩阵为满秩的。如果一个矩阵不满秩，说明其包含线性相关的列向量或行向量，其行列式为0。两个矩阵的乘积$AB$的秩$rank(AB) \leq min(rank(A), rank(B))$。</p><p><strong>范数</strong> 在“线性代数1-向量和向量空间”中已经提及<br>$$<br>\ell_p(v) = \parallel v \parallel_p = {(\sum_{i=1}^{n}|v_i|^p)}^{1/p}, \tag{1.13}<br>$$</p><h1 id="3-矩阵类型"><a href="#3-矩阵类型" class="headerlink" title="3. 矩阵类型"></a>3. 矩阵类型</h1><p><strong>对称矩阵</strong> 对称矩阵（Symmetric Matrix）指其转置等于自己的矩阵，即满足$A = A^T$。</p><p><strong>对角矩阵</strong> 对角矩阵（Diagonal Matrix）是一个主对角线之外的元素皆为0 的矩阵。对角线上的元素可以为0 或其他值。一个$n × n$的对角矩阵$A$满足<br>$$<br>[A]_{ij} = 0 \qquad if \quad i\neq j \quad \forall i,j \in \{1, …, n\} \tag{1.14}<br>$$<br>对角矩阵A也可以记为diag(a)，a 为一个n维向量，并满足<br>$$<br>[A]_{ii} = a_i \tag{1.15}<br>$$</p><p>其中$n × n$的对角矩阵$A = diag(a)$和$n$维向量b的乘积为一个$n$维向量<br>$$<br>Ab = diag(a)b = a \odot b \tag{1.16}<br>$$<br>其中$\odot$表示点乘，即$(a \odot b)_i = a_ib_i$。</p><p><strong>单位矩阵</strong> 单位矩阵（Identity Matrix）是一种特殊的的对角矩阵，其主对角线元素为1，其余元素为0。$n$阶单位矩阵$I_n$，是一个$n × n$的方块矩阵。可以记为$I_n = diag(1, 1, …, 1)$。一个m × n的矩阵A和单位矩阵的乘积等于其本身。<br>$$<br>AI_n = I_mA = A \tag{1.17}<br>$$<br><strong>逆矩阵</strong> 对于一个$n × n$的方块矩阵$A$，如果存在另一个方块矩阵$B$使得<br>$$<br>AB = BA = I_n \tag{1.18}<br>$$<br>其中$I_n$为单位阵，则称$A$是可逆的。矩阵$B$称为矩阵A的逆矩阵（Inverse Matrix），记为$A^{−1}$。</p><blockquote><p>一个方阵的行列式等于0当且仅当该方阵不可逆时。</p></blockquote><p><strong>正定矩阵</strong> 对于一个$n×n$的对称矩阵$A$，如果对于所有的非零向量$x \in \mathbb{R}^n$都满足<br>$$<br>x^TAx &gt; 0 \tag{1.19}<br>$$<br>则$A$为<strong>正定矩阵（Positive-Definite Matrix）</strong>。如果$x^TAx \geq 0$，则$A$是<strong>半正定矩阵（Positive-Semidefinite Matrix）</strong>。</p><p><strong>正交矩阵</strong> 正交矩阵（Orthogonal Matrix）$A$为一个方块矩阵，其逆矩阵等于其转置矩阵。<br>$$<br>A^T = A^{-1} \tag{1.20}<br>$$<br>等价于$A^TA = AA^T = I_n$。</p><p><strong>Gram矩阵</strong> 向量空间中一组向量$v_1, v_2 , … , v_n$的Gram 矩阵（Gram Matrix）;G是内积的对称矩阵，其元素$G_{ij}$为${v_i}^T v_j$。</p><h1 id="4-特征值与特征矢量"><a href="#4-特征值与特征矢量" class="headerlink" title="4. 特征值与特征矢量"></a>4. 特征值与特征矢量</h1><p>如果一个标量$\lambda$和一个非零向量v满足<br>$$<br>Av = \lambda v \tag{1.21}<br>$$</p><p>则$\lambda$和$v$分别称为矩阵$A$的<strong>特征值（Eigenvalue）</strong>和<strong>特征向量（Eigenvector）</strong>。</p><h1 id="5-矩阵分解"><a href="#5-矩阵分解" class="headerlink" title="5. 矩阵分解"></a>5. 矩阵分解</h1><p>一个矩阵通常可以用一些比较“简单”的矩阵来表示，称为<strong>矩阵分解（Matrix Decomposition, Matrix Factorization）</strong>。</p><p><strong>奇异值分解</strong> 一个$m×n$的矩阵$A$的奇异值分解（Singular Value Decomposition，SVD）定义为<br>$$<br>A = U\sum V^T \tag{1.22}<br>$$<br>其中$U$和$V$分别为$m × m$和$n × n$的正交矩阵，$\sum$为$m × n$的对角矩阵，其对角线上的元素称为奇异值（Singular Value）。</p><p><strong>特征分解</strong> 一个$n × n$的方块矩阵$A$的特征分解（Eigendecomposition）定义为<br>$$<br>A = Q\Lambda Q^{-1} \tag{1.23}<br>$$<br>其中$Q$为$n×n$的方块矩阵，其每一列都为$A$的特征向量，Λ为对角阵，其每一个对角元素为$A$的特征值。<br>如果$A$为对称矩阵，则A可以被分解为<br>$$<br>A = Q\Lambda Q^T \tag{1.24}<br>$$<br>其中Q为正交阵。</p><p>主要参考<a href="https://github.com/nndl/nndl.github.io" target="_blank" rel="noopener">https://github.com/nndl/nndl.github.io</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;线性代数2，主要回顾关于矩阵的相关知识。错误之处，还望诸君不吝指教。&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术备忘" scheme="https://buracagyang.github.io/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/"/>
    
      <category term="基础知识" scheme="https://buracagyang.github.io/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>线性代数1-向量和向量空间</title>
    <link href="https://buracagyang.github.io/2019/06/11/linear-algebra-1/"/>
    <id>https://buracagyang.github.io/2019/06/11/linear-algebra-1/</id>
    <published>2019-06-11T09:36:42.000Z</published>
    <updated>2019-06-21T06:11:44.865Z</updated>
    
    <content type="html"><![CDATA[<p>同步于<a href="https://blog.csdn.net/buracag_mc" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/" target="_blank" rel="noopener">音尘杂记</a></p><p>后续几篇笔记主要想回顾整理一下需要用到的数学基础知识，主要包括了线性代数、微积分、概念论、数学优化和信息论等内容。相对比较基础，权当复习回顾完善整个知识体系结构。错误之处，还望诸君不吝指教。</p><a id="more"></a><h1 id="1-向量"><a href="#1-向量" class="headerlink" title="1. 向量"></a>1. 向量</h1><p><strong>标量（Scalar）</strong>是一个实数，只有大小，没有方向。而<strong>向量（Vector）</strong>是由一组实数组成的有序数组，同时具有大小和方向。例，一个n维<strong>向量a</strong> 是由n个有序实数组成，表示为：<br>$$<br>a = [a_1, a_2, …, a_n], \tag{1.1}<br>$$<br>其中$a_i$称为向量a的第$i$个分量，或第$i$维。向量符号通常用黑体小写字母$a, b, c$或小写希腊字母$\alpha,\beta, \gamma$ 等来表示。</p><h1 id="2-向量空间"><a href="#2-向量空间" class="headerlink" title="2. 向量空间"></a>2. 向量空间</h1><p><strong>向量空间（Vector Space）</strong>，也称<strong>线性空间（Linear Space）</strong>，是指由向量组成的集合，并满足以下两个条件：</p><ul><li><p>向量加法：向量空间$V$中的两个向量<strong>a</strong>和<strong>b</strong>，它们的和<strong>a + b</strong>也属于空间$V$；</p></li><li><p>标量乘法：向量空间$V$中的任一向量<strong>a</strong>和任一标量$c$，它们的乘积$c · a$也属于空间$V$。</p></li></ul><p><strong>欧氏空间</strong> 一个常用的线性空间是<strong>欧氏空间（Euclidean Space）</strong>。一个欧氏空间表示通常为$\mathbb{R}^n$，其中n为空间<strong>维度（Dimension）</strong>。欧氏空间中向量的加法和标量乘法定义为：<br>$$<br>\begin{eqnarray}<br>[a_1, a_2, … , a_n] + [b_1, b_2, … , b_n] &amp;=&amp; [a_1 + b_1, a_2 + b_2, … , a_n + b_n], \tag{1.2} \\<br>c[a_1, a_2, … , a_n] &amp;=&amp; [ca_1, ca_2, … , ca_n] \tag{1.3}<br>\end{eqnarray}<br>$$<br>其中$a, b, c \in{\mathbb{R}}$为一个标量。</p><p><strong>线性子空间</strong> 向量空间$V$的线性子空间$U$是$V$的一个子集，并且满足向量空间的条件（向量加法和标量乘法）。</p><p><strong>线性无关</strong> 线性空间$V$中的一组向量${v_1, v_2, … , v_n}$，如果对任意的一组标量$\lambda_1, \lambda_2, … , \lambda_n$，满足$\lambda_1v_1 + \lambda_2v_2 + ·… + \lambda_nv_n = 0$，则必然$\lambda_1 = \lambda_2 = … =\lambda_n = 0$，那么${v_1, v_2, … , v_n}$是线性无关的，也称为线性独立的。</p><p><strong>基向量</strong> 向量空间$V$的<strong>基（Base）</strong>$B = {e_1, e_2, … , e_n}$ 是$V$的有限子集，其元素之间线性无关。向量空间$V$所有的向量都可以按唯一的方式表达为$B$中向量的线性组合。对任意$v \in V$，存在一组标量$(\lambda_1, \lambda_2, … , \lambda_n)$ 使得:<br>$$<br>v = \lambda_1e_1 + \lambda_2e_2 + … + \lambda_ne_n \tag{1.4}<br>$$<br>其中基$B$中的向量称为基向量（Base Vector）。如果基向量是有序的，则标量$(\lambda_1, \lambda_2, … , \lambda_n)$ 称为向量$v$关于基$B$的<strong>坐标（Coordinates）</strong>。</p><p>n维空间$V$的一组<strong>标准基（Standard Basis）</strong>为:<br>$$<br>\begin{eqnarray}<br>e_1 &amp;=&amp; [1, 0, …, 0], \tag{1.5} \\<br>e_2 &amp;=&amp; [0, 1, …, 0], \tag{1.6} \\<br>&amp;…&amp;, \tag{1.7} \\<br>e_n &amp;=&amp; [0, 0, …, 1], \tag{1.8}<br>\end{eqnarray}<br>$$</p><p>向量空间$V$中的任一向量$v = [v_1, v_2, … , v_n]$可以唯一的表示为:<br>$$<br>[v_1, v_2, … , v_n] = v_1e_1 + v_2e_2 + … + v_ne_n, \tag{1.9}<br>$$<br>其中$v_1, v_2, … , v_n$也称为向量$v$的<strong>笛卡尔坐标（Cartesian Coordinate）</strong>。向量空间中的每个向量可以看作是一个线性空间中的笛卡儿坐标。</p><p>内积<strong> 一个n维线性空间中的两个向量$a$和$b$，其内积为:<br>$$<br>⟨a, b⟩ = \sum_{i=1}^{n}a_ib_i, \tag{1.10}<br>$$</strong>正交<strong> 如果向量空间中两个向量的内积为0，则它们</strong>正交（Orthogonal）**。如果向量空间中一个向量$v$与子空间$U$中的每个向量都正交，那么向量$v$和子空间$U$正交。</p><h1 id="3-常见的向量"><a href="#3-常见的向量" class="headerlink" title="3. 常见的向量"></a>3. 常见的向量</h1><p><strong>全0向量</strong>指所有元素都为0的向量，用<strong>0</strong>表示。全0向量为笛卡尔坐标系中的原点。</p><p><strong>全1向量</strong>指所有值为1的向量，用<strong>1</strong>表示。</p><p><strong>one-hot向量</strong>为有且只有一个元素为1，其余元素都为0 的向量。one-hot向量是在数字电路中的一种状态编码，指对任意给定的状态，状态寄存器中只有1位为1，其余位都为0。</p><h1 id="4-范数"><a href="#4-范数" class="headerlink" title="4. 范数"></a>4. 范数</h1><p><strong>范数（Norm）</strong>是一个表示向量“长度”的函数，为向量空间内的所有向量赋予非零的正长度或大小。对于一个n维向量<strong>v</strong>，一个常见的范数函数为$\ell_p$范数<br>$$<br>\ell_p(v) = \parallel v \parallel_p = {(\sum_{i=1}^{n}|v_i|^p)}^{1/p}, \tag{1.11}<br>$$<br>其中$p \geq 0$为一个标量的参数。常见的$p$的取值有1，2，$\infty$等。</p><p>$\ell_1$<strong>范数 </strong>， $p = 1$<br>$$<br>\ell_1(v) = \sum_{i=1}^{n}|v_i|, \tag{1.12}<br>$$<br>$\ell_2$<strong>范数 </strong>， $p = 2$<br>$$<br>\ell_2(v) = \sqrt{\sum_{i=1}^{n}|v_i|^2} = \sqrt{v^Tv}, \tag{1.13}<br>$$<br>$\ell_2$范数又称为<strong>Euclidean范数</strong>或者<strong>Frobenius范数</strong>。从几何角度，向量也可以表示为从原点出发的一个有向线段，其$\ell_2$范数为线段的长度，也常称为向量的模。</p><p>$\ell_{\infty}$<strong>范数 </strong>， $p = \infty$,表示为各个元素的最大绝对值<br>$$<br>\ell_{\infty}(v) = ||v||_{\infty} = max\{v_1,v_2, …, v_n\}, \tag{1.14}<br>$$</p><p>主要参考<a href="https://github.com/nndl/nndl.github.io" target="_blank" rel="noopener">https://github.com/nndl/nndl.github.io</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;后续几篇笔记主要想回顾整理一下需要用到的数学基础知识，主要包括了线性代数、微积分、概念论、数学优化和信息论等内容。相对比较基础，权当复习回顾完善整个知识体系结构。错误之处，还望诸君不吝指教。&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术备忘" scheme="https://buracagyang.github.io/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/"/>
    
      <category term="基础知识" scheme="https://buracagyang.github.io/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>分位数回归简介</title>
    <link href="https://buracagyang.github.io/2019/06/03/quantile-regression/"/>
    <id>https://buracagyang.github.io/2019/06/03/quantile-regression/</id>
    <published>2019-06-03T06:24:43.000Z</published>
    <updated>2019-06-03T10:51:29.009Z</updated>
    
    <content type="html"><![CDATA[<p>同步于<a href="https://blog.csdn.net/buracag_mc" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/" target="_blank" rel="noopener">音尘杂记</a></p><p>最近在做一个比较有意思(难搞…)的项目。大致介绍一下相关背景：根据历史的一个工作情况(历史表现，也就是有多少人做了多少工作量)，以及未来的一个预估工作量(预测值)，我们需要预估一个<strong>合理的</strong>人员投入;一言概之，根据历史表现和预测件量预估人员投入。</p><a id="more"></a><p><strong>时序问题？</strong><br>咋一看，这不就是一个时序问题嘛！人力投入如下：<br>$$<br>Y_t = f(T_t, S_t, C_t, I_t)<br>$$<br>其中$T_t$代表长期趋势特征，$S_t$代表季节性或者季节变动，$C_t$代表周期性或循环波动，$I_t$代表随机性或不规则波动。接下来获取特征和历史人员投入，这不就可以预估得到了未来人力投入嘛。</p><p>但是，我们再仔细考虑一下。事情还不仅仅是如此简单。原因有两点：</p><ul><li>与常见的销量、件量等的预测不同，人力的投入不仅仅是一个时序数据，内生的跟工作量强相关；</li><li>预估人员投入的一个很重要的目标是，求得一个合理的人员投入(范围)。</li></ul><p><strong>常规机器学习问题？</strong><br>或者，再稍微拓展一下，由于人员投入是跟工作量是强相关的，我们可不可以用机器学习的思路来解决这个问题。也即：<br>$$<br>Y_t = f(workload, other_features)<br>$$<br>其实也是存在问题的，在上述的有监督学习中，对于每一个instance我们是需要有一个监督值的。对于该场景下，貌似每个instance都存在一个人力投入值；但是我们的目标是需要预估一个<strong>合理的</strong>人力投入，如果单纯地去拟合当前的人力投入，岂不是认为目前的投入即是最优的了，既然如此就没有做这个任务的必要了。</p><p><strong>经济学模型和其他尝试</strong><br>我们也曾尝试从经典的柯布-道格拉斯生产函数形式、<a href="https://doi.org/10.1080/03610926.2014.1001495" target="_blank" rel="noopener">部分随机人力规划系统</a>以及<a href="https://doi.org/10.1016/j.simpat.2015.07.004" target="_blank" rel="noopener">基于强化学习</a>等的的一些思路进行过分析过，均因效果不甚理想或者业务场景不相符而被pass掉。</p><p>最后，考虑到我们的主要目标是预估一个<strong>合理的</strong>人力投入，我们引入了衡量工作质量的一个变量。通过综合考虑质量和效能的关系，以保证预估出的人员数量，在保证工作量的情况或者说在降低人力投入量后工作质量不至于太差，反之亦然。最后，我们用了一个比较简单的方法来解决这个事情 – 分位数回归（Quantile Regression, QR）。</p><p>在介绍分为数回归的知识点之前，需要简要说一下推导过程不然显得太过突兀：<br>定义工作量为$W$,业务指标准时完成量为$W1$,员工数量为$P$，显然，<br>$$<br> \frac{W1}{W} = \frac{W1}{P}\frac{P}{W}<br>$$<br>这里的$\frac{W1}{W}$用来衡量质量情况，$\frac{P}{W}$的倒数$\frac{W}{P}$用来衡量效能情况。我们可以认为，在同一个类型下(工作场景、工作时间)，实际工作效能$\frac{W1}{P}$是一个相对客观的不变的值，令其为$k$。接下来我们便可以用分位数回归的方法求得系数也即$k$值，然后根据需要的质量情况，得到最终的效能范围，再结合预测件量情况，即可得到一个较为合理的人员投入范围。</p><p>首先，我们知道随机变量X的分布函数为：<br>$$<br>F(x) = P(X\leq x)<br>$$<br>则随机变量X的$\tau$分位数的定义为：<br>$$<br>Q_\tau(X) = arginf\{x\in R ; F(x)\geq\tau\}(0&lt;\tau&lt;1)<br>$$<br>若将分布函数F(x)的逆定义为：<br>$$<br>F_X^{-1}(\tau) = inf\{y\in R ; F(y)\geq\tau\}<br>$$</p><p>故：<br>$$<br>Q_\tau(X) = F_X^{-1}(\tau)<br>$$<br>和传统的线性回归估计方法不同的是，分位数回归估计的是一组自变量X与因变量Y的分位数之间线性关系的建模方法，偏向于条件分位数的变化。故OLS在数据出现尖峰(异常值)、长尾分布或者显著异方差等情况时，OLS结果不稳定,但是分位数的估计量确相对稳健。</p><p>设随机向量(X, Y),其中Y在X=x的情况下的条件累积分布函数为$F_{Y|X=x}$(y|x)，则其$\tau$条件分位数定义为：<br>$$<br>Q_\tau(Y|X=x) = arginf\{y\in R ; F(y|x)\geq\tau\}(0&lt;\tau&lt;1)<br>$$</p><p>这里直接附上对于OLS和分位数回归的相关对比：</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">OLS</th><th style="text-align:center">分位数回归估计</th></tr></thead><tbody><tr><td style="text-align:center">原理</td><td style="text-align:center">以平均数为基准，求解最短距离</td><td style="text-align:center">以不同的分位数为基准，求解最短距离</td></tr><tr><td style="text-align:center">前提条件</td><td style="text-align:center">独立、正态、同方差</td><td style="text-align:center">独立</td></tr><tr><td style="text-align:center">假设要求</td><td style="text-align:center">强假设</td><td style="text-align:center">弱假设</td></tr><tr><td style="text-align:center">求解方法</td><td style="text-align:center">OLS</td><td style="text-align:center">加权最小一乘估计</td></tr><tr><td style="text-align:center">检验类型</td><td style="text-align:center">参数检验</td><td style="text-align:center">非参数检验</td></tr><tr><td style="text-align:center">异方差</td><td style="text-align:center">影响大</td><td style="text-align:center">影响小</td></tr><tr><td style="text-align:center">拟合曲线</td><td style="text-align:center">一条拟合曲线</td><td style="text-align:center">一簇拟合曲线</td></tr></tbody></table><p><strong>分位数回归参数估计的思想</strong></p><hr><p>与线性回归不同的是，QR估计量的特点在于，是通过样本到回归曲线的垂直距离的加权和求得；其中权重设置为，在拟合曲线之下的样本权重为$1 - \tau$，拟合曲线之上的样本权重为$\tau$， 即：<br>$$<br>L(\theta) = \min_{\xi\subset{R}}\{\sum_{i:Y_i\ge\xi}\tau|Y_i - \xi| + \sum_{i:Y_i\le\xi}(1 - \tau)|Y_i - \xi|\}<br>$$</p><p>上式可等价为：<br>$$<br>L(\theta) = \min_{\xi\subset{R}}\sum_{i=1}^n\rho_\tau(Y_i - \xi)<br>$$<br>其中，$\rho_\tau(u)=u(\tau-I(u&lt;0))$, $I(Z)$为示性函数。</p><p>QR的损失函数$L(\theta)$不是对称的，是由两条从原点出发的分别位于第一和第二象限的射线组成，显然其斜率比为$\tau:1-\tau$。</p><p>以上，仅是关于分位数回归知识的大概简介，最主要的部分是关于损失函数的设计。</p><p>最后，应用到该项目中时，我们对原始数据进行了离散化的处理，以及经过斯皮尔曼检验后的数据进行训练。由于其是一个计算密集型的任务，应用到全国众多网点时(数万),可以开多个线程池进行并行处理。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;最近在做一个比较有意思(难搞…)的项目。大致介绍一下相关背景：根据历史的一个工作情况(历史表现，也就是有多少人做了多少工作量)，以及未来的一个预估工作量(预测值)，我们需要预估一个&lt;strong&gt;合理的&lt;/strong&gt;人员投入;一言概之，根据历史表现和预测件量预估人员投入。&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术备忘" scheme="https://buracagyang.github.io/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/"/>
    
      <category term="统计学运用" scheme="https://buracagyang.github.io/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E8%BF%90%E7%94%A8/"/>
    
  </entry>
  
  <entry>
    <title>Logistic loss函数</title>
    <link href="https://buracagyang.github.io/2019/05/29/logistic-loss-function/"/>
    <id>https://buracagyang.github.io/2019/05/29/logistic-loss-function/</id>
    <published>2019-05-29T09:16:09.000Z</published>
    <updated>2019-06-03T10:51:02.378Z</updated>
    
    <content type="html"><![CDATA[<p>同步于<a href="https://blog.csdn.net/buracag_mc" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/" target="_blank" rel="noopener">音尘杂记</a></p><p>前面在浏览sklearn中关于<a href="https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression" title="Logistic Regression" target="_blank" rel="noopener">Logistic Regression</a>部分，看到关于带正则项的LR目标损失函数的定义形式的时候，对具体表达式有点困惑，后查阅资料，将思路整理如下。</p><a id="more"></a><h1 id="1-sklearn文档中的LR损失函数"><a href="#1-sklearn文档中的LR损失函数" class="headerlink" title="1. sklearn文档中的LR损失函数"></a>1. sklearn文档中的LR损失函数</h1><p>先看sklearn对于LR目标损失函数(带L2)的定义：<br>$$<br>\min_{w, c} \frac{1}{2}w^T w + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1)<br>$$</p><p>看到这个表达形式，其实是有两个疑问：</p><ul><li><p>logistic loss的表达形式</p></li><li><p>正则项的惩罚系数</p></li></ul><p>对于第二个问题，其实比较容易解释。通常我们在最小化结构风险时，会给我们的惩罚项乘上一个惩罚系数λ(通常1 &lt; λ &lt; 0)，<br>$$<br>\min_{w, λ} \sum_{i=1}^nloss(y, y_i) + λw^T w<br>$$<br>一般，为方便处理，做一个技巧性地处理，对多项式乘上一个正数 1/2λ, 得到：<br>$$<br>\min_{w, λ} \frac{1}{2λ}\sum_{i=1}^nloss(y, y_i) + \frac{1}{2}w^T w<br>$$<br>令C = 1/2λ即可。</p><p>但是对于第一个形式，当时比较困惑；特意翻看了一下我以前记录的关于<a href="https://blog.csdn.net/buracag_mc/article/details/77620686" title="LR损失函数" target="_blank" rel="noopener">LR以及LR损失函数</a>的一些笔记。</p><h1 id="2-LR损失函数"><a href="#2-LR损失函数" class="headerlink" title="2. LR损失函数"></a>2. LR损失函数</h1><p>为了方便说明笔者当时的疑惑所在，便将当时脑海里存在的logistic loss函数形式 和 sklearn中LR损失函数的推导方法分别记为旧思路和新思路吧。</p><h2 id="2-1-logistic基础知识"><a href="#2-1-logistic基础知识" class="headerlink" title="2.1 logistic基础知识"></a>2.1 logistic基础知识</h2><p>如指数分布、高斯分布等分布一样，logistic是一种变量的分布，它也有自己的概率分布函数和概率密度函数，其中概率分布函数如下：<br>$$<br>F(x) = P(X \leq x) = \frac{1}{1+e^{-(x-\mu)/\gamma}}<br>$$</p><p>对概率分布函数求导，记得到对应的概率密度函数：<br>$$<br>f(x) = \frac{e^{-(x- \mu)/ \gamma}}{\gamma(1+e^{-(x-\mu)/\gamma})^2}<br>$$</p><p>其中，$\mu$就是分布对应的均值，$\gamma$是对应的形状参数。</p><p>下文，为简介方便起见，将$-(x-\mu)/\gamma$ 替换为 $-x$,故记为：<br>$$<br>F(x) = \frac{1}{1+exp(-x)}<br>$$</p><p>对应示例图如下：<br><img src="/2019/05/29/logistic-loss-function/20170827141819860.png" alt="这里写图片描述"></p><p>logistic有一个很重要的性质是：<br>$$<br>F(-x) = \frac{1}{1+exp(x)} = \frac{1}{1+\frac{1}{exp(-x)}} =<br>\frac{exp(-x)}{1+exp(-x)}=1-\frac{1}{1+exp(-x)}=1-F(x)<br>$$</p><p>通常，应用到LR中，有如下形式：</p><blockquote><p><strong>(1)</strong><br>$$<br>P(Y=1|\beta,x) = \frac{1}{1+exp(-\beta x)} = \frac{e^{\beta x}}{1+e^{\beta x}}<br>$$<br>$$<br>P(Y=0|\beta,x) = 1 - \frac{1}{1+exp(-\beta x)} = \frac{1}{1+e^{\beta x}}<br>$$</p></blockquote><blockquote><p>一个事件的几率(odds)，定义为该事件发生与不发生的概率比值，若事件发生概率为p：</p></blockquote><p>$$<br>odds = \frac{p}{1-p}<br>$$</p><p>那么该事件的对数几率（log odds或者logit）如下：<br>$$<br>logit(p)=log\frac{p}{1−p}<br>$$</p><p>那么，对于上述二项，Y=1的对数几率就是：<br>$$<br>log \frac{P(Y=1|\beta,x)}{1−P(Y=1|\beta,x)}=log \frac{P(Y=1|\beta,x)}{P(Y=0|\beta,x)}=\beta x<br>$$</p><p>也就是说，输出Y=1的对数几率是由输入x的线性函数表示的模型，这就是逻辑回归模型。易知，当 $\beta x$的值越大，$P(Y=1|\beta,x)$越接近1；$\beta x$越小,$P(Y=1|\beta,x)$ 越接近0。</p><p>其实，LR就是一个线性分类的模型。与线性回归不同的是：LR将线性方程输出的很大范围的数压缩到了[0,1]区间上；更优雅地说：<strong>LR就是一个被logistic方程归一化后的线性回归</strong>。</p><h2 id="2-2-旧思路"><a href="#2-2-旧思路" class="headerlink" title="2.2 旧思路"></a>2.2 旧思路</h2><p>旧思路要从LR的参数求解过程说起。</p><p>我们知道统计学中一种很常用的方法是根据最大化似然函数的值来估计总体参数。在机器学习领域，我们听到的更多是损失函数的概念，常通过构建损失函数，然后最小化损失函数估计目标参数。在这里，<strong>最大化对数似然函数与最小化对数似然损失函数其实是等价的</strong>，下面我们可以看到。</p><ul><li><p>假设我们有n个独立的训练样本$\{(x_1,y_1),(x_2,y_2),(x_3,y_3),…,(x_n,y_n)\},y={0,1}$,那么每一个观察到的样本$(x_i,y_i)$出现的概率是：<br>$$<br>P(y_i,x_i) = P(y_i=1 | x_i)^{y_i}(1-P(y_i=1 | x_i))^{1-y_i}<br>$$<br>显然，$y_i$为1时，保留前半部分；$y_i$为0时，保留后半部分。</p></li><li><p>构建似然函数：<br>$$<br>L(\beta) = \prod P(y_i=1|x_i)^{y_i}(1-P(y_i=1|x_i))^{1-y_i}<br>$$</p></li><li><p>OK,对似然函数取对数，得到对数似然函数：</p></li></ul><p>$$LL(\beta) = log(L(\beta))= log(\prod P(y_i=1|x_i)^{y_i}(1-P(y_i=1|x_i))^{1-y_i}) $$</p><p> $= \sum_{i=1}^{n}(y_i log P(y_i=1|x_i) + (1-y_i)log(1-P(y_i=1|x_i)))$</p><p> $= \sum_{i=1}^{n}y_i log \frac{P(y_i=1|x_i)}{1-P(y_i=1|x_i)} + \sum_{i=1}^{n}log(1-P(y_i=1|x_i))$</p><p> $= \sum_{i=1}^{n}y_i(\beta x) + \sum_{i=1}^{n}logP(y_i=0|x_i)$</p><p> $= \sum_{i=1}^{n}y_i(\beta x) - \sum_{i=1}^{n}log(1+e^{\beta x})$</p><ul><li><p>用 $LL(\beta)$ 对 $\beta$ 求偏导，得：<br>$\frac{\partial LL(\beta)}{\partial \beta}<br>= \sum_{i=1}^{n}y_ix_i - \sum_{i=1}^{n} \frac{e^{\beta x_i}}{1+e^{\beta x_i}}.x_i$</p><p>$= \sum_{i=1}^{n}(y_i - P(y_i=1|x_i))x_i$<br>该式是无法解析求解，故会用到一些优化算法进行求解(梯度下降、牛顿法等)，这不是本文重点，便不再赘述。</p></li></ul><p>咋一看的确与sklearn中的形式差别有点大，所以请看新思路。</p><h2 id="2-3-新思路"><a href="#2-3-新思路" class="headerlink" title="2.3 新思路"></a>2.3 新思路</h2><p>在式(1)中， $x$表示特征向量，$\beta$表示相应的超参数，此时$y\in({0, 1})$表示样本对应的标签(label)。</p><p>这里，特别要讲的是另一种表达形式，将标签与预测函数在形式上统一了：</p><blockquote><p><strong>(2)</strong><br>$$<br>P(g=\pm1 |\beta, x) = \frac{1}{1+exp(-g\beta x)}<br>$$</p></blockquote><p>此时的样本标签$g\in({1, -1})$。</p><p>虽然式(1)与式(2)看起来似乎不同，但是我们可以有如下证明：<br>$$<br>P(Y=1|\beta,x) = \frac{e^{\beta x}}{1+e^{\beta x}} =  \frac{1}{1+exp(-\beta x)} = P(g=1 |\beta, x)<br>$$<br>同理，我们可以证明$P(Y=0|\beta,x)$ 和 $P(g=-1|\beta,x)$是等价的。</p><p>既然两种形式是等价的，为了适应更加广泛的分类loss最小化的框架，故采用第二种形式来表示LR.毕竟<strong>Simple is better than complex.</strong></p><p>首先定义$x_i$为特征向量，$y_i$为样本标签,则目标损失函数可以表示为：<br>$$<br>arg\min_{\beta}\sum_{i=1}L(y_i, f(x_i))<br>$$<br>其中，f是我们的回归方程，L是目标损失函数。</p><p>对应到LR中，我们有<br>$$<br>f(x) = \beta x<br>$$<br>$$<br>L(y, f(x)) = log(1 + exp(-yf(x)))<br>$$<br>如果将LR的第二种表达形式带入到损失函数L中，可得：<br>$$<br>L(y, f(x)) = log(1 + exp(-yf(x))) = log(\frac{1}{P(y|\beta,x)})<br>$$</p><p>再进一步：<br>$$<br>arg\min_{\beta}\sum_{i=1}L(y_i, f(x_i)) = arg\min_{\beta}\sum_{i=1}log(\frac{1}{P(y_i|\beta,x_i)})<br>$$<br>$$<br>= arg\max_{\beta}\sum_{i=1}log(P(y_i|\beta,x_i))= arg\max_{\beta}\prod_{i=1}P(y_i|\beta,x_i)<br>$$<br><strong>等式最后即为极大似然估计的表达形式。</strong></p><h1 id="3-思考"><a href="#3-思考" class="headerlink" title="3. 思考"></a>3. 思考</h1><p>其实到这儿，我们不难发现在旧思路中，推导极大化对数似然函数中的第二步：<br>$= \sum_{i=1}^{n}(y_i log P(y_i=1|x_i) + (1-y_i)log(1-P(y_i=1|x_i)))$</p><p>与新思路中的：<br>$$<br>=arg\max_{\beta}\sum_{i=1}log(P(y_i|\beta,x_i))<br>$$<br><strong>本质是统一的。</strong></p><p>最后</p><blockquote><p><strong>“Simple is better than complex.”   – The Zen of Python, by Tim Peters</strong></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;前面在浏览sklearn中关于&lt;a href=&quot;https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression&quot; title=&quot;Logistic Regression&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Logistic Regression&lt;/a&gt;部分，看到关于带正则项的LR目标损失函数的定义形式的时候，对具体表达式有点困惑，后查阅资料，将思路整理如下。&lt;/p&gt;
    
    </summary>
    
    
      <category term="统计学运用" scheme="https://buracagyang.github.io/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E8%BF%90%E7%94%A8/"/>
    
      <category term="算法备忘" scheme="https://buracagyang.github.io/tags/%E7%AE%97%E6%B3%95%E5%A4%87%E5%BF%98/"/>
    
      <category term="机器学习" scheme="https://buracagyang.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Python中定义类的相关知识</title>
    <link href="https://buracagyang.github.io/2019/05/29/python-basic-about-class/"/>
    <id>https://buracagyang.github.io/2019/05/29/python-basic-about-class/</id>
    <published>2019-05-29T09:12:54.000Z</published>
    <updated>2019-06-03T10:51:24.181Z</updated>
    
    <content type="html"><![CDATA[<p>同步于<a href="https://blog.csdn.net/buracag_mc" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/" target="_blank" rel="noopener">音尘杂记</a></p><p>主要介绍了在python中，抽象类的定义、多态的概念、类中属性的封装以及类中常见的修饰器。</p><a id="more"></a><h1 id="1-抽象类"><a href="#1-抽象类" class="headerlink" title="1. 抽象类"></a>1. 抽象类</h1><p>与Java一样，Python也有抽象类的概念，抽象类是一个特殊的类。其特殊之处在于</p><ul><li>只能被继承，不能被实例化；</li><li>子类必须完全覆写(实现)其“抽象方法”和“抽象属性”后才能被实例化。</li></ul><p>可以有两种实现方式: 利用NotImplementedError实现和利用abctractmethod实现</p><h2 id="1-1-NotImplementedError"><a href="#1-1-NotImplementedError" class="headerlink" title="1.1 NotImplementedError"></a>1.1 NotImplementedError</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2018/11/20 10:11</span></span><br><span class="line"><span class="comment"># @File     : test_interface.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"><span class="comment"># @Desc     :</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#########################################</span></span><br><span class="line"><span class="comment"># 利用NotImplementedError</span></span><br><span class="line"><span class="comment">#########################################</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Payment</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pay</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChildPay</span><span class="params">(Payment)</span>:</span></span><br><span class="line">    <span class="comment"># 必须实现pay方法,否则报错NotImplementedError</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pay</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"TestPay pay"</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">payed</span><span class="params">(self, money)</span>:</span></span><br><span class="line">print(<span class="string">"Payed: &#123;&#125;"</span>.format(money))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">child_pay = ChildPay()</span><br><span class="line">child_pay.payed(<span class="number">20</span>)</span><br></pre></td></tr></table></figure><h2 id="1-2-abctractmethod"><a href="#1-2-abctractmethod" class="headerlink" title="1.2 abctractmethod"></a>1.2 abctractmethod</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2018/11/20 10:11</span></span><br><span class="line"><span class="comment"># @File     : test_interface.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"><span class="comment"># @Desc     :</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> abc <span class="keyword">import</span> ABCMeta, abstractmethod</span><br><span class="line"></span><br><span class="line"><span class="comment"># #########################################</span></span><br><span class="line"><span class="comment"># abstractmethod</span></span><br><span class="line"><span class="comment"># 子类必须全部重写父类的abstractmethod方法</span></span><br><span class="line"><span class="comment"># 非abstractmethod方法可以不实现重写</span></span><br><span class="line"><span class="comment"># 带abstractmethod方法的类不能实例化</span></span><br><span class="line"><span class="comment"># #########################################</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Payment</span><span class="params">(metaclass=ABCMeta)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span></span></span><br><span class="line">self.name = name</span><br><span class="line"></span><br><span class="line"><span class="meta">@abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pay</span><span class="params">(self, money)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span><span class="params">(self, money)</span>:</span></span><br><span class="line">        print(<span class="string">"Payment get &#123;&#125;"</span>.format(money))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">total</span><span class="params">(self, money)</span>:</span></span><br><span class="line">        print(<span class="string">"Payment total &#123;&#125;"</span>.format(money))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChildPay</span><span class="params">(Payment)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pay</span><span class="params">(self, money)</span>:</span></span><br><span class="line">        print(<span class="string">"ChildPay pay &#123;&#125;"</span>.format(money))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span><span class="params">(self, money)</span>:</span></span><br><span class="line">        print(<span class="string">"ChildPay get &#123;&#125;"</span>.format(money))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">child_pay = ChildPay(<span class="string">"safly"</span>)</span><br><span class="line">child_pay.pay(<span class="number">100</span>)</span><br><span class="line">child_pay.get(<span class="number">200</span>)</span><br><span class="line">child_pay.total(<span class="number">400</span>)</span><br><span class="line"><span class="comment"># 不能实例化</span></span><br><span class="line"><span class="comment"># TypeError: Can't instantiate abstract class Payment</span></span><br><span class="line"><span class="comment"># with abstract methods get, pay</span></span><br><span class="line"><span class="comment"># a = Payment("safly")</span></span><br></pre></td></tr></table></figure><h1 id="2-多态概念"><a href="#2-多态概念" class="headerlink" title="2. 多态概念"></a>2. 多态概念</h1><p>向不同的对象发送同一条消息(obj.func(): 是调用了obj的方法func, 又称向obj发送了一条消息func)，不同的对象在接受时会产生不同的行为（即不同的处理方法）。</p><p>也就是说，每个对象可以用自己的方式去响应共同的消息。所谓消息，就是调用函数，不同的对象可以执行不同的函数。</p><p>例： 男生.放松了()， 女生.放松了()，男生是打篮球，女生是看综艺，虽然二者消息一样，但是处理方法不同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2018/11/20 10:11</span></span><br><span class="line"><span class="comment"># @File     : test_interface.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"><span class="comment"># @Desc     :</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> abc <span class="keyword">import</span> ABCMeta, abstractmethod</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base</span><span class="params">(metaclass=ABCMeta)</span>:</span></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">relax</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Boy</span><span class="params">(Base)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">relax</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"playing basketball"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Girl</span><span class="params">(Base)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">relax</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"watching TV"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">boy = Boy()</span><br><span class="line">girl = Girl()</span><br><span class="line">boy.talk()  <span class="comment"># playing basketball</span></span><br><span class="line">girl.talk()  <span class="comment"># watching TV</span></span><br></pre></td></tr></table></figure><h1 id="3-属性封装"><a href="#3-属性封装" class="headerlink" title="3. __属性封装"></a>3. __属性封装</h1><h2 id="3-1-私有静态属性、私有方法"><a href="#3-1-私有静态属性、私有方法" class="headerlink" title="3.1 私有静态属性、私有方法"></a>3.1 私有静态属性、私有方法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2018/11/20 10:11</span></span><br><span class="line"><span class="comment"># @File     : test_interface.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"><span class="comment"># @Desc     :</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># #########################################</span></span><br><span class="line"><span class="comment"># __属性封装</span></span><br><span class="line"><span class="comment"># 私有静态属性、私有方法</span></span><br><span class="line"><span class="comment"># #########################################</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment"># 私有静态属性</span></span><br><span class="line">    __kind = <span class="string">"private kind"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 调用私有静态属性</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_kind</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> Dog.__kind</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 私有方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__func</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"__func"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 调用私有方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.__func()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"><span class="comment"># 如下调用错误,因为需要在类内调用</span></span><br><span class="line"><span class="comment"># print(Dog.__kind)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提倡如下调用方式</span></span><br><span class="line">d = Dog()</span><br><span class="line">print(d.get_kind())</span><br><span class="line">print(d.func())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不提倡如下调用方式</span></span><br><span class="line"><span class="comment"># d._Dog__func()</span></span><br><span class="line"><span class="comment"># print(Dog.__dict__)</span></span><br><span class="line"><span class="comment"># print(Dog._Dog__kind)</span></span><br><span class="line"><span class="comment"># print(Dog._Dog__func)</span></span><br></pre></td></tr></table></figure><h2 id="3-2-私有对象属性"><a href="#3-2-私有对象属性" class="headerlink" title="3.2 私有对象属性"></a>3.2 私有对象属性</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2018/11/20 10:11</span></span><br><span class="line"><span class="comment"># @File     : test_interface.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"><span class="comment"># @Desc     :</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># #########################################</span></span><br><span class="line"><span class="comment"># 私有对象属性</span></span><br><span class="line"><span class="comment"># #########################################</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, weight)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.__weight = weight</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_weight</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.__weight</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">room = Dog(<span class="string">"doggy"</span>, <span class="number">5</span>)</span><br><span class="line">print(room.name)  <span class="comment"># doggy</span></span><br><span class="line">print(room.get_weight())  <span class="comment"># 5</span></span><br><span class="line"><span class="comment"># 不能如下方法调用私有对象属性</span></span><br><span class="line"><span class="comment"># print(room.__weight)</span></span><br></pre></td></tr></table></figure><h2 id="3-3-私有属性不被继承"><a href="#3-3-私有属性不被继承" class="headerlink" title="3.3 私有属性不被继承"></a>3.3 私有属性不被继承</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2018/11/20 10:11</span></span><br><span class="line"><span class="comment"># @File     : test_interface.py</span></span><br><span class="line"><span class="comment"># @Software : PyCharm</span></span><br><span class="line"><span class="comment"># @Desc     :</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># #########################################</span></span><br><span class="line"><span class="comment"># 私有属性不能被继承</span></span><br><span class="line"><span class="comment"># #########################################</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DogParent</span><span class="params">(object)</span>:</span></span><br><span class="line">__private = <span class="string">'PRIVATE'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        self.__name = name</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__func</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"__DogParent func"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DogChild</span><span class="params">(DogParent)</span>:</span></span><br><span class="line">    <span class="comment"># 如下的方法是错误的</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_private</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> DogParent.__private</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">dog_parent = DogParent(<span class="string">"Tom"</span>)</span><br><span class="line">print(dir(dog_parent))</span><br><span class="line">print(<span class="string">"-------------"</span>)</span><br><span class="line">dog_child = DogChild(<span class="string">"Tommy"</span>)</span><br><span class="line">print(dir(dog_child))</span><br><span class="line"><span class="comment"># 调用报错AttributeError: type object 'DogChild' has no attribute '_DogChild__private'</span></span><br><span class="line"><span class="comment"># print(dog_child.get_private())</span></span><br></pre></td></tr></table></figure><h1 id="4-类中的常见修饰器"><a href="#4-类中的常见修饰器" class="headerlink" title="4. 类中的常见修饰器"></a>4. 类中的常见修饰器</h1><p>主要介绍最常见的装饰器，classmethod, staticmethod和property</p><h2 id="4-1-classmethod"><a href="#4-1-classmethod" class="headerlink" title="4.1 classmethod"></a>4.1 classmethod</h2><p>@classmethod<br>不需要self参数，但是classmethod方法的第一个参数是需要表示自身类的cls 参数；不管是从类本身调用还是从实例化后的对象调用，都用第一个参数把类传进来。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DogParent</span><span class="params">(object)</span>:</span></span><br><span class="line">__private = <span class="string">'PRIVATE'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        self.__name = name</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__func</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"__DogParent func"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 类方法</span></span><br><span class="line"><span class="meta">@classmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">change_name</span><span class="params">(cls, new_name)</span>:</span></span><br><span class="line">cls.__name = new_name</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_name</span><span class="params">(cls)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls.__name</span><br><span class="line"></span><br><span class="line"><span class="comment"># 普通方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">change_name2</span><span class="params">(self, new_name)</span>:</span></span><br><span class="line">        self.__name = new_name</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_name2</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.__name</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">DogParent.change_name(DogParent, <span class="string">"Tom2"</span>)</span><br><span class="line">print(DogParent.get_name(DogParent))</span><br><span class="line"></span><br><span class="line">DogParent.change_name2(<span class="string">"Tom3"</span>)</span><br><span class="line">print(DogParent.get_name2())</span><br></pre></td></tr></table></figure></p><h2 id="4-2-staticmethod"><a href="#4-2-staticmethod" class="headerlink" title="4.2 staticmethod"></a>4.2 staticmethod</h2><p>staticmethod不需要表示自身对象的self和自身类的cls参数，就跟使用普通的函数一样;这样有一个好处：</p><ul><li>有利于我们代码的优雅，把某些应该属于某个类的函数给放到那个类里去，同时有利于命名空间的整洁</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DogParent</span><span class="params">(object)</span>:</span></span><br><span class="line">__private = <span class="string">'PRIVATE'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        self.__name = name</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__func</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"__DogParent func"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 类方法</span></span><br><span class="line"><span class="meta">@classmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">change_name</span><span class="params">(cls, new_name)</span>:</span></span><br><span class="line">cls.__name = new_name</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_name</span><span class="params">(cls)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls.__name</span><br><span class="line"></span><br><span class="line"><span class="comment"># 普通方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">change_name2</span><span class="params">(self, new_name)</span>:</span></span><br><span class="line">        self.__name = new_name</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_name2</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.__name</span><br><span class="line"></span><br><span class="line"><span class="comment"># 静态方法</span></span><br><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_nickname</span><span class="params">(nickname)</span>:</span></span><br><span class="line">print(<span class="string">"nickname: &#123;&#125;"</span>.format(nickname))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">DogParent.set_nickname(<span class="string">"tom's nickname~"</span>)</span><br></pre></td></tr></table></figure><h2 id="4-3-property"><a href="#4-3-property" class="headerlink" title="4.3 property"></a>4.3 property</h2><p>@property 把一个方法伪装成一个属性,这个属性的值，是这个方法的返回值；这个方法不能有参数，类不能调用，只能对象调用。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, height, weight)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.height = height</span><br><span class="line">        self.weight = weight</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bmi</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.weight / (self.height ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">method</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"method"</span>)</span><br></pre></td></tr></table></figure></p><p>其实，property的作用不仅于此。简单点讲，@property的本质其实就是实现了get，set，delete三种方法。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, nickname)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.nickname = nickname</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">nickname</span><span class="params">(self)</span>:</span></span><br><span class="line"><span class="comment"># 相当于实现了get方法</span></span><br><span class="line">        print(<span class="string">"nickname: &#123;&#125;"</span>.self.nickname)</span><br><span class="line"></span><br><span class="line"><span class="meta">@property.setter</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nickname</span><span class="params">(self, new_nickname)</span>:</span></span><br><span class="line"><span class="comment"># 相当于实现了set方法</span></span><br><span class="line">self.nickname = new_nickname</span><br><span class="line">print(<span class="string">"new nickname: &#123;&#125;"</span>.format(new_nickname))</span><br><span class="line"></span><br><span class="line"><span class="meta">@property.deleter</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nickname</span><span class="params">(self)</span>:</span></span><br><span class="line"><span class="comment"># 相当于实现了delete方法</span></span><br><span class="line"><span class="keyword">del</span> Person.nickname</span><br><span class="line">print(<span class="string">"deleted nickname"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">person = Person(<span class="string">"Tom"</span>, <span class="string">'tommmy'</span>)</span><br><span class="line"><span class="comment"># get</span></span><br><span class="line">person.nickname()</span><br><span class="line"></span><br><span class="line"><span class="comment"># setter </span></span><br><span class="line">person.nickname = <span class="string">'new_tommmy'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># deleter</span></span><br><span class="line"><span class="keyword">del</span> person.nickname</span><br><span class="line"></span><br><span class="line"><span class="comment">#删除完毕后,再次调用报如下错误</span></span><br><span class="line"><span class="comment"># AttributeError: type object 'person' has no attribute 'nickname'</span></span><br><span class="line"><span class="comment"># person.nickname</span></span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;主要介绍了在python中，抽象类的定义、多态的概念、类中属性的封装以及类中常见的修饰器。&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术备忘" scheme="https://buracagyang.github.io/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/"/>
    
      <category term="Python" scheme="https://buracagyang.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>单层感知器为什么不能解决异或(XOR)问题</title>
    <link href="https://buracagyang.github.io/2019/05/29/single-layer-perceptron/"/>
    <id>https://buracagyang.github.io/2019/05/29/single-layer-perceptron/</id>
    <published>2019-05-29T09:05:20.000Z</published>
    <updated>2019-06-03T10:51:33.221Z</updated>
    
    <content type="html"><![CDATA[<p>同步于<a href="https://blog.csdn.net/buracag_mc" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/" target="_blank" rel="noopener">音尘杂记</a></p><p>单层感知器为什么不能解决异或问题(XOR)问题？给出两个思路去考虑这个小问题~</p><a id="more"></a><p>最近翻到了自己在印象笔记中学习记录的一些知识点，后续准备系统地整理放在自己的博客上，还请各位不吝指教。</p><h1 id="1-感知器模型"><a href="#1-感知器模型" class="headerlink" title="1. 感知器模型"></a>1. 感知器模型</h1><ul><li><p>感知器模型是美国学者罗森勃拉特（Frank Rosenblatt）为研究大脑的存储、学习和认知过程而提出的一类具有自学习能力的神经网络模型，它把神经网络的研究从纯理论探讨引向了从工程上的实现。</p></li><li><p>Rosenblatt提出的感知器模型是一个只有单层计算单元的前向神经网络，称为单层感知器。</p></li></ul><h1 id="2-单层感知器模型算法概述"><a href="#2-单层感知器模型算法概述" class="headerlink" title="2. 单层感知器模型算法概述"></a>2. 单层感知器模型算法概述</h1><p>在学习基础的NN知识的时候，单个神经元的结构必定是最先提出来的，单层感知器模型算法与神经元结构类似；</p><p>大概思想是：首先把<strong>连接权</strong>和<strong>阈值</strong>初始化为较小的非零随机数，然后把有n个连接权值的输入送入网络，经加权运算处理，得到的输出如果与所期望的输出有较大的差别(对比神经元模型中的激活函数)，就对连接权值参数进行自动调整，经过多次反复，直到所得到的输出与所期望的输出间的差别满足要求为止。</p><p>如下为简单起见，仅考虑只有一个输出的简单情况。设$x_i(t)$是时刻$t$感知器的输入（i=1,2,……,n），$ω_i(t)$是相应的连接权值，$y(t)$是实际的输出，$d(t)$是所期望的输出，且感知器的输出或者为1，或者为0。</p><h1 id="3-线性不可分问题"><a href="#3-线性不可分问题" class="headerlink" title="3. 线性不可分问题 "></a>3. 线性不可分问题 </h1><p>单层感知器不能表达的问题被称为线性不可分问题。 1969年，明斯基证明了“异或”问题是线性不可分问题。</p><h1 id="4-“与”、”或”、”非”问题的证明"><a href="#4-“与”、”或”、”非”问题的证明" class="headerlink" title="4. “与”、”或”、”非”问题的证明"></a>4. “与”、”或”、”非”问题的证明</h1><ul><li>由于单层感知器的输出为：</li></ul><p>$$ y(x1,x2) = f(ω1 <em> x1 + ω2 </em> x2 - θ) $$</p><p>所以，用感知器实现简单逻辑运算的情况如下：</p><ul><li><p>“与”运算（And, x1∧x2）<br>令 ω1 = ω2 = 1，θ = 1.5，则: y = f(1 <em> x1 + 1 </em> x2 - 1.5)<br>显然，当x1和x2均为1时，y的值1；而当x1和x2有一个为0时，y的值就为0.</p></li><li><p>“或”运算（Or, x1∨x2）<br>令ω1 = ω2=1, θ = 0.5，则: y=f(1 <em> x1 + 1 </em> x2 - 0.5)<br>显然，只要x1和x2中有一个为1，则y的值就为1；只有当x1和x2都为0时，y的值才为0。</p></li><li><p>“非”运算（Not, ～X1）<br>令ω1 = -1， ω2 = O， θ = -0.5，则:   y = f((-1) <em> x1 + 1 </em> x2 + 0.5)<br>显然，无论x2为何值，x1为1时，y的值都为0；x1为0时，y的值为1。即y总等于～x1。</p></li><li><p>“异或”运算（x1 XOR x2）</p></li></ul><h1 id="5-“异或”问题的证明"><a href="#5-“异或”问题的证明" class="headerlink" title="5. “异或”问题的证明"></a>5. “异或”问题的证明</h1><h2 id="5-1-单层感知机不能解决”异或”问题证明方法一"><a href="#5-1-单层感知机不能解决”异或”问题证明方法一" class="headerlink" title="5.1 单层感知机不能解决”异或”问题证明方法一"></a>5.1 单层感知机不能解决”异或”问题证明方法一</h2><p>如果“异或”（XOR）问题能用单层感知器解决，则由XOR的真值映射关系如下：</p><table><thead><tr><th style="text-align:center">(x1, x2)</th><th style="text-align:center">y</th></tr></thead><tbody><tr><td style="text-align:center">(0, 0)</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">(0, 1)</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">(1, 0)</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">(1, 1)</td><td style="text-align:center">0</td></tr></tbody></table><p>则ω1、 ω2 和θ 必须满足如下方程组：<br>1). ω1 + ω2 - θ ＜ 0    –&gt;   θ &gt; ω1 + ω2<br>2). ω1 + 0 - θ ≥ 0      –&gt;   0 ≥ θ - ω1<br>3). 0 + 0 - θ ＜ 0      –&gt;   θ &gt; 0<br>4). 0 + ω2 - θ ≥ 0      –&gt;   0 ≥ θ - ω2<br>显然，该方程组是矛盾的，无解！这就说明单层感知器是无法解决异或问题的。</p><h2 id="5-2-单层感知机不能解决”异或”问题证明方法二"><a href="#5-2-单层感知机不能解决”异或”问题证明方法二" class="headerlink" title="5.2 单层感知机不能解决”异或”问题证明方法二"></a>5.2 单层感知机不能解决”异或”问题证明方法二</h2><p>首先需要证明以下定理：</p><blockquote><p>样本集线性可分的充分必要条件是正实例点集所构成的凸壳与负实例点集所构成的凸壳互不相交    </p></blockquote><ul><li><p>必要性：假设样本集T线性可分，则存在一个超平面W将数据集正实例点和负实例点完全正确地划分到超平面两侧。显然两侧的点分别构成的凸壳互不相交；</p></li><li><p>充分性：假设存在两个凸壳A、B相交，且存在超平面W将A和B线性分割，令A在B的凸壳内部的点为a，因为线性可交，则A中不存在两点之间的连线与超平面W相交，而凸壳B中任意一点与A中的点的连线均与超平面W相交，则B内部的点a也与A中任一点之间的连线不与W相交，与B壳中任一点与A中的点的连线均与超平面W相交矛盾。</p></li></ul><p><strong>故：只有正负实例点所构成的两个凸壳不相交时样本集才线性可分。</strong></p><p>显然，对于此例，负实例样本集[(0, 0), (1, 1)] 和 正实例样本集[(0, 1), (1, 0)]是二维中是不能被线性分割的。<br><img src="/2019/05/29/single-layer-perceptron/1.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;单层感知器为什么不能解决异或问题(XOR)问题？给出两个思路去考虑这个小问题~&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术备忘" scheme="https://buracagyang.github.io/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/"/>
    
      <category term="算法备忘" scheme="https://buracagyang.github.io/tags/%E7%AE%97%E6%B3%95%E5%A4%87%E5%BF%98/"/>
    
  </entry>
  
  <entry>
    <title>AIC和BIC相关知识</title>
    <link href="https://buracagyang.github.io/2019/05/29/aic-and-bic/"/>
    <id>https://buracagyang.github.io/2019/05/29/aic-and-bic/</id>
    <published>2019-05-29T07:14:58.000Z</published>
    <updated>2019-07-28T08:55:35.276Z</updated>
    
    <content type="html"><![CDATA[<p>同步于<a href="https://blog.csdn.net/buracag_mc" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/" target="_blank" rel="noopener">音尘杂记</a></p><p>前面在回顾<a href="https://github.com/scikit-learn/scikit-learn" target="_blank" rel="noopener">sklearn</a>时，在广义线性模型中看到选择模型时可以采用AIC和BIC准则，特地复习了下统计学基础，简记如下，以抛砖引玉。</p><a id="more"></a><h2 id="1-模型拟合优度检验"><a href="#1-模型拟合优度检验" class="headerlink" title="1. 模型拟合优度检验"></a>1. 模型拟合优度检验</h2><p>最基础的一个模型拟合优度的检验量就是R square(方程的确定系数)。<br>已知一组样本观测值 $(X_i, Y_i)$,其中i=1,2,3,…,n得到如下样本回归方程：<br>$$<br>\hat{Y_i} = \hat{\beta_0} + \hat{\beta_1}X_i<br>$$<br>而Y的第i个观测值与样本均值的离差 $y_i = Y_i - \bar{Y}$，其可以分解为两部分之和：<br>$$<br>y_i = Y_i - \bar{Y} = (Y_i - \hat{Y_i}) + (\hat{Y_i} - \bar{Y}) = e_i + \hat{y_i}<br>$$<br>其中 $\hat{y_i} = (\hat{Y_i} - \bar{Y})$是样本拟合值与观测值的平均值之差，可认为是由回归直线解释的部分，通常称之为”离差”；</p><p>$e_i = (Y_i - \hat{Y_i})$是实际观测值与回归拟合值之差，是回归直线不能解释的部分，通常称之为”残差”。</p><p>如果 $Y_i = \hat{Y_i}$,即实际观测值落在样本回归”线”上，则拟合最好。</p><p>对于所有样本点，<strong>可以证明</strong>：<br>$$<br>\sum{y_i}^2 = \sum{\hat{y_i}^2} + \sum{e_i^2} + 2\sum{\hat{y_i}^2e_i} = \sum{\hat{y_i}^2} + \sum{e_i^2}<br>$$<br>记:<br>$TSS = \sum{y_i^2} = \sum{(Y_i - \bar{Y})^2}$为总体平方和(Total Sum of Squares)<br>$ESS = \sum{\hat{y_i}^2} = \sum{(\hat{Y_i} - \bar{Y})^2}$为回归平方和(Explained Sum of Squares, <strong>注意有的教材又称之为Regression Sum of Squares</strong>)<br>$RSS = \sum{e_i^2} = \sum{(Y_i - \hat{Y_i})^2}$为残差平方和(Residual Sum of Squares, <strong>注意有的教材又称之为Error Sum of Squares</strong>)<br>$$<br>TSS = ESS + RSS<br>$$<br>所以Y的观测值围绕其均值的总离差(total variation)可分解为两部分：一部分来自回归线(ESS)，另一部分则来自与随机误差(RSS)</p><blockquote><p>在给定样本中，TSS不变，如果实际观测点离样本回归线越近，则ESS在TSS中占的比重越大，因此定义<strong>拟合优度：回归平方和ESS与TSS的比值。</strong></p></blockquote><p>记 $R^2 = \frac{ESS}{TSS} = 1 - \frac{RSS}{TSS}$，称 $R^2$为(样本)可决系数/判定系数</p><p>对于回归方程来说，$R^2$有以下几个意义：</p><ol><li>R square可以作为选择不同模型的标准。在拟合数据之前，不能确定数据的确定模型关系，可以对变量的不同数学形式进行拟合，再看R square的大小。</li><li>在数据的关系存在非线性可能情况下：<br>a) R squared越大不一定拟合越好；<br>b) 如何一个模型的R square很小，不一定代表数据之间没有关系，而很有可能是选择的模型不对，或者存在有其他的函数关系。</li><li><strong>当自变量个数增加时，尽管有的自变量与的线性关系不显著，其R square也会增大</strong>，对于这种情况需采用Adjusted R squared进行调整。</li></ol><h2 id="2-调整R-square"><a href="#2-调整R-square" class="headerlink" title="2. 调整R square"></a>2. 调整R square</h2><p>由于在模型中增加变量时，$R^2$没有下降，所以存在一种过度拟合模型的内在趋势，即向模型中增加变量固然可以改善数据拟合程度，但这样也会导致预测的方差正大，这时就需要用到调整 $R^2$。<br>$$<br>\bar{R_2} = 1 - \frac{n-1}{n-k}(1-R^2)<br>$$<br>调整$R^2$用作拟合优度的度量，它能够适当消除在模型中增加变量所导致的自由度损失。</p><p>调整 $R^2$对模型扩张时自由度的损失进行了弥补，但又存在一个问题，随着样本容量的增大，这种弥补是否足以保证该准则肯定能让分析者得到正确的模型，所以提出了另外两个拟合度量指标，一个是赤池信息准则(Akaike Information Criterion, AIC)，另一个是施瓦茨或贝叶斯信息准则(Bayesian Information Criterion,BIC)。</p><h2 id="3-AIC和BIC"><a href="#3-AIC和BIC" class="headerlink" title="3. AIC和BIC"></a>3. AIC和BIC</h2><p>$$<br>AIC(K) = s_y^2(1-R^2)e^{2k/n}<br>$$</p><p>$$<br>BIC(K) = s_y^2(1-R^2)n^{k/n}<br>$$</p><p>$s_y^2$中没有对自由度进行修正，虽然随着$R^2$的提高，这两个指标都有所改善(下降),但在其他条件不变的情况下，模型规模扩大又会使这两个指标恶化。与$\bar{R^2}$一样，实现同样的拟合程度，这些指标在平均每次观测使用参数个数(K/n)较少时更有效。使用对数通常更方便，多数统计软件报告度量指标是：<br>$$<br>AIC(K) = ln(\frac{e^{\prime}e}{n}) + \frac{2K}{n}<br>$$</p><p>$$<br>BIC(K) = ln(\frac{e^{\prime}e}{n}) + \frac{Kln{n}}{n}<br>$$</p><p><u><strong>更一般地：</strong></u><br>$$<br>AIC(K) = 2K - 2ln(L)<br>$$<br>其中k是模型参数个数，L为似然函数。从一组可供选择的模型中选择最佳模型时，通常选择AIC最小的模型。</p><p>当两个模型之间存在较大差异时，差异主要体现在似然函数项，当似然函数差异不显著时，上市第一项，即模型复杂度则起作用，从而参数个数少的模型是较好的选择。</p><p>一般而言，当模型复杂度提高(k增大)时，似然函数L也会增大，从而使AIC变小，但是k过大时，似然函数增速减缓，导致AIC增大，模型过于复杂容易造成过拟合现象。目标是选取AIC最小的模型，AIC不仅要提高模型拟合度(极大似然)，而且引入了惩罚项，使模型参数尽可能少，有助于降低过拟合的可能性。<br>$$<br>BIC(K) = Kln{n} - 2ln(L)<br>$$<br>其中k是模型参数个数，n为样本数量，L为似然函数。与AIC类似地，引入了模型参数个数作为惩罚项，但是<strong>BIC的惩罚项比AIC的大</strong>，考虑了样本数量，样本数量过多时，可有效防止模型精度过高造成的模型复杂度过高；其中 $kln{n}$惩罚项在维度过大且训练样本数据相对较少的情况下，可以有效避免出现维度灾难现象。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;前面在回顾&lt;a href=&quot;https://github.com/scikit-learn/scikit-learn&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;sklearn&lt;/a&gt;时，在广义线性模型中看到选择模型时可以采用AIC和BIC准则，特地复习了下统计学基础，简记如下，以抛砖引玉。&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术备忘" scheme="https://buracagyang.github.io/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/"/>
    
      <category term="统计学运用" scheme="https://buracagyang.github.io/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E8%BF%90%E7%94%A8/"/>
    
  </entry>
  
  <entry>
    <title>A/B-test显著性检验</title>
    <link href="https://buracagyang.github.io/2019/05/28/ab-test/"/>
    <id>https://buracagyang.github.io/2019/05/28/ab-test/</id>
    <published>2019-05-28T08:39:43.000Z</published>
    <updated>2019-07-28T09:24:01.135Z</updated>
    
    <content type="html"><![CDATA[<p>同步于<a href="https://blog.csdn.net/buracag_mc" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/" target="_blank" rel="noopener">音尘杂记</a>；</p><h1 id="1-A-B-test解读"><a href="#1-A-B-test解读" class="headerlink" title="1. A/B-test解读"></a>1. A/B-test解读</h1><p>A/B-test是为同一个目标制定两个方案，在同一时间维度，分别让组成成分相同（相似）的用户群组随机的使用一个方案，收集各群组的用户体验数据和业务数据，最后根据显著性检验分析评估出最好版本正式采用。</p><p>使用A/B-test首先需要建立一个待测试的版本服务，这个版本服务可能在标题、字体、排版、背景颜色、措辞等方面与原有版本服务有所不同，然后将这两个版本服务以随机的方式同时推送给测试用户。接下来分别统计两个版本的用户转化率指标，然后根据样本数据进行显著性检验。</p><a id="more"></a><h1 id="2-测试目的"><a href="#2-测试目的" class="headerlink" title="2. 测试目的"></a>2. 测试目的</h1><p>页面（版本）的某一特定更新对转化率指标（如UV-线索转化率、UV-上架转化率、UV-成交转化率或者线索-上架转化率等）的影响效果。</p><h1 id="3-A-B-test显著性检验"><a href="#3-A-B-test显著性检验" class="headerlink" title="3. A/B-test显著性检验"></a>3. A/B-test显著性检验</h1><p>随机将测试用户群分为2部分，用户群1使用A方案，用户群2使用B方案，经过一定测试时间后，根据收集到的两方案样本观测数据，根据显著性检验结果选取最好方案。</p><p>为了下文方便说明，我们不妨设A方案为参考方案（或旧方案），B方案为实验方案（或新方案）。以下我们以xx二手车的线索-车辆成交转化率为例（注：所有数据均属虚构，仅做示例说明之用），假设进行A/B-test的时间是一周。</p><h2 id="3-1-选择观测指标"><a href="#3-1-选择观测指标" class="headerlink" title="3.1 选择观测指标"></a>3.1 选择观测指标</h2><p>使用A方案的人数$N_A$，使用B方案的人数$N_B$，通常情况下$N_A = N_B = N$；由样本计算出A方案的线索-车辆成交转化率为$\hat{P}_a$，B方案的线索-车辆成交转化率 为$\hat{P}_b$；总体A的分布：$A \sim B(N, P_a)$，总体B的分布：$B \sim B(N, P_b)$；</p><p>根据中心极限定理可知，$\hat{P}_a$和$\hat{P}_b$均可认为近似服从正态分布：<br>$$<br>\begin{eqnarray}<br>\hat{P}_a \sim N(P_a, \hat{P}_a(1-\hat{P}_a) / N) \\<br>\hat{P}_b \sim N(P_b, \hat{P}_b(1-\hat{P}_b) / N)<br>\end{eqnarray} \tag{1.1}<br>$$</p><p>所以根据正态分布的性质：<br>$$<br>X = \hat{P}_b - \hat{P}_a \sim N(P_b-P_a, \hat{P}_b(1-\hat{P}_b) / N + \hat{P}_a(1-\hat{P}_a) / N) \tag{1.2}<br>$$</p><h2 id="3-2-建立原假设和备择假设"><a href="#3-2-建立原假设和备择假设" class="headerlink" title="3.2 建立原假设和备择假设"></a>3.2 建立原假设和备择假设</h2><p>由于我们的期望结果是B方案所带来的线索-车辆成交转化率高于A方案所带来的线索 -车辆成交转化率，所以原假设和备择假设如下：<br>$$<br>\begin{eqnarray}<br>H_0: X = P_b - P_a \leq 0 \\<br>H_1: X = P_b - P_a &gt; 0<br>\end{eqnarray} \tag{1.3}<br>$$</p><h2 id="3-3-构建检验统计量"><a href="#3-3-构建检验统计量" class="headerlink" title="3.3 构建检验统计量"></a>3.3 构建检验统计量</h2><p>检验统计量：<br>$$<br>Z = \frac{\hat{P}_b - \hat{P}_a}{\sqrt{\frac{\hat{P}_b(1-\hat{P}_b)}{N} + \frac{\hat{P}_a(1-\hat{P}_a)}{N}}} \tag{1.4}<br>$$</p><h2 id="3-4-显著性检验结论"><a href="#3-4-显著性检验结论" class="headerlink" title="3.4 显著性检验结论"></a>3.4 显著性检验结论</h2><p>给定显著性水平$\alpha$为。当$Z &gt; Z_{\alpha}$时，拒绝原假设，认为B方案所带来的线索-车辆成交转化率高于A方案所带来的线索-车辆成交转化率，建议可以进行推广；当$Z \leq Z_{\alpha}$时，不能拒绝原假设，即认为B方案所带来的线索-车辆成交转化率不高于A方案所带来的线索-车辆成交转化率，建议暂不建议进行推广。</p><h1 id="4-A-B-test示例"><a href="#4-A-B-test示例" class="headerlink" title="4. A/B-test示例"></a>4. A/B-test示例</h1><p>假设我们进行A/B-test一周，参考版本（通常默认是原始版本，简记为A）和实验版本（添加了特定改进的版本,简记为B），分别得到了1000个线索，A的线索-车辆成交转化率为7%，B的线索-车辆成交转化率为8%。如下表所示：</p><table><thead><tr><th>版本</th><th>总线索数</th><th>成交数(单位：辆)</th><th>转化率</th></tr></thead><tbody><tr><td>参考版本(A)</td><td>1,000</td><td>70</td><td>7.00%</td></tr><tr><td>实验版本(B)</td><td>1,000</td><td>80</td><td>8.00%</td></tr></tbody></table><p>在这儿，我们是肯定B比A版本所带来的转化率高呢，还是说这仅仅是由于一些随机的因素导致的这样的区别呢？我们严格按照A/B-test显著性检验过程进行如下计算。 </p><ul><li><p>选取测量指标：</p><p>$N_A = N_B = N = 1000$；其中$\hat{P}_a = 7%$，$\hat{P}_b = 8%$</p></li></ul><ul><li>构建原假设和备择假设：<br>$$<br>\begin{eqnarray}<br>H_0&amp;:&amp; B版本所带来的线索-车辆成交转化率不高于A版本，即X=P_b - P_a \leq 0 \\<br>H_1&amp;:&amp; B版本所带来的线索-车辆成交转化率高于A版本，即X=P_b - P_a &gt; 0<br>\end{eqnarray}<br>$$</li></ul><ul><li>构建检验统计量：<br>$$<br>Z = \frac{\hat{P}_b - \hat{P}_a}{\sqrt{\frac{\hat{P}_b(1-\hat{P}_b)}{N} + \frac{\hat{P}_a(1-\hat{P}_a)}{N}}}<br>$$<br>带入值，可以计算得到Z=0.849105726，</li></ul><ul><li><p>显著性检验结论：</p><p>如果取显著性水平$\alpha = 0.5$，则$Z_{\alpha} = 1.644854$，所以不能拒绝原假设，即认为B版本不一定比A版本所带来的线索-车辆成交转化率高。</p></li></ul><p>如果我们将A/B-test的时间拉长，如两周时长的A/B-test分别得到5000条线索量；或者说同样做一周时间的A/B-test，但是测试的比例更大，分别得到5000条线索量。即 N=5000，且线索-车辆成交转化率保持不变。计算得出$Z_{\alpha}=1.89865812$，在同样显著性水 平下，可以拒绝原假设，得出B比A版本所带来的线索-车辆成交转化率高的结论。</p><p>上述结论是符合我们的主观感受的。在小样本量时，新版所带来的线索-车辆成交转化率高于旧版本所带来的线索-车辆成交转化率，其原因也有可能是受到随机波动等因素影响，故不能肯定地说明新版要比旧版所带来的线索-车辆成交转化率高；但在大样本量时，或者说长期来看，新版本所带来的线索-车辆成交转化率都稳定地高于旧版本所带来的线索-车辆成交转化率，我们有理由相信，确实新版本所带来的线索-车辆成交转化率高于旧版本所带来的线索-车辆成交转化率。</p><h1 id="5-A-B-test样本量的确定"><a href="#5-A-B-test样本量的确定" class="headerlink" title="5. A/B-test样本量的确定"></a>5. A/B-test样本量的确定</h1><p>由上述示例可以看出，样本量的不同对于最终结果是有很大影响的。所以在进行抽样之 前的很重要一步是确定样本量；在实践中，样本量是应该在正式抽样进行A/B-test之前便确认的。放到这里讲的原因是为了通过上述示例加深我们对样本量重要性的认识。</p><p>实践中，我们对于样本量的确认，可以根据标准误（或者说我们需要检验的差异变化） 来求出，记标准误为$d$：<br>$$<br>d = Z_{\alpha} \times \hat{\sigma} \tag{1.5}<br>$$</p><p>其中$Z_{\alpha}$是在显著性水平$\alpha$下的临界值；$\hat{\sigma}$是由样本估计出的总体标准差。</p><p>显然，在给定显著性水平$\alpha$、需要检验的差异变化$d$和A版本（参考版本，旧版本）的线索-车辆成交转化率$\hat{P}_a$历史值（或经验值，或小样本预实验后得出的值[8]）后，即可推导得出我们进行A/B-test所需的样本量。</p><p>$$<br>N = \frac{Z_{\alpha}^2}{d^2}(\hat{P}_a(1-\hat{P}_a) + \hat{P}_b(1-\hat{P}_b)) \tag{1.6}<br>$$</p><h1 id="6-指标推广"><a href="#6-指标推广" class="headerlink" title="6. 指标推广"></a>6. 指标推广</h1><p>上文说明的是根据A/B-test进行新、旧版线索-车辆成交转化率的显著性检验。同理，如果需要根据A/B-test进行新、旧版本的UV-线索转化率、UV-上架转化率或者线索-上架转化率等的显著性检验，只需相应修改显著性检验过程中的观测指标($\hat{P}_a, \hat{P}_b$)即可。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;；&lt;/p&gt;
&lt;h1 id=&quot;1-A-B-test解读&quot;&gt;&lt;a href=&quot;#1-A-B-test解读&quot; class=&quot;headerlink&quot; title=&quot;1. A/B-test解读&quot;&gt;&lt;/a&gt;1. A/B-test解读&lt;/h1&gt;&lt;p&gt;A/B-test是为同一个目标制定两个方案，在同一时间维度，分别让组成成分相同（相似）的用户群组随机的使用一个方案，收集各群组的用户体验数据和业务数据，最后根据显著性检验分析评估出最好版本正式采用。&lt;/p&gt;
&lt;p&gt;使用A/B-test首先需要建立一个待测试的版本服务，这个版本服务可能在标题、字体、排版、背景颜色、措辞等方面与原有版本服务有所不同，然后将这两个版本服务以随机的方式同时推送给测试用户。接下来分别统计两个版本的用户转化率指标，然后根据样本数据进行显著性检验。&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术备忘" scheme="https://buracagyang.github.io/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/"/>
    
      <category term="统计学运用" scheme="https://buracagyang.github.io/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E8%BF%90%E7%94%A8/"/>
    
  </entry>
  
  <entry>
    <title>利用numpy.vectorize提升计算速度</title>
    <link href="https://buracagyang.github.io/2019/05/09/numpy-vectorize/"/>
    <id>https://buracagyang.github.io/2019/05/09/numpy-vectorize/</id>
    <published>2019-05-09T09:11:56.441Z</published>
    <updated>2019-06-12T12:45:42.710Z</updated>
    
    <content type="html"><![CDATA[<hr><p>同步于<a href="https://blog.csdn.net/buracag_mc/article/details/88748607" title="https://blog.csdn.net/buracag_mc/article/details/88748607" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/2019/03/18/increase-calculation-speed-with-numpy-vectorize/" target="_blank" rel="noopener">音尘杂记</a></p><p>在实际项目中，对超大矩阵进行计算或者对超大的DataFrame进行计算是一个经常会出现的场景。这里先不考虑开发机本身内存等客观硬件因素，仅从设计上讨论一下不同实现方式带来的性能差异，抛砖引玉。</p><a id="more"></a><p>项目中有这样一个需求，需要根据历史销量数据计算SKU(Stock Keeping Unit)之间的相似度，或者更通俗一点说是根据历史销量数据求不同SKU之间出现的订单交集以及并集大小(注:SKU数量大概15k左右，订单数大概1000k左右)。</p><p>这里给几条示例数据，可以更直观形象地理解这个需求：</p><p><img src="/2019/05/09/numpy-vectorize/1.png" alt="1"></p><p>然后需要根据这些历史的orderno-sku(订单-商品)数据求解出sku的相似度矩阵。其中SKU1和SKU2之间的相似度定义为:</p><p><img src="/2019/05/09/numpy-vectorize/2.png" alt="2"></p><p>可以很快速地想到几种解决方案：</p><ul><li><p>直接for loops；</p></li><li><p>for loops稍微改进采用列表生成器；</p></li><li><p>采用多进程并行计算；</p></li><li><p><strong>采用numpy.vectorize</strong></p></li></ul><h1 id="1-for-loops计算相似度矩阵"><a href="#1-for-loops计算相似度矩阵" class="headerlink" title="1.for loops计算相似度矩阵"></a>1.for loops计算相似度矩阵</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@timer</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_corr_matrix_for_loops</span><span class="params">(order_df)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    for loops计算相似度矩阵</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    df = order_df.groupby([<span class="string">'sku'</span>]).agg(&#123;<span class="string">'orderno'</span>: <span class="keyword">lambda</span> x: set(x)&#125;).reset_index()</span><br><span class="line">    <span class="keyword">del</span> order_df</span><br><span class="line">    gc.collect()</span><br><span class="line">    l = len(df)</span><br><span class="line">    sku_series = df.sku.astype(str)</span><br><span class="line">    corr_matrix_arr = np.ones((l, l))</span><br><span class="line"></span><br><span class="line">    tbar = trange(l)</span><br><span class="line">    tbar.set_description(<span class="string">"compute corr matrix"</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tbar:</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i + <span class="number">1</span>, l):</span><br><span class="line">            corr_matrix_arr[j, i] = corr_matrix_arr[i, j] = len(df.iloc[i, <span class="number">1</span>] &amp; df.iloc[j, <span class="number">1</span>]) / len(</span><br><span class="line">                df.iloc[i, <span class="number">1</span>] | df.iloc[j, <span class="number">1</span>])</span><br><span class="line">    corr_matrix_df = pd.DataFrame(columns=sku_series, index=sku_series, data=corr_matrix_arr)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> corr_matrix_df</span><br></pre></td></tr></table></figure><p>计算耗时：2000s+<br><img src="/2019/05/09/numpy-vectorize/3.png" alt="3"></p><h1 id="2-list-generator计算相似度矩阵"><a href="#2-list-generator计算相似度矩阵" class="headerlink" title="2.list generator计算相似度矩阵"></a>2.list generator计算相似度矩阵</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@timer</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_corr_matrix_generator</span><span class="params">(order_df)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    列表生成器计算相似度矩阵</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    df = order_df.groupby([<span class="string">'sku'</span>]).agg(&#123;<span class="string">'orderno'</span>: <span class="keyword">lambda</span> x: set(x)&#125;).reset_index()</span><br><span class="line">    <span class="keyword">del</span> order_df</span><br><span class="line">    gc.collect()</span><br><span class="line">    l= len(df)</span><br><span class="line">    sku_series = df.sku.astype(str)</span><br><span class="line">    corr_matrix_arr = np.ones((l, l))</span><br><span class="line"></span><br><span class="line">    l1 = df.orderno</span><br><span class="line">    l2 = np.array(df[<span class="string">'orderno'</span>].apply(len), dtype=np.int8)</span><br><span class="line"></span><br><span class="line">    result_list = [[i, j, len(l1[i] &amp; l1[j])] <span class="keyword">for</span> i <span class="keyword">in</span> range(l)</span><br><span class="line">                   <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>, l) <span class="keyword">if</span> len(l1[i] &amp; l1[j]) &gt; <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, j, k <span class="keyword">in</span> result_list:</span><br><span class="line">        corr_matrix_arr[j, i] = corr_matrix_arr[i, j] = k * <span class="number">1.0</span> / (l2[i] + l2[j] - k)</span><br><span class="line">    corr_matrix_df = pd.DataFrame(columns=sku_series, index=sku_series, data=corr_matrix_arr)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> corr_matrix_df</span><br></pre></td></tr></table></figure><p>计算耗时：1296s<br><img src="/2019/05/09/numpy-vectorize/4.png" alt="4"></p><h1 id="3-多进程计算相似度矩阵"><a href="#3-多进程计算相似度矩阵" class="headerlink" title="3.多进程计算相似度矩阵"></a>3.多进程计算相似度矩阵</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@timer</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_corr_matrix_multiprocessing</span><span class="params">(order_df)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    多进程计算相似度矩阵</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    df = order_df.groupby([<span class="string">'sku'</span>]).agg(&#123;<span class="string">'orderno'</span>: <span class="keyword">lambda</span> x: set(x)&#125;).reset_index()</span><br><span class="line">    <span class="keyword">del</span> order_df</span><br><span class="line">    gc.collect()</span><br><span class="line">    l = len(df)</span><br><span class="line">    sku_series = df.sku.astype(str)</span><br><span class="line">    </span><br><span class="line">    l1 = df.orderno</span><br><span class="line">    l2 = np.array(df[<span class="string">'orderno'</span>].apply(len), dtype=np.int8)</span><br><span class="line">    <span class="keyword">del</span> df</span><br><span class="line">    gc.collect()</span><br><span class="line"></span><br><span class="line">    arr2 = np.zeros((l, l), dtype=np.float32)</span><br><span class="line">    pairs = [[i, j] <span class="keyword">for</span> i <span class="keyword">in</span> range(l - <span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> range(i + <span class="number">1</span>, l)]</span><br><span class="line"></span><br><span class="line">    loops = int(math.ceil((l ** <span class="number">2</span> - l) / <span class="number">10</span> ** <span class="number">6</span> / <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">    tbar = trange(loops)</span><br><span class="line">    tbar.set_description(<span class="string">"compute corr matrix"</span>)</span><br><span class="line">    pool = Pool(<span class="number">4</span>)</span><br><span class="line">    <span class="keyword">for</span> loop <span class="keyword">in</span> tbar:</span><br><span class="line">        temp_lists = [[i, j, l1[i], l1[j]] <span class="keyword">for</span> i, j <span class="keyword">in</span> pairs[(<span class="number">10</span> ** <span class="number">6</span> * loop): (<span class="number">10</span> ** <span class="number">6</span> * (loop + <span class="number">1</span>))]]</span><br><span class="line">        temp_results = pool.map(cal, temp_lists)</span><br><span class="line">        <span class="keyword">for</span> i, j, k <span class="keyword">in</span> temp_results:</span><br><span class="line">            arr2[i, j] = k</span><br><span class="line">    pool.close()</span><br><span class="line">    pool.join()</span><br><span class="line"></span><br><span class="line">    arr1 = l2 + l2.reshape((l, <span class="number">1</span>))</span><br><span class="line">    arr2 = arr2 + arr2.T  <span class="comment"># 变对称阵</span></span><br><span class="line">    arr3 = arr2 / (arr1 - arr2) + np.eye(l)</span><br><span class="line">    <span class="keyword">del</span> arr1</span><br><span class="line">    <span class="keyword">del</span> arr2</span><br><span class="line">    gc.collect()</span><br><span class="line"></span><br><span class="line">    corr_matrix_df = pd.DataFrame(columns=sku_series, index=sku_series, data=arr3)</span><br><span class="line">    <span class="keyword">return</span> corr_matrix_df</span><br></pre></td></tr></table></figure><p>计算耗时：1563s<br><img src="/2019/05/09/numpy-vectorize/5.png" alt="5"></p><h1 id="4-numpy-vectorize计算相似度矩阵"><a href="#4-numpy-vectorize计算相似度矩阵" class="headerlink" title="4.numpy.vectorize计算相似度矩阵"></a>4.numpy.vectorize计算相似度矩阵</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@timer</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_corr_matrix_vectorize</span><span class="params">(order_df)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    numpy.vectorice计算相似度矩阵</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    df = order_df.groupby([<span class="string">'sku'</span>]).agg(&#123;<span class="string">'orderno'</span>: <span class="keyword">lambda</span> x: set(x)&#125;).reset_index()</span><br><span class="line">    l = len(df)</span><br><span class="line">    sku_series = df.sku.astype(str)</span><br><span class="line">    arr = df.orderno.values</span><br><span class="line">    corr_matrix_arr = np.zeros((l, l))</span><br><span class="line">    f_vec = np.vectorize(len)</span><br><span class="line">    arr1 = f_vec(arr)</span><br><span class="line"></span><br><span class="line">    tbar = trange(l - <span class="number">1</span>)</span><br><span class="line">    tbar.set_description(<span class="string">"compute corr matrix"</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tbar(l - <span class="number">1</span>):</span><br><span class="line">        corr_matrix_arr[i, (i + <span class="number">1</span>): l] = f_vec(arr[(i + <span class="number">1</span>): l] &amp; arr[i])</span><br><span class="line">    corr_matrix_arr1 = np.add.outer(arr1, arr1)</span><br><span class="line">    temp = corr_matrix_arr / (corr_matrix_arr1 - corr_matrix_arr)</span><br><span class="line">    temp = temp + temp.T + np.eye(l)</span><br><span class="line">    <span class="keyword">return</span> pd.DataFrame(columns=sku_series, index=sku_series, data=temp)</span><br></pre></td></tr></table></figure><p>计算耗时：72s<br><img src="/2019/05/09/numpy-vectorize/6.png" alt="6"></p><p>可以看到，使用numpy.vectorize提升了20倍左右！</p><p><strong>思考：</strong><br>结合到实际业务中，其实有很多可以改进的地方：1. 并不需要计算所有SKU之间的相似度（提速）; 2. 可以只保存上三角阵或保存有效的相似SKU数据(降低内存)。这块儿就不展开赘述了。</p>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc/article/details/88748607&quot; title=&quot;https://blog.csdn.net/buracag_mc/article/details/88748607&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/2019/03/18/increase-calculation-speed-with-numpy-vectorize/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在实际项目中，对超大矩阵进行计算或者对超大的DataFrame进行计算是一个经常会出现的场景。这里先不考虑开发机本身内存等客观硬件因素，仅从设计上讨论一下不同实现方式带来的性能差异，抛砖引玉。&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术备忘" scheme="https://buracagyang.github.io/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/"/>
    
      <category term="Python" scheme="https://buracagyang.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>SVM推导过程注解</title>
    <link href="https://buracagyang.github.io/2019/05/08/svm-proving-process/"/>
    <id>https://buracagyang.github.io/2019/05/08/svm-proving-process/</id>
    <published>2019-05-08T09:51:18.299Z</published>
    <updated>2019-06-03T10:51:36.385Z</updated>
    
    <content type="html"><![CDATA[<hr><p>同步于<a href="https://blog.csdn.net/buracag_mc/article/details/76762249" title="https://blog.csdn.net/buracag_mc/article/details/76762249" target="_blank" rel="noopener">CSDN</a>;<a href="https://www.runblog.online/2019/03/18/svm-process/" target="_blank" rel="noopener">音尘杂记</a></p><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>支持向量机(Support Vector Machine)的原理其实比较简单，它是基于结构风险最小化理论之上在特征空间中建构最优分割超平面。在二维中就是线，在三维中就是面，但我们统称为超平面。</p><a id="more"></a><p>就我所看到的相关书本、论文以及网上博文情况来看，其一般步骤通常如下：</p><ul><li>在二维平面中的线性可分情况开始讲解，求解硬间隔最优化</li><li>随后放宽条件，这时可以引入松弛向量，然后求解软间隔最优化</li><li>再后面拓展到线性不可分的情况，这时引入核函数方法（kernel trick），将低维数据映射到高维特征空间，在高维特征空间中，这些训练样本便是线性可分的了。</li></ul><p>SVM在数据挖掘与统计机器学习的书中是必讲的，网上优秀的教程也很多；故这里我只是将某些一笔带过或者模棱两可的推导步骤结合自己学习过程做一些补充，错误与不尽之处还望大家不吝指教！欢迎大家使劲儿拍砖耶！</p><h1 id="求解硬间隔最优化时的相关注解"><a href="#求解硬间隔最优化时的相关注解" class="headerlink" title="求解硬间隔最优化时的相关注解"></a>求解硬间隔最优化时的相关注解</h1><ul><li><p>首先我们回忆一下初中所学的知识,两条平行线的方程分别为：<br>$ax + by = c1$<br>$ax + by = c2$           (1)<br>两条平行线的距离d为：<br>$ d = \frac{|c_1-c_2|}{\sqrt(a^2+b^2)} $ (2)</p></li><li><p>范数(norm)相关知识：<br>p-范数 $||X||_p = (|x_1|^p + |x_2|^p+…+ |x_n|^p)^{1/p}$;也即:</p><ul><li><p>1-范数 =$|x_1| + |x_2|+…+ |x_n|$</p></li><li><p>2-范数 =$ (|x_1|^2 + |x_2|^2 + …+|x_n|^2)^{1/2}$</p></li><li><p>$\infty-范数 = MAX(|x_1|, |x_2|, …, |x_n|)$</p></li></ul></li></ul><p>跟博文<a href="http://blog.csdn.net/buracag_mc/article/details/75159437" title="http://blog.csdn.net/buracag_mc/article/details/75159437" target="_blank" rel="noopener">http://blog.csdn.net/buracag_mc/article/details/75159437</a>中所讲的闵可夫斯基距离是否有些似曾相识；的确是这样的，p-范数确实满足范数的定义。其中三角不等式的证明不是平凡的，这个结论通常称为闵可夫斯基不等式。</p><p>其中2-范数简单记为||X||,也就是我们通常意义上所说的欧式距离！</p><p>先描述一下，假设我们有N个训练样本${(x_1, y_1),(x_1, y_1), …, (x_n, y_n)}$，x是2维向量，而$y_i \in {+1, -1}$是训练样本的标签，分别代表两个不同的类。这里我们需要用这些样本去训练学习一个线性分类器：$f(x)=sgn(w^Tx + b)$，sgn函数就是一个符号函数，也就是说$w^Tx+ b$大于0的时候，输出f(x) = 1，小于0的时候，f(x) = -1。而$w^Tx + b=0$就是我们要寻找的分类超平面，如下图所示：<br><img src="http://img.blog.csdn.net/20170806110734659?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYnVyYWNhZ19tYw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p>我们需要这个超平面分隔这两类的效果最好，也就是说让这个超平面到这两个类的最近的那个样本的距离相同且最大。为了更好的说明，找到两个和这个超平面平行和距离相等的超平面，其实在平面几何中我们知道这就是平行线的移动，OK,如果各移动m个单位就达到要求，即：</p><p>$H_1: y = w^Tx + b=m$<br>$H_2: y = w^Tx + b=-m$</p><p>形式是不跟教材中的不一样？没关系，这里我们只是需要方程两边同时除以一个m即可：</p><p>$H_1: y = (\frac{w}{m})^Tx + \frac{b}{m}=1$<br>$H_2: y = (\frac{w}{m})^Tx + \frac{b}{m}=-1$(4)</p><p>这里为了统一起见，我们令w = w/m, b=b/m，注意与前面所说的$w^Tx + b=0$中的w和b是有区别的。(其实对于$w^Tx + b=0$,我们可以进行同样处理$H_1: y = (\frac{w}{m})^Tx + \frac{b}{m}=\frac{0}{m}$,再令w=w/m, b=b/m,即可完全统一了)</p><p>在H1左侧的函数值大于1，所有其分类为+1；在H2右侧的函数值小于1，所有其分类为-1，<br>可以统一记为记为$y_i(W^T.x_i + b) \geq 1$<br>这样便是我们熟悉的形式了！</p><hr><p>下面大家便可以猜想到了，求$H_1$和$H_2$之间的最大距离。当然如果是在二维平面中(当然，这里是以二维特征来说的，当然就是二维平面了)，易知便是两条平行线之间的距离，根据前面所所述平行线的距离即可求出，这里我们称之为margin。<br>即：margin = 2/||W||<br>这里对于二维特征$W^T = (w_1,w_2)$，||W||便是参数W的二范数(有的教科书又称之“模”)，将上式展开表示我们熟悉的平行线的距离了$margin = \frac{2}{\sqrt(w_1^2 + w_2^2)}$</p><p>但是，在统计机器学习中，我们要让它符合更多一般的情况，美其名曰便是“泛化能力”。将特征空间拓展到多维的情况，便是用向量来进行表示了，故在多维特征空间中，我们同样求margin= 2/||W||。</p><p>要使margin最大，即需W最小，故我们设我们的目标函数：<br>$min \frac{1}{2}||W||^2$<br>$s.t. yi(W^Tx_i + b) \geq 1, \forall x_i$                                              (5)</p><p>很多人会纠结W前面的系数1/2，这里加不加1/2其实没关系，这是为了求导时消去。其实在机器学习中， 我们常见的平方损失函数便是进行了同样的处理，在前面加了个常数系数1/2。</p><p>对于(5)式，准确的讲这是一个带有不等式约束的条件极值问题，根据高等数学和基础运筹学内容可以知道，我们可以用<strong>拉格朗日方法求解</strong>。</p><p>这里我必须要补充的一点是：通过查阅教科书以及在阅读网上的优秀教程，我发现不同教科书和网上不同的教程都有不同的说法，虽然实质是不变的，但当时我遇到的坑必须给大家给填了。</p><p>首先带不等式约束的条件极值问题中会有大于号约束、小于号约束两种(这里我们暂且先不说带等号，下文将KKT条件的时候一并补充)</p><ul><li><p>第一种说法如下：将所有不等式约束条件<strong>统一为小于号约束</strong>，然后拉格朗日方程的构建规则是用约束方程乘以非负的拉格朗日系数，然后再<strong>加上</strong>目标函数即可。</p></li><li><p>第二种说法如下：将所有不等式约束条件<strong>统一为大于号约束</strong>，然后拉格朗日方程的构建规则是用约束方程乘以非负的拉格朗日系数，然后再从目标函数中<strong>减去</strong>即可。</p></li></ul><p>其实我们可以发现这两种说法是等价的！事实确实如此，但是很多博文在讲解拉格朗日函数的构建时要么说用目标函数加上约束方程乘以非负的拉格朗日系数，要么说用目标函数减去约束方程乘以非负的拉格朗日系数。</p><p>可能某些文章作者完全没有申明大前提，他们准确的说法应该是，<strong><em>当统一成小于号约束时，拉格朗日函数的构建时是用目标函数加上约束方程乘以非负的拉格朗日系数；当统一成大于号约束时，拉格朗日函数的构建时是用目标函数减去约束方程乘以非负的拉格朗日系数。</em></strong>在不提前申明不同的大前提下，可能会误导不细心以及课程学的不仔细的读者(当时包括我=_=！)，导致某些人纳闷了，咦，这个拉格朗日咋一会儿是加上约束约束乘以拉格朗日系数，一会儿又是减去约束方程乘以拉格朗日系数啊？？？</p><p>为了统一与方便说明起见，故下文我们运用的第一种规则，将不等式约束条件统一成小于号约束。于是得到拉格朗日方程如下：<br>$L(w,b,a) = \frac{1}{2}||W||^2 + \sum_{i=1}^{n}a_i(1-y_i(wx_i+b)) = \frac{1}{2}||W||^2 - \sum_{i=1}^{n}a_i(y_i(wx_i+b)) + \sum_{i=1}^{n}a_i $<br>(6)</p><p>拉格朗日函数构建好后接下来便是简单的求解问题了，分别对W和b求偏导数并令其为零，得到如下结果：<br>$W = \sum_{i=1}^{n}a_iy_ix_i$                                    (7)<br>$\sum_{i=1}^{n}a_iy_i = 0$                                                 (8)    </p><p>带入(6)式即可得到:<br>$Max.W(a) =\sum_{i=1}^{n}a_i - \frac{1}{2}\sum_{i=1,j=1}^{n}a_ia_jy_iy_jx_i^Tx_j$<br>$s.t. a_i \geq 0, \sum_{i=1}^{n}a_iy_i = 0$(9)</p><p>为什么$min \frac{1}{2}||W||^2$问题变成了<br>$Max.W(a) =\sum_{i=1}^{n}a_i - \frac{1}{2}\sum_{i=1,j=1}^{n}a_ia_jy_iy_jx_i^Tx_j$<br>当然是对偶问题的求解了！对偶问题是怎么推导过来的？很多文章仅仅只是一笔带过了这么重要的推导内容。。。导致很多人有些小困惑哈~，为什么构建拉格朗日函数后就将求最小化问题变成求最大化问题？OK，既然本文的定位是SVM推导过程中的解析及注解，必定是要把这个问题完整给推导清楚的。</p><h2 id="SVM中对偶问题的注解"><a href="#SVM中对偶问题的注解" class="headerlink" title="SVM中对偶问题的注解"></a>SVM中对偶问题的注解</h2><p>再回看(6)式，<br>$L(w,b,a) = \frac{1}{2}||W||^2 + \sum_{i=1}^{n}a_i(1-y_i(W^Tx_i+b)) = \frac{1}{2}||W||^2 - \sum_{i=1}^{n}a_i(y_i(W^Tx_i+b)) + \sum_{i=1}^{n}a_i $<br>$s.t. a_i \geq 0$</p><p>我们要处理的最优化问题最正确的表达形式其实为：<br>        <img src="http://img.blog.csdn.net/20170806113012070?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYnVyYWNhZ19tYw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述">            (10)<br>上式才是严格带有不等式约束条件下的拉格朗日条件极值的表达式。我读的很多介绍SVM的文章(包括我看的书本)都是没说的！(10)式便是一个凸规划问题。</p><p>其意义是先对a求偏导，令其等于0消掉a，然后再对W和b求L的最小值。</p><p>要直接求解(10)式是有难度的，幸好这个问题可以通过拉格朗日对偶问题来解决。常说对偶问题对偶问题，现在就是真正发挥这把利器的时候了。对(10)式做一个简单的等价变换：<br>                        <img src="http://img.blog.csdn.net/20170806113254715?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYnVyYWNhZ19tYw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述">(11)</p><p><strong>上式即为对偶变换</strong>，这样就把这个凸规划问题转换成了对偶问题</p><p>其意义是：原凸规划问题可以转化为先对W和b求偏导，令两个偏导数都等于0消掉W和b，然后再对a求L的最大值。与(10)的意义是相反的，或者说是对偶的！不知我讲到这步，大家是否对对偶问题有了一个豁然开朗的感觉——啊！原来对偶问题就是这啊！！</p><p>然后将求得的(7)式和(8)式带入(6)式，得：<br>                            <img src="http://img.blog.csdn.net/20170806113534514?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYnVyYWNhZ19tYw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述">        (12)<br>将(12)式带入(11)式得：<br>                <img src="http://img.blog.csdn.net/20170806113608420?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYnVyYWNhZ19tYw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述">            (13)<br>再考虑到(8)式，对偶问题的完整表达为：<br><img src="http://img.blog.csdn.net/20170806113656054?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYnVyYWNhZ19tYw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述">                            (14)</p><p>到了这一步，我们便可以直接用数值方法计算求解拉格朗日乘数a了。求得a过后根据(7)式可以得到W，然后根据超平面方程可以求出b。最终便得到了我们想要的超平面和分类决策函数，也就是我们训练好的SVM分类器。那么对于待分类样本X，其分类为为：<br>                                      <img src="http://img.blog.csdn.net/20170806113836750?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYnVyYWNhZ19tYw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述">    (15)</p><p>我们根据(15)式可以发现，对于一个待分类样本，我们先计算待分类样本和训练样本的内积然后加权就和再加上b值即可。训练样本特别大的情况下，如果对所有训练样本做运算是否太耗时了啊？很多教科书以及网上教程都是直接说根据KKT条件可知，只有支持向量的乘子(拉格朗日乘数)$a_i$不等于0，其他训练样本的乘子都为0，这样便会大大减少运算量，也是后面SVM引入核函数(kernel)的铺垫。这又会引起新的疑惑，为什么只有支持向量对应的乘子不为0呢？</p><h2 id="SVM中KKT条件注解"><a href="#SVM中KKT条件注解" class="headerlink" title="SVM中KKT条件注解"></a>SVM中KKT条件注解</h2><p>这里还是继续讨论一下带等式和不等式约束的条件极值问题。任何极值问题的约束条件不外乎3种：等式、大于号和小于号，为了统一起见，我们将不等式约束统一为小于号。<br>例如：<br>$min(max)    f(x) $<br>$s.t.     g_i(x) \leq0,i=1,2…n_1$<br>$     h_j(x) = 0,j=1,2…n_2$</p><p>那么一个极值优化问题我们转化为：<br><img src="http://img.blog.csdn.net/20170806114231142?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYnVyYWNhZ19tYw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><ul><li>KKT条件就是函数的最优值必须满足以下条件：<ul><li>L对各个x的偏导为零</li><li>h(x) = 0</li><li>$\sum_{i=1}^{n_1}a_ig_i(x) =0 , a_i\geq0$</li></ul></li></ul><p>假设一个目标函数，3个不等式约束条件把自变量约束在一定范围，而目标函数是在这个范围内寻找最优解。<br><img src="http://img.blog.csdn.net/20170806114343592?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYnVyYWNhZ19tYw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><ul><li><p>1.函数开始也不知道该取哪一个值是吧，假设某一次取得自变量集合为x1*，发现不满足约束，然后再换呀换；</p></li><li><p>2.假设到x2<em>满足约束条件，但是这个时候函数值不是最优的，并且x2</em>使得g1(x)与g2(x)等于0了，而g3(x)还是小于0。这个时候，我们发现在x2*的基础上再寻找一组更优解要靠谁呢？当然是要靠约束条件g1(x)与g2(x)，因为他们等于0了，很极限呀，一不小心，走错了就不满足这两个约束的条件了，这个时候我们会选择g1(x)与g2(x)的梯度方向往下走，以寻找最优值解。</p></li><li><p>3.这个时候需不需要管约束条件g3(x)呢？正常来说管不管都可以，如果管，也取g3在x2<em>处的梯度的话，由于g3已经满足小于0的条件，这时候再取在x2</em>处的梯度，有可能更快得到结果，也有可能适得其反；如果不管g3，由于g1和g2已经在边缘了，只取g1和g2的梯度，是肯定会让目标函数接近解的；故我们这时候是不用考虑g3的；</p></li><li><p>4.再往下走，到了x3*处发现g2和g3等于0了，也就是说走到边了，而g1是满足约束小于0的，这时候我们重复上一步，取g2和g3的梯度方向作为变化方向，而不用管g1.</p></li><li><p>5.一直循环3(4)步，直到找到最优解。</p></li></ul><p>可以看到的是，如果如果g1、g2=0时，由于他们本身的条件是小于0的，我们是需要优化他们的，操作上便是乘以一个正常数a作为他们梯度增长的倍数(或者说学习效率)，那些暂且不需要考虑的约束，例如这里说的g3，我们可以乘以系数0，即在下一次的优化中是不用考虑这些约束的。综上所述的话：<br>$\sum_{i=1}^{n_1}a_ig_i(x) = 0, a_i\geq0$</p><p>如上，简单直观地说便是KKT条件中第三个式子的意义了。</p><p>回到SVM的推导上来，对于(6)式，我们知道其KKT条件中的第三个式子为:<br>$\sum_{i=1}^{n_1}a_i(1-y_i(W^T.x_i+b)) = 0$，</p><p>我们知道除了支持向量，对于其他训练样本有：</p><ul><li><p>$y_i(W^T.x_i + b) &gt; 1$ 也即$1 - y_i(W^T.x_i + b) &lt;0$根据前面所述的内容知道，其对应的乘子为0。</p></li><li><p>对于支持向量来说：$y_i(W^T.x_i + b) =1$ 也即$1 - y_i(W^T.x_i + b) =0$，其对应的乘子不为0。</p></li></ul><p>也就是说，新来的待分类样本只需与支持向量求内积即可，这便大大减少了计算量！这便是KKT条件在SVM关键推导中的应用。</p><p>这里我再补偿一下另外一种思路，其实本质还是KKT条件：<br>由于(5)式与(10)式等价，即：<br><img src="http://img.blog.csdn.net/20170806114946494?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYnVyYWNhZ19tYw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述">        (16)</p><p>故要使(16)式成立，只有令$a_i(1-y_i(W^T.x_i+b)) = 0$成立，由此得到KKT的第三个条件：<br>$\sum_{i=1}^{n_1}a_i(1-y_i(W^T.x_i+b)) = 0$<br>同样可出结论：支持向量对应的乘子为正系数；如果一个样本不是支持向量，则其对应的乘子为0。</p><hr>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;同步于&lt;a href=&quot;https://blog.csdn.net/buracag_mc/article/details/76762249&quot; title=&quot;https://blog.csdn.net/buracag_mc/article/details/76762249&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN&lt;/a&gt;;&lt;a href=&quot;https://www.runblog.online/2019/03/18/svm-process/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;音尘杂记&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;支持向量机(Support Vector Machine)的原理其实比较简单，它是基于结构风险最小化理论之上在特征空间中建构最优分割超平面。在二维中就是线，在三维中就是面，但我们统称为超平面。&lt;/p&gt;
    
    </summary>
    
    
      <category term="技术备忘" scheme="https://buracagyang.github.io/tags/%E6%8A%80%E6%9C%AF%E5%A4%87%E5%BF%98/"/>
    
      <category term="统计学运用" scheme="https://buracagyang.github.io/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E8%BF%90%E7%94%A8/"/>
    
      <category term="算法备忘" scheme="https://buracagyang.github.io/tags/%E7%AE%97%E6%B3%95%E5%A4%87%E5%BF%98/"/>
    
  </entry>
  
</feed>
